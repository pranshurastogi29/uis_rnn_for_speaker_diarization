{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9716,
     "status": "ok",
     "timestamp": 1570003716425,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "guhScHfwcN_b",
    "outputId": "d0cb939e-14d6-49f5-9fc9-ec3cf06796e1"
   },
   "outputs": [],
   "source": [
    "# Run this cell and select the kaggle.json file downloaded\n",
    "# from the Kaggle account settings page.\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "# this is my kaggle .json you can upload your kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 35552,
     "status": "ok",
     "timestamp": 1570016396130,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "gTRET41LcaQs",
    "outputId": "7998cc45-7311-4b09-e158-5c9ec6f268ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# this cell is important to execute as the toy dataset in your drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13870,
     "status": "ok",
     "timestamp": 1570003720588,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "6vXFIL_lcX5E",
    "outputId": "49b405b6-e98c-4835-c89b-af45d0841ea1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 65 Oct  2 08:08 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "# Let's make sure the kaggle.json file is present.\n",
    "!ls -lha kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30915,
     "status": "ok",
     "timestamp": 1570003737638,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "Ci7T91r7cYB-",
    "outputId": "b4cd2463-b999-44a8-e3bc-e5a2107fb069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
      "ref                                                       title                                               size  lastUpdated          downloadCount  \n",
      "--------------------------------------------------------  -------------------------------------------------  -----  -------------------  -------------  \n",
      "dgomonov/new-york-city-airbnb-open-data                   New York City Airbnb Open Data                       2MB  2019-08-12 16:24:45          23056  \n",
      "lakshyaag/india-trade-data                                India - Trade Data                                   1MB  2019-08-16 16:13:58          10969  \n",
      "therohk/ireland-historical-news                           The Irish Times - Waxy-Wany News                    47MB  2019-08-24 15:36:54           2360  \n",
      "dareenalharthi/jamalon-arabic-books-dataset               Jamalon Arabic Books Dataset                         1MB  2019-08-15 18:58:06            646  \n",
      "rajeevw/ufcdata                                           UFC-Fight historical data from 1993 to 2019          2MB  2019-07-05 09:58:02            767  \n",
      "ma7555/schengen-visa-stats                                Schengen Visa Stats 2017/2018                        1MB  2019-07-25 10:55:37           1794  \n",
      "bradklassen/pga-tour-20102018-data                        PGA Tour Golf Data                                  97MB  2019-09-26 23:22:00           6716  \n",
      "shuyangli94/food-com-recipes-and-user-interactions        Food.com Recipes and Interactions                  261MB  2019-08-29 22:23:38            303  \n",
      "ruslankl/european-union-lgbt-survey-2012                  EU LGBT Survey                                     577KB  2019-07-19 11:15:25            250  \n",
      "tristan581/17k-apple-app-store-strategy-games             17K Mobile Strategy Games                            8MB  2019-08-26 08:22:16            472  \n",
      "akhilv11/border-crossing-entry-data                       Border Crossing Entry Data                           3MB  2019-08-21 14:51:34            349  \n",
      "codersree/mount-rainier-weather-and-climbing-data         Mount Rainier Weather and Climbing Data             25KB  2019-08-27 23:33:36           2110  \n",
      "kapilverma/hindi-bible                                    Hindi Bible                                          4MB  2019-09-07 18:04:35             38  \n",
      "samhiatt/xenocanto-avian-vocalizations-canv-usa           Avian Vocalizations from CA & NV, USA                1GB  2019-08-10 00:16:10            411  \n",
      "lishuyangkaggle/cocktails-hotaling-co                     Cocktails (Hotaling & Co.)                          75KB  2019-07-08 23:49:34           1681  \n",
      "gustavomodelli/forest-fires-in-brazil                     Forest Fires in Brazil                              31KB  2019-08-24 16:09:16            882  \n",
      "chirin/africa-economic-banking-and-systemic-crisis-data   Africa Economic, Banking and Systemic Crisis Data   14KB  2019-07-21 02:00:17            335  \n",
      "martj42/international-football-results-from-1872-to-2017  International football results from 1872 to 2019   523KB  2019-07-22 17:03:13          21777  \n",
      "valentynsichkar/traffic-signs-preprocessed                Traffic Signs Preprocessed                            0B  2019-08-31 18:22:11            100  \n",
      "hmavrodiev/sofia-air-quality-dataset                      Sofia air quality dataset                            3GB  2019-09-14 05:48:09            105  \n"
     ]
    }
   ],
   "source": [
    "# Next, install the Kaggle API client.\n",
    "!pip install -q kaggle\n",
    "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
    "# so move it there.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "\n",
    "# This permissions change avoids a warning on Kaggle tool startup.\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "# List available datasets.\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 93911,
     "status": "ok",
     "timestamp": 1570003800641,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "M_ijJyG4cYFV",
    "outputId": "ce15f0b4-8197-476a-ca77-81f0829c2281"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d mfekadu/darpa-timit-acousticphonetic-continuous-speech\n",
    "!unzip darpa-timit-acousticphonetic-continuous-speech.zip\n",
    "!unzip data.zip\n",
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFm5GTA2cYIu"
   },
   "outputs": [],
   "source": [
    "dict1={\"training\": True,\n",
    "\"device\": \"cuda\",\n",
    "\"unprocessed_data\":'TRAIN/*/*/*.wav  ',\n",
    "\"train_path\" : 'train_tisv',\n",
    "\"train_path_unprocessed\": 'TRAIN/*/*/*.wav',\n",
    "\"test_path\": 'test_tisv',\n",
    "\"test_path_unprocessed\": 'TRAIN/*/*/*.wav',\n",
    "\"data_preprocessed\":True ,\n",
    "\"sr\": 16000,\n",
    "\"nfft\": 512,\n",
    "\"window\": 0.025 ,\n",
    "\"hop\": 0.01 ,\n",
    "\"nmels\": 40,\n",
    "\"tisv_frame\": 180 , \n",
    "\"hidden\": 768 ,\n",
    "\"num_layer\": 3 ,\n",
    "\"proj\": 256 ,\n",
    "\"model_path\": 'model' ,\n",
    "\"N\" : 4 ,\n",
    "\"M\" : 5 ,\n",
    "\"num_workers\": 0 ,\n",
    "\"lr\": 0.01 ,\n",
    "\"epochs\": 450 ,\n",
    "\"log_interval\": 30 ,\n",
    "\"log_file\": 'speech_id_checkpoint/Stats',\n",
    "\"checkpoint_interval\": 120 ,\n",
    "\"checkpoint_dir\": 'speech_id_checkpoint',\n",
    "\"restore\": False ,\n",
    "\"N\" : 4 ,\n",
    "\"M\" : 6 ,\n",
    "\"num_workers\": 8,\n",
    "\"epochs_t\": 10 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQutuf3DcYQ_"
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.autograd as grad\n",
    "import torch.nn.functional as F\n",
    "def get_centroids(embeddings):\n",
    "    centroids = []\n",
    "    for speaker in embeddings:\n",
    "        centroid = 0\n",
    "        for utterance in speaker:\n",
    "            centroid = centroid + utterance\n",
    "        centroid = centroid/len(speaker)\n",
    "        centroids.append(centroid)\n",
    "    centroids = torch.stack(centroids)\n",
    "    return centroids\n",
    "\n",
    "def get_centroid(embeddings, speaker_num, utterance_num):\n",
    "    centroid = 0\n",
    "    for utterance_id, utterance in enumerate(embeddings[speaker_num]):\n",
    "        if utterance_id == utterance_num:\n",
    "            continue\n",
    "        centroid = centroid + utterance\n",
    "    centroid = centroid/(len(embeddings[speaker_num])-1)\n",
    "    return centroid\n",
    "\n",
    "def get_cossim(embeddings, centroids):\n",
    "    # Calculates cosine similarity matrix. Requires (N, M, feature) input\n",
    "    cossim = torch.zeros(embeddings.size(0),embeddings.size(1),centroids.size(0))\n",
    "    for speaker_num, speaker in enumerate(embeddings):\n",
    "        for utterance_num, utterance in enumerate(speaker):\n",
    "            for centroid_num, centroid in enumerate(centroids):\n",
    "                if speaker_num == centroid_num:\n",
    "                    centroid = get_centroid(embeddings, speaker_num, utterance_num)\n",
    "                output = F.cosine_similarity(utterance,centroid,dim=0)+1e-6\n",
    "                cossim[speaker_num][utterance_num][centroid_num] = output\n",
    "    return cossim\n",
    "\n",
    "def calc_loss(sim_matrix):\n",
    "    # Calculates loss from (N, M, K) similarity matrix\n",
    "    per_embedding_loss = torch.zeros(sim_matrix.size(0), sim_matrix.size(1))\n",
    "    for j in range(len(sim_matrix)):\n",
    "        for i in range(sim_matrix.size(1)):\n",
    "            per_embedding_loss[j][i] = -(sim_matrix[j][i][j] - ((torch.exp(sim_matrix[j][i]).sum()+1e-6).log_()))\n",
    "    loss = per_embedding_loss.sum()    \n",
    "    return loss, per_embedding_loss\n",
    "\n",
    "def normalize_0_1(values, max_value, min_value):\n",
    "    normalized = np.clip((values - min_value) / (max_value - min_value), 0, 1)\n",
    "    return normalized\n",
    "\n",
    "def mfccs_and_spec(wav_file, wav_process = False, calc_mfccs=False, calc_mag_db=False):    \n",
    "    sound_file, _ = librosa.core.load(wav_file, sr=hp.data.sr)\n",
    "    window_length = int(hp.data.window*hp.data.sr)\n",
    "    hop_length = int(hp.data.hop*hp.data.sr)\n",
    "    duration = hp.data.tisv_frame * hp.data.hop + hp.data.window\n",
    "    \n",
    "    # Cut silence and fix length\n",
    "    if wav_process == True:\n",
    "        sound_file, index = librosa.effects.trim(sound_file, frame_length=window_length, hop_length=hop_length)\n",
    "        length = int(hp.data.sr * duration)\n",
    "        sound_file = librosa.util.fix_length(sound_file, length)\n",
    "        \n",
    "    spec = librosa.stft(sound_file, n_fft=hp.data.nfft, hop_length=hop_length, win_length=window_length)\n",
    "    mag_spec = np.abs(spec)\n",
    "    \n",
    "    mel_basis = librosa.filters.mel(hp.data.sr, hp.data.nfft, n_mels=hp.data.nmels)\n",
    "    mel_spec = np.dot(mel_basis, mag_spec)\n",
    "    \n",
    "    mag_db = librosa.amplitude_to_db(mag_spec)\n",
    "    #db mel spectrogram\n",
    "    mel_db = librosa.amplitude_to_db(mel_spec).T\n",
    "    \n",
    "    mfccs = None\n",
    "    if calc_mfccs:\n",
    "        mfccs = np.dot(librosa.filters.dct(40, mel_db.shape[0]), mel_db).T\n",
    "    \n",
    "    return mfccs, mel_db, mag_db\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    w = grad.Variable(torch.tensor(1.0))\n",
    "    b = grad.Variable(torch.tensor(0.0))\n",
    "    embeddings = torch.tensor([[0,1,0],[0,0,1], [0,1,0], [0,1,0], [1,0,0], [1,0,0]]).to(torch.float).reshape(3,2,3)\n",
    "    centroids = get_centroids(embeddings)\n",
    "    cossim = get_cossim(embeddings, centroids)\n",
    "    sim_matrix = w*cossim + b\n",
    "    loss, per_embedding_loss = calc_loss(sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJi3dXl1cYX9"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import contextlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import librosa\n",
    "import wave\n",
    "\n",
    "import webrtcvad\n",
    "sr=dict1['sr']\n",
    "def read_wave(path, sr):\n",
    "    \"\"\"Reads a .wav file.\n",
    "    Takes the path, and returns (PCM audio data, sample rate).\n",
    "    Assumes sample width == 2\n",
    "    \"\"\"\n",
    "    with contextlib.closing(wave.open(path, 'rb')) as wf:\n",
    "        num_channels = wf.getnchannels()\n",
    "        sample_width = wf.getsampwidth()\n",
    "        assert sample_width == 2\n",
    "        sample_rate = wf.getframerate()\n",
    "        pcm_data = wf.readframes(wf.getnframes())\n",
    "    data, _ = librosa.load(path, sr)\n",
    "    assert len(data.shape) == 1\n",
    "    assert sr in (8000, 16000, 32000, 48000)\n",
    "    return data, pcm_data\n",
    "    \n",
    "class Frame(object):\n",
    "    \"\"\"Represents a \"frame\" of audio data.\"\"\"\n",
    "    def __init__(self, bytes, timestamp, duration):\n",
    "        self.bytes = bytes\n",
    "        self.timestamp = timestamp\n",
    "        self.duration = duration\n",
    "\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    \"\"\"Generates audio frames from PCM audio data.\n",
    "    Takes the desired frame duration in milliseconds, the PCM data, and\n",
    "    the sample rate.\n",
    "    Yields Frames of the requested duration.\n",
    "    \"\"\"\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n",
    "    offset = 0\n",
    "    timestamp = 0.0\n",
    "    duration = (float(n) / sample_rate) / 2.0\n",
    "    while offset + n < len(audio):\n",
    "        yield Frame(audio[offset:offset + n], timestamp, duration)\n",
    "        timestamp += duration\n",
    "        offset += n\n",
    "\n",
    "\n",
    "def vad_collector(sample_rate, frame_duration_ms,\n",
    "                  padding_duration_ms, vad, frames):\n",
    "    \"\"\"Filters out non-voiced audio frames.\n",
    "    Given a webrtcvad.Vad and a source of audio frames, yields only\n",
    "    the voiced audio.\n",
    "    Uses a padded, sliding window algorithm over the audio frames.\n",
    "    When more than 90% of the frames in the window are voiced (as\n",
    "    reported by the VAD), the collector triggers and begins yielding\n",
    "    audio frames. Then the collector waits until 90% of the frames in\n",
    "    the window are unvoiced to detrigger.\n",
    "    The window is padded at the front and back to provide a small\n",
    "    amount of silence or the beginnings/endings of speech around the\n",
    "    voiced frames.\n",
    "    Arguments:\n",
    "    sample_rate - The audio sample rate, in Hz.\n",
    "    frame_duration_ms - The frame duration in milliseconds.\n",
    "    padding_duration_ms - The amount to pad the window, in milliseconds.\n",
    "    vad - An instance of webrtcvad.Vad.\n",
    "    frames - a source of audio frames (sequence or generator).\n",
    "    Returns: A generator that yields PCM audio data.\n",
    "    \"\"\"\n",
    "    num_padding_frames = int(padding_duration_ms / frame_duration_ms)\n",
    "    # We use a deque for our sliding window/ring buffer.\n",
    "    ring_buffer = collections.deque(maxlen=num_padding_frames)\n",
    "    # We have two states: TRIGGERED and NOTTRIGGERED. We start in the\n",
    "    # NOTTRIGGERED state.\n",
    "    triggered = False\n",
    "\n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        is_speech = vad.is_speech(frame.bytes, sample_rate)\n",
    "        if not triggered:\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_voiced = len([f for f, speech in ring_buffer if speech])\n",
    "            # If we're NOTTRIGGERED and more than 90% of the frames in\n",
    "            # the ring buffer are voiced frames, then enter the\n",
    "            # TRIGGERED state.\n",
    "            if num_voiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = True\n",
    "                start = ring_buffer[0][0].timestamp\n",
    "                # We want to yield all the audio we see from now until\n",
    "                # we are NOTTRIGGERED, but we have to start with the\n",
    "                # audio that's already in the ring buffer.\n",
    "                for f, s in ring_buffer:\n",
    "                    voiced_frames.append(f)\n",
    "                ring_buffer.clear()\n",
    "        else:\n",
    "            # We're in the TRIGGERED state, so collect the audio data\n",
    "            # and add it to the ring buffer.\n",
    "            voiced_frames.append(frame)\n",
    "            ring_buffer.append((frame, is_speech))\n",
    "            num_unvoiced = len([f for f, speech in ring_buffer if not speech])\n",
    "            # If more than 90% of the frames in the ring buffer are\n",
    "            # unvoiced, then enter NOTTRIGGERED and yield whatever\n",
    "            # audio we've collected.\n",
    "            if num_unvoiced > 0.9 * ring_buffer.maxlen:\n",
    "                triggered = False\n",
    "                yield (start, frame.timestamp + frame.duration)\n",
    "                ring_buffer.clear()\n",
    "                voiced_frames = []\n",
    "    # If we have any leftover voiced audio when we run out of input,\n",
    "    # yield it.\n",
    "    if voiced_frames:\n",
    "        yield (start, frame.timestamp + frame.duration)\n",
    "\n",
    "def VAD_chunk(aggressiveness, path):\n",
    "    audio, byte_audio = read_wave(path, sr)\n",
    "    vad = webrtcvad.Vad(int(aggressiveness))\n",
    "    frames = frame_generator(20, byte_audio,sr)\n",
    "    frames = list(frames)\n",
    "    times = vad_collector(sr, 20, 200, vad, frames)\n",
    "    speech_times = []\n",
    "    speech_segs = []\n",
    "    for i, time in enumerate(times):\n",
    "        start = np.round(time[0],decimals=2)\n",
    "        end = np.round(time[1],decimals=2)\n",
    "        j = start\n",
    "        while j + .4 < end:\n",
    "            end_j = np.round(j+.4,decimals=2)\n",
    "            speech_times.append((j, end_j))\n",
    "            speech_segs.append(audio[int(j*sr):int(end_j*sr)])\n",
    "            j = end_j\n",
    "        else:\n",
    "            speech_times.append((j, end))\n",
    "            speech_segs.append(audio[int(j*sr):int(end*sr)])\n",
    "    return speech_times, speech_segs\n",
    "speech_times, speech_segs = VAD_chunk(3,\"TRAIN/DR8/MRDM0/SI1044.WAV.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf7Xgs4KcYbj"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpeakerDatasetTIMIT(Dataset):\n",
    "    def __init__(self,dict1):\n",
    "\n",
    "        if dict1['training']:\n",
    "            self.path = dict1['train_path_unprocessed']\n",
    "            self.utterance_number = dict1['M']\n",
    "        else:\n",
    "            self.path = dict1['test_path_unprocessed']\n",
    "            self.utterance_number = dict1['M']\n",
    "        self.speakers = glob.glob(os.path.dirname(self.path))\n",
    "        shuffle(self.speakers)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.speakers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        speaker = self.speakers[idx]\n",
    "        wav_files = glob.glob(speaker+'/*.WAV')\n",
    "        shuffle(wav_files)\n",
    "        wav_files = wav_files[0:self.utterance_number]\n",
    "        \n",
    "        mel_dbs = []\n",
    "        for f in wav_files:\n",
    "            _, mel_db, _ = mfccs_and_spec(f, wav_process = True)\n",
    "            mel_dbs.append(mel_db)\n",
    "        return torch.Tensor(mel_dbs)\n",
    "\n",
    "class SpeakerDatasetTIMITPreprocessed(Dataset):\n",
    "    \n",
    "    def __init__(self,dict1, shuffle=True, utter_start=0):\n",
    "        \n",
    "        # data path\n",
    "        if dict1['training']:\n",
    "            self.path = dict1['train_path']\n",
    "            self.utter_num = dict1['M']\n",
    "        else:\n",
    "            self.path = dict1['test_path']\n",
    "            self.utter_num = dict1['M']\n",
    "        self.file_list = os.listdir(self.path)\n",
    "        self.shuffle=shuffle\n",
    "        self.utter_start = utter_start\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        np_file_list = os.listdir(self.path)\n",
    "        \n",
    "        if self.shuffle:\n",
    "            selected_file = random.sample(np_file_list, 1)[0]  # select random speaker\n",
    "        else:\n",
    "            selected_file = np_file_list[idx]               \n",
    "        \n",
    "        utters = np.load(os.path.join(self.path, selected_file))        # load utterance spectrogram of selected speaker\n",
    "        if self.shuffle:\n",
    "            utter_index = np.random.randint(0, utters.shape[0], self.utter_num)   # select M utterances per speaker\n",
    "            utterance = utters[utter_index]       \n",
    "        else:\n",
    "            utterance = utters[self.utter_start: self.utter_start+self.utter_num] # utterances of a speaker [batch(M), n_mels, frames]\n",
    "\n",
    "        utterance = utterance[:,:,:160]               # TODO implement variable length batch size\n",
    "\n",
    "        utterance = torch.tensor(np.transpose(utterance, axes=(0,2,1)))     # transpose [batch, frames, n_mels]\n",
    "        return utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKdgRtf1cYfo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class SpeechEmbedder(nn.Module):\n",
    "    \n",
    "    def __init__(self,dict1):\n",
    "        super(SpeechEmbedder, self).__init__()    \n",
    "        self.LSTM_stack = nn.LSTM(dict1['nmels'], dict1['hidden'], dict1['num_layer'], batch_first=True)\n",
    "        for name, param in self.LSTM_stack.named_parameters():\n",
    "          if 'bias' in name:\n",
    "             nn.init.constant_(param, 0.0)\n",
    "          elif 'weight' in name:\n",
    "             nn.init.xavier_normal_(param)\n",
    "        self.projection = nn.Linear(dict1['hidden'], dict1['proj'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, _ = self.LSTM_stack(x.float()) #(batch, frames, n_mels)\n",
    "        #only use last frame\n",
    "        x = x[:,x.size(1)-1]\n",
    "        x = self.projection(x.float())\n",
    "        x = x / torch.norm(x, dim=1).unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "class GE2ELoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        super(GE2ELoss, self).__init__()\n",
    "        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n",
    "        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, embeddings):\n",
    "        torch.clamp(self.w, 1e-6)\n",
    "        centroids = get_centroids(embeddings)\n",
    "        cossim = get_cossim(embeddings, centroids)\n",
    "        sim_matrix = self.w*cossim.to(self.device) + self.b\n",
    "        loss, _ = calc_loss(sim_matrix)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 593594,
     "status": "ok",
     "timestamp": 1570004300356,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "XpZwo55hcYiX",
    "outputId": "674ad565-f174-4318-b351-69a72d016339"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start text independent utterance feature extraction\n",
      "total speaker number : 462\n",
      "train : 414, test : 48\n",
      "0th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "1th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "2th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "3th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "4th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "5th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "6th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "7th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "8th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "9th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "10th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "11th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "12th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "13th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "14th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "15th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "16th speaker processing...\n",
      "2 (14, 40, 180)\n",
      "17th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "18th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "19th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "20th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "21th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "22th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "23th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "24th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "25th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "26th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "27th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "28th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "29th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "30th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "31th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "32th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "33th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "34th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "35th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "36th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "37th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "38th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "39th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "40th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "41th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "42th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "43th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "44th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "45th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "46th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "47th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "48th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "49th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "50th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "51th speaker processing...\n",
      "2 (14, 40, 180)\n",
      "52th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "53th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "54th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "55th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "56th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "57th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "58th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "59th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "60th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "61th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "62th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "63th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "64th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "65th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "66th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "67th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "68th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "69th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "70th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "71th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "72th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "73th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "74th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "75th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "76th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "77th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "78th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "79th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "80th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "81th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "82th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "83th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "84th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "85th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "86th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "87th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "88th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "89th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "90th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "91th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "92th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "93th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "94th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "95th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "96th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "97th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "98th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "99th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "100th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "101th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "102th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "103th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "104th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "105th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "106th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "107th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "108th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "109th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "110th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "111th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "112th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "113th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "114th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "115th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "116th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "117th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "118th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "119th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "120th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "121th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "122th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "123th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "124th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "125th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "126th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "127th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "128th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "129th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "130th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "131th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "132th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "133th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "134th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "135th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "136th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "137th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "138th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "139th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "140th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "141th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "142th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "143th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "144th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "145th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "146th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "147th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "148th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "149th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "150th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "151th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "152th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "153th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "154th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "155th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "156th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "157th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "158th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "159th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "160th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "161th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "162th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "163th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "164th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "165th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "166th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "167th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "168th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "169th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "170th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "171th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "172th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "173th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "174th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "175th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "176th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "177th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "178th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "179th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "180th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "181th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "182th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "183th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "184th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "185th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "186th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "187th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "188th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "189th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "190th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "191th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "192th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "193th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "194th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "195th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "196th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "197th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "198th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "199th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "200th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "201th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "202th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "203th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "204th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "205th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "206th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "207th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "208th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "209th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "210th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "211th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "212th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "213th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "214th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "215th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "216th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "217th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "218th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "219th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "220th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "221th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "222th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "223th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "224th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "225th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "226th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "227th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "228th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "229th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "230th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "231th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "232th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "233th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "234th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "235th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "236th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "237th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "238th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "239th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "240th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "241th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "242th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "243th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "244th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "245th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "246th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "247th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "248th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "249th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "250th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "251th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "252th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "253th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "254th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "255th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "256th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "257th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "258th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "259th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "260th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "261th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "262th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "263th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "264th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "265th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "266th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "267th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "268th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "269th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "270th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "271th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "272th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "273th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "274th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "275th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "276th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "277th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "278th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "279th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "280th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "281th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "282th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "283th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "284th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "285th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "286th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "287th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "288th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "289th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "290th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "291th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "292th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "293th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "294th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "295th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "296th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "297th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "298th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "299th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "300th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "301th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "302th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "303th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "304th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "305th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "306th speaker processing...\n",
      "2 (10, 40, 180)\n",
      "307th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "308th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "309th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "310th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "311th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "312th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "313th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "314th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "315th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "316th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "317th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "318th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "319th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "320th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "321th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "322th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "323th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "324th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "325th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "326th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "327th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "328th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "329th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "330th speaker processing...\n",
      "2 (22, 40, 180)\n",
      "331th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "332th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "333th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "334th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "335th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "336th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "337th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "338th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "339th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "340th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "341th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "342th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "343th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "344th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "345th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "346th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "347th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "348th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "349th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "350th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "351th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "352th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "353th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "354th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "355th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "356th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "357th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "358th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "359th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "360th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "361th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "362th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "363th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "364th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "365th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "366th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "367th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "368th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "369th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "370th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "371th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "372th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "373th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "374th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "375th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "376th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "377th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "378th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "379th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "380th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "381th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "382th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "383th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "384th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "385th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "386th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "387th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "388th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "389th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "390th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "391th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "392th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "393th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "394th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "395th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "396th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "397th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "398th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "399th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "400th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "401th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "402th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "403th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "404th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "405th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "406th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "407th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "408th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "409th speaker processing...\n",
      "2 (14, 40, 180)\n",
      "410th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "411th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "412th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "413th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "414th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "415th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "416th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "417th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "418th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "419th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "420th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "421th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "422th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "423th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "424th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "425th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "426th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "427th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "428th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "429th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "430th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "431th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "432th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "433th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "434th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "435th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "436th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "437th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "438th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "439th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "440th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "441th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "442th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "443th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "444th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "445th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "446th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "447th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "448th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "449th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "450th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "451th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "452th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "453th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "454th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "455th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "456th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "457th speaker processing...\n",
      "2 (20, 40, 180)\n",
      "458th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "459th speaker processing...\n",
      "2 (18, 40, 180)\n",
      "460th speaker processing...\n",
      "2 (16, 40, 180)\n",
      "461th speaker processing...\n",
      "2 (18, 40, 180)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "audio_path = glob.glob(os.path.dirname(dict1['unprocessed_data']))                                      \n",
    "a=[]\n",
    "def save_spectrogram_tisv(dict1):\n",
    "    \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n",
    "        Each partial utterance is splitted by voice detection using DB\n",
    "        and the first and the last 180 frames from each partial utterance are saved. \n",
    "        Need : utterance data set (TIMIT)\n",
    "    \"\"\"\n",
    "    print(\"start text independent utterance feature extraction\")\n",
    "    os.makedirs(dict1['train_path'], exist_ok=True)   # make folder to save train file\n",
    "    #os.makedirs(dict1['train_path'], exist_ok=True)    # make folder to save test file\n",
    "    utter_min_len = (dict1['tisv_frame'] * dict1['hop'] + dict1['window']) * dict1['sr']   # lower bound of utterance length\n",
    "    total_speaker_num = len(audio_path)\n",
    "    train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "    print(\"total speaker number : %d\"%total_speaker_num)\n",
    "    print(\"train : %d, test : %d\"%(train_speaker_num, total_speaker_num-train_speaker_num))\n",
    "    for i, folder in enumerate(audio_path):\n",
    "        print(\"%dth speaker processing...\"%i)\n",
    "        utterances_spec = []\n",
    "        for utter_name in os.listdir(folder):\n",
    "            if utter_name[-4:] == '.wav':\n",
    "                utter_path = os.path.join(folder, utter_name)         # path of each utterance\n",
    "                utter, sr = librosa.core.load(utter_path, dict1['sr'])        # load utterance audio\n",
    "                intervals = librosa.effects.split(utter, top_db=60)       # voice activity detection\n",
    "                for interval in intervals:\n",
    "                    if (interval[1]-interval[0]) > utter_min_len:           # If partial utterance is sufficient long,\n",
    "                        utter_part = utter[interval[0]:interval[1]]         # save first and last 180 frames of spectrogram.\n",
    "                        S = librosa.core.stft(y=utter_part, n_fft=dict1['nfft'],\n",
    "                                              win_length=int(dict1['window'] * dict1['sr']), hop_length=int(dict1['hop'] * dict1['sr']))\n",
    "                        S = np.abs(S) ** 2\n",
    "                        mel_basis = librosa.filters.mel(sr=dict1['sr'], n_fft=dict1['nfft'], n_mels=dict1['nmels'])\n",
    "                        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "                        utterances_spec.append(S[:, :dict1['tisv_frame']])    # first 180 frames of partial utterance\n",
    "                        utterances_spec.append(S[:, -dict1['tisv_frame']:])   # last 180 frames of partial utterance\n",
    "\n",
    "        utterances_spec = np.array(utterances_spec)\n",
    "        print(\"2\",utterances_spec.shape)\n",
    "        if i<train_speaker_num:      # save spectrogram as numpy file\n",
    "            np.save(os.path.join(dict1['train_path'], \"speaker%d.npy\"%i), utterances_spec)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_spectrogram_tisv(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3179529,
     "status": "ok",
     "timestamp": 1570010098972,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "DpFsAK4XcY1p",
    "outputId": "f0c359ef-b3f8-4288-9702-52ea5e6fb131"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f2282d45358>\n",
      "Wed Oct  2 08:47:54 2019\tEpoch:1[30/103],Iteration:30\tLoss:33.0512\tTLoss:32.5863\t\n",
      "\n",
      "Wed Oct  2 08:48:05 2019\tEpoch:1[60/103],Iteration:60\tLoss:30.0816\tTLoss:32.1748\t\n",
      "\n",
      "Wed Oct  2 08:48:17 2019\tEpoch:1[90/103],Iteration:90\tLoss:32.7073\tTLoss:31.0912\t\n",
      "\n",
      "Wed Oct  2 08:48:34 2019\tEpoch:2[30/103],Iteration:133\tLoss:24.4485\tTLoss:28.5128\t\n",
      "\n",
      "Wed Oct  2 08:48:45 2019\tEpoch:2[60/103],Iteration:163\tLoss:24.4520\tTLoss:28.6056\t\n",
      "\n",
      "Wed Oct  2 08:48:57 2019\tEpoch:2[90/103],Iteration:193\tLoss:32.1484\tTLoss:27.9410\t\n",
      "\n",
      "Wed Oct  2 08:49:14 2019\tEpoch:3[30/103],Iteration:236\tLoss:25.2105\tTLoss:26.4450\t\n",
      "\n",
      "Wed Oct  2 08:49:25 2019\tEpoch:3[60/103],Iteration:266\tLoss:21.3958\tTLoss:26.1612\t\n",
      "\n",
      "Wed Oct  2 08:49:37 2019\tEpoch:3[90/103],Iteration:296\tLoss:23.1856\tTLoss:25.6706\t\n",
      "\n",
      "Wed Oct  2 08:49:54 2019\tEpoch:4[30/103],Iteration:339\tLoss:21.9701\tTLoss:25.3947\t\n",
      "\n",
      "Wed Oct  2 08:50:06 2019\tEpoch:4[60/103],Iteration:369\tLoss:33.0846\tTLoss:25.1355\t\n",
      "\n",
      "Wed Oct  2 08:50:17 2019\tEpoch:4[90/103],Iteration:399\tLoss:31.5411\tTLoss:24.8402\t\n",
      "\n",
      "Wed Oct  2 08:50:34 2019\tEpoch:5[30/103],Iteration:442\tLoss:31.2048\tTLoss:25.1564\t\n",
      "\n",
      "Wed Oct  2 08:50:46 2019\tEpoch:5[60/103],Iteration:472\tLoss:27.0784\tTLoss:25.5434\t\n",
      "\n",
      "Wed Oct  2 08:50:58 2019\tEpoch:5[90/103],Iteration:502\tLoss:21.5169\tTLoss:24.8037\t\n",
      "\n",
      "Wed Oct  2 08:51:15 2019\tEpoch:6[30/103],Iteration:545\tLoss:25.5906\tTLoss:25.9930\t\n",
      "\n",
      "Wed Oct  2 08:51:26 2019\tEpoch:6[60/103],Iteration:575\tLoss:20.8379\tTLoss:24.4497\t\n",
      "\n",
      "Wed Oct  2 08:51:38 2019\tEpoch:6[90/103],Iteration:605\tLoss:24.0772\tTLoss:24.5297\t\n",
      "\n",
      "Wed Oct  2 08:51:55 2019\tEpoch:7[30/103],Iteration:648\tLoss:21.8267\tTLoss:22.6047\t\n",
      "\n",
      "Wed Oct  2 08:52:07 2019\tEpoch:7[60/103],Iteration:678\tLoss:17.6206\tTLoss:22.5581\t\n",
      "\n",
      "Wed Oct  2 08:52:18 2019\tEpoch:7[90/103],Iteration:708\tLoss:17.4912\tTLoss:22.6627\t\n",
      "\n",
      "Wed Oct  2 08:52:35 2019\tEpoch:8[30/103],Iteration:751\tLoss:16.3436\tTLoss:19.6193\t\n",
      "\n",
      "Wed Oct  2 08:52:47 2019\tEpoch:8[60/103],Iteration:781\tLoss:20.4569\tTLoss:19.7322\t\n",
      "\n",
      "Wed Oct  2 08:52:58 2019\tEpoch:8[90/103],Iteration:811\tLoss:25.9171\tTLoss:20.6095\t\n",
      "\n",
      "Wed Oct  2 08:53:15 2019\tEpoch:9[30/103],Iteration:854\tLoss:27.5067\tTLoss:18.7974\t\n",
      "\n",
      "Wed Oct  2 08:53:27 2019\tEpoch:9[60/103],Iteration:884\tLoss:21.7971\tTLoss:19.3324\t\n",
      "\n",
      "Wed Oct  2 08:53:39 2019\tEpoch:9[90/103],Iteration:914\tLoss:15.6710\tTLoss:19.6432\t\n",
      "\n",
      "Wed Oct  2 08:53:56 2019\tEpoch:10[30/103],Iteration:957\tLoss:14.3498\tTLoss:21.3963\t\n",
      "\n",
      "Wed Oct  2 08:54:07 2019\tEpoch:10[60/103],Iteration:987\tLoss:16.6675\tTLoss:20.3061\t\n",
      "\n",
      "Wed Oct  2 08:54:19 2019\tEpoch:10[90/103],Iteration:1017\tLoss:16.8604\tTLoss:19.5792\t\n",
      "\n",
      "Wed Oct  2 08:54:36 2019\tEpoch:11[30/103],Iteration:1060\tLoss:27.0945\tTLoss:16.9344\t\n",
      "\n",
      "Wed Oct  2 08:54:48 2019\tEpoch:11[60/103],Iteration:1090\tLoss:18.9968\tTLoss:17.5584\t\n",
      "\n",
      "Wed Oct  2 08:55:00 2019\tEpoch:11[90/103],Iteration:1120\tLoss:12.5148\tTLoss:17.5078\t\n",
      "\n",
      "Wed Oct  2 08:55:17 2019\tEpoch:12[30/103],Iteration:1163\tLoss:11.5752\tTLoss:15.9352\t\n",
      "\n",
      "Wed Oct  2 08:55:28 2019\tEpoch:12[60/103],Iteration:1193\tLoss:7.7919\tTLoss:16.5233\t\n",
      "\n",
      "Wed Oct  2 08:55:40 2019\tEpoch:12[90/103],Iteration:1223\tLoss:12.6955\tTLoss:16.7515\t\n",
      "\n",
      "Wed Oct  2 08:55:57 2019\tEpoch:13[30/103],Iteration:1266\tLoss:11.2009\tTLoss:16.6343\t\n",
      "\n",
      "Wed Oct  2 08:56:08 2019\tEpoch:13[60/103],Iteration:1296\tLoss:20.8849\tTLoss:15.6307\t\n",
      "\n",
      "Wed Oct  2 08:56:20 2019\tEpoch:13[90/103],Iteration:1326\tLoss:20.5228\tTLoss:15.9759\t\n",
      "\n",
      "Wed Oct  2 08:56:37 2019\tEpoch:14[30/103],Iteration:1369\tLoss:21.4270\tTLoss:17.6309\t\n",
      "\n",
      "Wed Oct  2 08:56:49 2019\tEpoch:14[60/103],Iteration:1399\tLoss:15.6929\tTLoss:16.3519\t\n",
      "\n",
      "Wed Oct  2 08:57:00 2019\tEpoch:14[90/103],Iteration:1429\tLoss:20.4312\tTLoss:16.0323\t\n",
      "\n",
      "Wed Oct  2 08:57:17 2019\tEpoch:15[30/103],Iteration:1472\tLoss:13.5190\tTLoss:14.8651\t\n",
      "\n",
      "Wed Oct  2 08:57:29 2019\tEpoch:15[60/103],Iteration:1502\tLoss:13.7222\tTLoss:14.5355\t\n",
      "\n",
      "Wed Oct  2 08:57:40 2019\tEpoch:15[90/103],Iteration:1532\tLoss:13.9795\tTLoss:14.3991\t\n",
      "\n",
      "Wed Oct  2 08:57:57 2019\tEpoch:16[30/103],Iteration:1575\tLoss:11.8054\tTLoss:15.7256\t\n",
      "\n",
      "Wed Oct  2 08:58:09 2019\tEpoch:16[60/103],Iteration:1605\tLoss:7.0069\tTLoss:16.0721\t\n",
      "\n",
      "Wed Oct  2 08:58:20 2019\tEpoch:16[90/103],Iteration:1635\tLoss:13.2495\tTLoss:14.5192\t\n",
      "\n",
      "Wed Oct  2 08:58:38 2019\tEpoch:17[30/103],Iteration:1678\tLoss:5.2444\tTLoss:13.8845\t\n",
      "\n",
      "Wed Oct  2 08:58:49 2019\tEpoch:17[60/103],Iteration:1708\tLoss:26.9891\tTLoss:14.6376\t\n",
      "\n",
      "Wed Oct  2 08:59:01 2019\tEpoch:17[90/103],Iteration:1738\tLoss:10.0025\tTLoss:14.0278\t\n",
      "\n",
      "Wed Oct  2 08:59:18 2019\tEpoch:18[30/103],Iteration:1781\tLoss:13.1889\tTLoss:13.8317\t\n",
      "\n",
      "Wed Oct  2 08:59:29 2019\tEpoch:18[60/103],Iteration:1811\tLoss:13.1810\tTLoss:14.3111\t\n",
      "\n",
      "Wed Oct  2 08:59:41 2019\tEpoch:18[90/103],Iteration:1841\tLoss:4.1140\tTLoss:13.8037\t\n",
      "\n",
      "Wed Oct  2 08:59:58 2019\tEpoch:19[30/103],Iteration:1884\tLoss:5.9036\tTLoss:12.1154\t\n",
      "\n",
      "Wed Oct  2 09:00:10 2019\tEpoch:19[60/103],Iteration:1914\tLoss:8.1270\tTLoss:12.0488\t\n",
      "\n",
      "Wed Oct  2 09:00:21 2019\tEpoch:19[90/103],Iteration:1944\tLoss:11.2072\tTLoss:12.2468\t\n",
      "\n",
      "Wed Oct  2 09:00:38 2019\tEpoch:20[30/103],Iteration:1987\tLoss:11.5100\tTLoss:12.2613\t\n",
      "\n",
      "Wed Oct  2 09:00:50 2019\tEpoch:20[60/103],Iteration:2017\tLoss:9.2071\tTLoss:11.5397\t\n",
      "\n",
      "Wed Oct  2 09:01:01 2019\tEpoch:20[90/103],Iteration:2047\tLoss:14.6611\tTLoss:11.1795\t\n",
      "\n",
      "Wed Oct  2 09:01:19 2019\tEpoch:21[30/103],Iteration:2090\tLoss:8.7909\tTLoss:13.1767\t\n",
      "\n",
      "Wed Oct  2 09:01:30 2019\tEpoch:21[60/103],Iteration:2120\tLoss:7.4696\tTLoss:12.2289\t\n",
      "\n",
      "Wed Oct  2 09:01:41 2019\tEpoch:21[90/103],Iteration:2150\tLoss:4.5442\tTLoss:11.8792\t\n",
      "\n",
      "Wed Oct  2 09:01:59 2019\tEpoch:22[30/103],Iteration:2193\tLoss:4.3483\tTLoss:10.7465\t\n",
      "\n",
      "Wed Oct  2 09:02:10 2019\tEpoch:22[60/103],Iteration:2223\tLoss:3.2447\tTLoss:10.2431\t\n",
      "\n",
      "Wed Oct  2 09:02:22 2019\tEpoch:22[90/103],Iteration:2253\tLoss:5.9109\tTLoss:10.4871\t\n",
      "\n",
      "Wed Oct  2 09:02:39 2019\tEpoch:23[30/103],Iteration:2296\tLoss:10.0616\tTLoss:9.6359\t\n",
      "\n",
      "Wed Oct  2 09:02:50 2019\tEpoch:23[60/103],Iteration:2326\tLoss:19.5078\tTLoss:10.0882\t\n",
      "\n",
      "Wed Oct  2 09:03:02 2019\tEpoch:23[90/103],Iteration:2356\tLoss:3.7269\tTLoss:9.8806\t\n",
      "\n",
      "Wed Oct  2 09:03:19 2019\tEpoch:24[30/103],Iteration:2399\tLoss:3.4745\tTLoss:11.3264\t\n",
      "\n",
      "Wed Oct  2 09:03:30 2019\tEpoch:24[60/103],Iteration:2429\tLoss:17.5514\tTLoss:11.2873\t\n",
      "\n",
      "Wed Oct  2 09:03:42 2019\tEpoch:24[90/103],Iteration:2459\tLoss:8.0913\tTLoss:10.6300\t\n",
      "\n",
      "Wed Oct  2 09:03:59 2019\tEpoch:25[30/103],Iteration:2502\tLoss:8.9409\tTLoss:9.3896\t\n",
      "\n",
      "Wed Oct  2 09:04:10 2019\tEpoch:25[60/103],Iteration:2532\tLoss:9.0288\tTLoss:8.9391\t\n",
      "\n",
      "Wed Oct  2 09:04:22 2019\tEpoch:25[90/103],Iteration:2562\tLoss:15.3971\tTLoss:9.1173\t\n",
      "\n",
      "Wed Oct  2 09:04:39 2019\tEpoch:26[30/103],Iteration:2605\tLoss:17.0183\tTLoss:9.6301\t\n",
      "\n",
      "Wed Oct  2 09:04:51 2019\tEpoch:26[60/103],Iteration:2635\tLoss:12.7521\tTLoss:9.5663\t\n",
      "\n",
      "Wed Oct  2 09:05:03 2019\tEpoch:26[90/103],Iteration:2665\tLoss:7.4629\tTLoss:9.6426\t\n",
      "\n",
      "Wed Oct  2 09:05:20 2019\tEpoch:27[30/103],Iteration:2708\tLoss:5.7530\tTLoss:8.7198\t\n",
      "\n",
      "Wed Oct  2 09:05:31 2019\tEpoch:27[60/103],Iteration:2738\tLoss:11.7818\tTLoss:9.0622\t\n",
      "\n",
      "Wed Oct  2 09:05:43 2019\tEpoch:27[90/103],Iteration:2768\tLoss:10.7869\tTLoss:8.6532\t\n",
      "\n",
      "Wed Oct  2 09:06:00 2019\tEpoch:28[30/103],Iteration:2811\tLoss:0.8928\tTLoss:10.4084\t\n",
      "\n",
      "Wed Oct  2 09:06:12 2019\tEpoch:28[60/103],Iteration:2841\tLoss:7.1985\tTLoss:9.1480\t\n",
      "\n",
      "Wed Oct  2 09:06:23 2019\tEpoch:28[90/103],Iteration:2871\tLoss:1.3539\tTLoss:9.1981\t\n",
      "\n",
      "Wed Oct  2 09:06:40 2019\tEpoch:29[30/103],Iteration:2914\tLoss:14.4418\tTLoss:9.2577\t\n",
      "\n",
      "Wed Oct  2 09:06:52 2019\tEpoch:29[60/103],Iteration:2944\tLoss:5.3905\tTLoss:9.1360\t\n",
      "\n",
      "Wed Oct  2 09:07:03 2019\tEpoch:29[90/103],Iteration:2974\tLoss:10.3040\tTLoss:9.5135\t\n",
      "\n",
      "Wed Oct  2 09:07:21 2019\tEpoch:30[30/103],Iteration:3017\tLoss:9.3867\tTLoss:7.9611\t\n",
      "\n",
      "Wed Oct  2 09:07:32 2019\tEpoch:30[60/103],Iteration:3047\tLoss:13.1753\tTLoss:8.8273\t\n",
      "\n",
      "Wed Oct  2 09:07:44 2019\tEpoch:30[90/103],Iteration:3077\tLoss:20.6811\tTLoss:9.4383\t\n",
      "\n",
      "Wed Oct  2 09:08:01 2019\tEpoch:31[30/103],Iteration:3120\tLoss:9.4033\tTLoss:9.1598\t\n",
      "\n",
      "Wed Oct  2 09:08:13 2019\tEpoch:31[60/103],Iteration:3150\tLoss:8.1405\tTLoss:8.7502\t\n",
      "\n",
      "Wed Oct  2 09:08:24 2019\tEpoch:31[90/103],Iteration:3180\tLoss:4.8829\tTLoss:8.6373\t\n",
      "\n",
      "Wed Oct  2 09:08:42 2019\tEpoch:32[30/103],Iteration:3223\tLoss:12.7416\tTLoss:8.8018\t\n",
      "\n",
      "Wed Oct  2 09:08:53 2019\tEpoch:32[60/103],Iteration:3253\tLoss:15.8901\tTLoss:8.9363\t\n",
      "\n",
      "Wed Oct  2 09:09:05 2019\tEpoch:32[90/103],Iteration:3283\tLoss:3.1608\tTLoss:8.5479\t\n",
      "\n",
      "Wed Oct  2 09:09:22 2019\tEpoch:33[30/103],Iteration:3326\tLoss:7.8565\tTLoss:6.5182\t\n",
      "\n",
      "Wed Oct  2 09:09:33 2019\tEpoch:33[60/103],Iteration:3356\tLoss:10.6441\tTLoss:7.1028\t\n",
      "\n",
      "Wed Oct  2 09:09:45 2019\tEpoch:33[90/103],Iteration:3386\tLoss:1.0953\tTLoss:7.4891\t\n",
      "\n",
      "Wed Oct  2 09:10:02 2019\tEpoch:34[30/103],Iteration:3429\tLoss:4.4465\tTLoss:8.4235\t\n",
      "\n",
      "Wed Oct  2 09:10:14 2019\tEpoch:34[60/103],Iteration:3459\tLoss:26.3219\tTLoss:8.9801\t\n",
      "\n",
      "Wed Oct  2 09:10:25 2019\tEpoch:34[90/103],Iteration:3489\tLoss:5.1418\tTLoss:8.1468\t\n",
      "\n",
      "Wed Oct  2 09:10:42 2019\tEpoch:35[30/103],Iteration:3532\tLoss:16.6586\tTLoss:6.8557\t\n",
      "\n",
      "Wed Oct  2 09:10:54 2019\tEpoch:35[60/103],Iteration:3562\tLoss:5.2196\tTLoss:7.8916\t\n",
      "\n",
      "Wed Oct  2 09:11:06 2019\tEpoch:35[90/103],Iteration:3592\tLoss:7.8051\tTLoss:8.0662\t\n",
      "\n",
      "Wed Oct  2 09:11:23 2019\tEpoch:36[30/103],Iteration:3635\tLoss:19.5397\tTLoss:9.1290\t\n",
      "\n",
      "Wed Oct  2 09:11:34 2019\tEpoch:36[60/103],Iteration:3665\tLoss:9.8250\tTLoss:8.8659\t\n",
      "\n",
      "Wed Oct  2 09:11:46 2019\tEpoch:36[90/103],Iteration:3695\tLoss:15.3050\tTLoss:7.9955\t\n",
      "\n",
      "Wed Oct  2 09:12:03 2019\tEpoch:37[30/103],Iteration:3738\tLoss:18.9323\tTLoss:10.1195\t\n",
      "\n",
      "Wed Oct  2 09:12:14 2019\tEpoch:37[60/103],Iteration:3768\tLoss:0.9247\tTLoss:9.8014\t\n",
      "\n",
      "Wed Oct  2 09:12:26 2019\tEpoch:37[90/103],Iteration:3798\tLoss:2.3713\tTLoss:9.2426\t\n",
      "\n",
      "Wed Oct  2 09:12:43 2019\tEpoch:38[30/103],Iteration:3841\tLoss:8.3954\tTLoss:6.7048\t\n",
      "\n",
      "Wed Oct  2 09:12:55 2019\tEpoch:38[60/103],Iteration:3871\tLoss:12.1003\tTLoss:7.5746\t\n",
      "\n",
      "Wed Oct  2 09:13:07 2019\tEpoch:38[90/103],Iteration:3901\tLoss:4.8564\tTLoss:7.4503\t\n",
      "\n",
      "Wed Oct  2 09:13:24 2019\tEpoch:39[30/103],Iteration:3944\tLoss:7.6284\tTLoss:8.2143\t\n",
      "\n",
      "Wed Oct  2 09:13:36 2019\tEpoch:39[60/103],Iteration:3974\tLoss:2.8641\tTLoss:7.9926\t\n",
      "\n",
      "Wed Oct  2 09:13:47 2019\tEpoch:39[90/103],Iteration:4004\tLoss:0.4250\tTLoss:7.5483\t\n",
      "\n",
      "Wed Oct  2 09:14:04 2019\tEpoch:40[30/103],Iteration:4047\tLoss:13.5164\tTLoss:7.0683\t\n",
      "\n",
      "Wed Oct  2 09:14:16 2019\tEpoch:40[60/103],Iteration:4077\tLoss:3.4462\tTLoss:7.3598\t\n",
      "\n",
      "Wed Oct  2 09:14:28 2019\tEpoch:40[90/103],Iteration:4107\tLoss:9.6992\tTLoss:7.6964\t\n",
      "\n",
      "Wed Oct  2 09:14:45 2019\tEpoch:41[30/103],Iteration:4150\tLoss:0.9912\tTLoss:5.8060\t\n",
      "\n",
      "Wed Oct  2 09:14:57 2019\tEpoch:41[60/103],Iteration:4180\tLoss:0.6005\tTLoss:6.0017\t\n",
      "\n",
      "Wed Oct  2 09:15:08 2019\tEpoch:41[90/103],Iteration:4210\tLoss:10.2097\tTLoss:6.3442\t\n",
      "\n",
      "Wed Oct  2 09:15:25 2019\tEpoch:42[30/103],Iteration:4253\tLoss:8.6098\tTLoss:7.2825\t\n",
      "\n",
      "Wed Oct  2 09:15:37 2019\tEpoch:42[60/103],Iteration:4283\tLoss:3.3901\tTLoss:7.6021\t\n",
      "\n",
      "Wed Oct  2 09:15:48 2019\tEpoch:42[90/103],Iteration:4313\tLoss:2.1591\tTLoss:7.9848\t\n",
      "\n",
      "Wed Oct  2 09:16:06 2019\tEpoch:43[30/103],Iteration:4356\tLoss:2.7188\tTLoss:6.9917\t\n",
      "\n",
      "Wed Oct  2 09:16:17 2019\tEpoch:43[60/103],Iteration:4386\tLoss:9.9682\tTLoss:7.5853\t\n",
      "\n",
      "Wed Oct  2 09:16:29 2019\tEpoch:43[90/103],Iteration:4416\tLoss:2.0774\tTLoss:7.4418\t\n",
      "\n",
      "Wed Oct  2 09:16:46 2019\tEpoch:44[30/103],Iteration:4459\tLoss:11.1372\tTLoss:7.4488\t\n",
      "\n",
      "Wed Oct  2 09:16:58 2019\tEpoch:44[60/103],Iteration:4489\tLoss:4.1340\tTLoss:6.9680\t\n",
      "\n",
      "Wed Oct  2 09:17:09 2019\tEpoch:44[90/103],Iteration:4519\tLoss:6.3545\tTLoss:6.7614\t\n",
      "\n",
      "Wed Oct  2 09:17:26 2019\tEpoch:45[30/103],Iteration:4562\tLoss:11.2011\tTLoss:7.3605\t\n",
      "\n",
      "Wed Oct  2 09:17:38 2019\tEpoch:45[60/103],Iteration:4592\tLoss:7.2925\tTLoss:7.4743\t\n",
      "\n",
      "Wed Oct  2 09:17:50 2019\tEpoch:45[90/103],Iteration:4622\tLoss:7.3890\tTLoss:7.2903\t\n",
      "\n",
      "Wed Oct  2 09:18:07 2019\tEpoch:46[30/103],Iteration:4665\tLoss:6.2547\tTLoss:6.5866\t\n",
      "\n",
      "Wed Oct  2 09:18:19 2019\tEpoch:46[60/103],Iteration:4695\tLoss:2.1968\tTLoss:6.8089\t\n",
      "\n",
      "Wed Oct  2 09:18:30 2019\tEpoch:46[90/103],Iteration:4725\tLoss:4.1502\tTLoss:6.9879\t\n",
      "\n",
      "Wed Oct  2 09:18:47 2019\tEpoch:47[30/103],Iteration:4768\tLoss:6.0324\tTLoss:6.6030\t\n",
      "\n",
      "Wed Oct  2 09:18:59 2019\tEpoch:47[60/103],Iteration:4798\tLoss:3.0326\tTLoss:6.5434\t\n",
      "\n",
      "Wed Oct  2 09:19:11 2019\tEpoch:47[90/103],Iteration:4828\tLoss:7.9654\tTLoss:6.7680\t\n",
      "\n",
      "Wed Oct  2 09:19:28 2019\tEpoch:48[30/103],Iteration:4871\tLoss:3.7629\tTLoss:4.5708\t\n",
      "\n",
      "Wed Oct  2 09:19:40 2019\tEpoch:48[60/103],Iteration:4901\tLoss:4.5778\tTLoss:5.8136\t\n",
      "\n",
      "Wed Oct  2 09:19:51 2019\tEpoch:48[90/103],Iteration:4931\tLoss:8.1110\tTLoss:6.1642\t\n",
      "\n",
      "Wed Oct  2 09:20:08 2019\tEpoch:49[30/103],Iteration:4974\tLoss:8.6073\tTLoss:6.8996\t\n",
      "\n",
      "Wed Oct  2 09:20:20 2019\tEpoch:49[60/103],Iteration:5004\tLoss:10.6636\tTLoss:6.5884\t\n",
      "\n",
      "Wed Oct  2 09:20:32 2019\tEpoch:49[90/103],Iteration:5034\tLoss:11.2910\tTLoss:6.7162\t\n",
      "\n",
      "Wed Oct  2 09:20:49 2019\tEpoch:50[30/103],Iteration:5077\tLoss:16.8668\tTLoss:6.4017\t\n",
      "\n",
      "Wed Oct  2 09:21:00 2019\tEpoch:50[60/103],Iteration:5107\tLoss:6.4982\tTLoss:6.2827\t\n",
      "\n",
      "Wed Oct  2 09:21:12 2019\tEpoch:50[90/103],Iteration:5137\tLoss:4.7879\tTLoss:6.6116\t\n",
      "\n",
      "Wed Oct  2 09:21:29 2019\tEpoch:51[30/103],Iteration:5180\tLoss:12.6618\tTLoss:7.0407\t\n",
      "\n",
      "Wed Oct  2 09:21:40 2019\tEpoch:51[60/103],Iteration:5210\tLoss:5.6616\tTLoss:7.1546\t\n",
      "\n",
      "Wed Oct  2 09:21:52 2019\tEpoch:51[90/103],Iteration:5240\tLoss:9.9184\tTLoss:7.3075\t\n",
      "\n",
      "Wed Oct  2 09:22:09 2019\tEpoch:52[30/103],Iteration:5283\tLoss:13.0550\tTLoss:6.3170\t\n",
      "\n",
      "Wed Oct  2 09:22:21 2019\tEpoch:52[60/103],Iteration:5313\tLoss:6.8880\tTLoss:7.1120\t\n",
      "\n",
      "Wed Oct  2 09:22:32 2019\tEpoch:52[90/103],Iteration:5343\tLoss:0.5955\tTLoss:6.6985\t\n",
      "\n",
      "Wed Oct  2 09:22:50 2019\tEpoch:53[30/103],Iteration:5386\tLoss:6.3303\tTLoss:6.7326\t\n",
      "\n",
      "Wed Oct  2 09:23:01 2019\tEpoch:53[60/103],Iteration:5416\tLoss:6.7808\tTLoss:6.3490\t\n",
      "\n",
      "Wed Oct  2 09:23:13 2019\tEpoch:53[90/103],Iteration:5446\tLoss:1.8201\tTLoss:5.9762\t\n",
      "\n",
      "Wed Oct  2 09:23:31 2019\tEpoch:54[30/103],Iteration:5489\tLoss:3.0764\tTLoss:5.4221\t\n",
      "\n",
      "Wed Oct  2 09:23:42 2019\tEpoch:54[60/103],Iteration:5519\tLoss:7.3472\tTLoss:5.7445\t\n",
      "\n",
      "Wed Oct  2 09:23:54 2019\tEpoch:54[90/103],Iteration:5549\tLoss:5.6066\tTLoss:6.2408\t\n",
      "\n",
      "Wed Oct  2 09:24:11 2019\tEpoch:55[30/103],Iteration:5592\tLoss:5.5037\tTLoss:6.3101\t\n",
      "\n",
      "Wed Oct  2 09:24:22 2019\tEpoch:55[60/103],Iteration:5622\tLoss:4.0014\tTLoss:5.8145\t\n",
      "\n",
      "Wed Oct  2 09:24:34 2019\tEpoch:55[90/103],Iteration:5652\tLoss:7.2987\tTLoss:5.3735\t\n",
      "\n",
      "Wed Oct  2 09:24:51 2019\tEpoch:56[30/103],Iteration:5695\tLoss:0.8850\tTLoss:6.3466\t\n",
      "\n",
      "Wed Oct  2 09:25:03 2019\tEpoch:56[60/103],Iteration:5725\tLoss:4.9086\tTLoss:5.7234\t\n",
      "\n",
      "Wed Oct  2 09:25:14 2019\tEpoch:56[90/103],Iteration:5755\tLoss:4.9673\tTLoss:6.4462\t\n",
      "\n",
      "Wed Oct  2 09:25:32 2019\tEpoch:57[30/103],Iteration:5798\tLoss:13.0367\tTLoss:5.8671\t\n",
      "\n",
      "Wed Oct  2 09:25:44 2019\tEpoch:57[60/103],Iteration:5828\tLoss:4.3800\tTLoss:5.5629\t\n",
      "\n",
      "Wed Oct  2 09:25:55 2019\tEpoch:57[90/103],Iteration:5858\tLoss:6.4923\tTLoss:5.5268\t\n",
      "\n",
      "Wed Oct  2 09:26:13 2019\tEpoch:58[30/103],Iteration:5901\tLoss:10.1371\tTLoss:6.6269\t\n",
      "\n",
      "Wed Oct  2 09:26:24 2019\tEpoch:58[60/103],Iteration:5931\tLoss:11.7503\tTLoss:6.6873\t\n",
      "\n",
      "Wed Oct  2 09:26:36 2019\tEpoch:58[90/103],Iteration:5961\tLoss:3.8490\tTLoss:6.3149\t\n",
      "\n",
      "Wed Oct  2 09:26:53 2019\tEpoch:59[30/103],Iteration:6004\tLoss:5.1160\tTLoss:5.9715\t\n",
      "\n",
      "Wed Oct  2 09:27:05 2019\tEpoch:59[60/103],Iteration:6034\tLoss:5.3054\tTLoss:6.1726\t\n",
      "\n",
      "Wed Oct  2 09:27:16 2019\tEpoch:59[90/103],Iteration:6064\tLoss:4.6733\tTLoss:5.7876\t\n",
      "\n",
      "Wed Oct  2 09:27:34 2019\tEpoch:60[30/103],Iteration:6107\tLoss:2.8303\tTLoss:5.8046\t\n",
      "\n",
      "Wed Oct  2 09:27:45 2019\tEpoch:60[60/103],Iteration:6137\tLoss:11.7619\tTLoss:5.5797\t\n",
      "\n",
      "Wed Oct  2 09:27:57 2019\tEpoch:60[90/103],Iteration:6167\tLoss:0.7544\tTLoss:5.5798\t\n",
      "\n",
      "Wed Oct  2 09:28:14 2019\tEpoch:61[30/103],Iteration:6210\tLoss:2.2454\tTLoss:5.8028\t\n",
      "\n",
      "Wed Oct  2 09:28:26 2019\tEpoch:61[60/103],Iteration:6240\tLoss:0.1235\tTLoss:5.3797\t\n",
      "\n",
      "Wed Oct  2 09:28:37 2019\tEpoch:61[90/103],Iteration:6270\tLoss:5.1011\tTLoss:5.5235\t\n",
      "\n",
      "Wed Oct  2 09:28:55 2019\tEpoch:62[30/103],Iteration:6313\tLoss:4.6194\tTLoss:5.9413\t\n",
      "\n",
      "Wed Oct  2 09:29:06 2019\tEpoch:62[60/103],Iteration:6343\tLoss:14.7357\tTLoss:5.8713\t\n",
      "\n",
      "Wed Oct  2 09:29:18 2019\tEpoch:62[90/103],Iteration:6373\tLoss:5.5325\tTLoss:6.0271\t\n",
      "\n",
      "Wed Oct  2 09:29:35 2019\tEpoch:63[30/103],Iteration:6416\tLoss:2.5533\tTLoss:5.9958\t\n",
      "\n",
      "Wed Oct  2 09:29:47 2019\tEpoch:63[60/103],Iteration:6446\tLoss:4.9078\tTLoss:5.8006\t\n",
      "\n",
      "Wed Oct  2 09:29:58 2019\tEpoch:63[90/103],Iteration:6476\tLoss:5.3325\tTLoss:5.4670\t\n",
      "\n",
      "Wed Oct  2 09:30:15 2019\tEpoch:64[30/103],Iteration:6519\tLoss:3.2487\tTLoss:4.9893\t\n",
      "\n",
      "Wed Oct  2 09:30:27 2019\tEpoch:64[60/103],Iteration:6549\tLoss:9.1539\tTLoss:5.4621\t\n",
      "\n",
      "Wed Oct  2 09:30:38 2019\tEpoch:64[90/103],Iteration:6579\tLoss:1.6864\tTLoss:5.7202\t\n",
      "\n",
      "Wed Oct  2 09:30:56 2019\tEpoch:65[30/103],Iteration:6622\tLoss:8.7338\tTLoss:5.6565\t\n",
      "\n",
      "Wed Oct  2 09:31:07 2019\tEpoch:65[60/103],Iteration:6652\tLoss:0.5461\tTLoss:5.4615\t\n",
      "\n",
      "Wed Oct  2 09:31:19 2019\tEpoch:65[90/103],Iteration:6682\tLoss:4.6213\tTLoss:5.6215\t\n",
      "\n",
      "Wed Oct  2 09:31:36 2019\tEpoch:66[30/103],Iteration:6725\tLoss:7.8697\tTLoss:5.3084\t\n",
      "\n",
      "Wed Oct  2 09:31:48 2019\tEpoch:66[60/103],Iteration:6755\tLoss:6.5785\tTLoss:5.3860\t\n",
      "\n",
      "Wed Oct  2 09:31:59 2019\tEpoch:66[90/103],Iteration:6785\tLoss:4.4063\tTLoss:5.4617\t\n",
      "\n",
      "Wed Oct  2 09:32:16 2019\tEpoch:67[30/103],Iteration:6828\tLoss:1.0644\tTLoss:4.7575\t\n",
      "\n",
      "Wed Oct  2 09:32:28 2019\tEpoch:67[60/103],Iteration:6858\tLoss:5.1812\tTLoss:4.9996\t\n",
      "\n",
      "Wed Oct  2 09:32:39 2019\tEpoch:67[90/103],Iteration:6888\tLoss:1.5995\tTLoss:5.5056\t\n",
      "\n",
      "Wed Oct  2 09:32:57 2019\tEpoch:68[30/103],Iteration:6931\tLoss:1.2510\tTLoss:5.0826\t\n",
      "\n",
      "Wed Oct  2 09:33:08 2019\tEpoch:68[60/103],Iteration:6961\tLoss:1.3818\tTLoss:5.5087\t\n",
      "\n",
      "Wed Oct  2 09:33:20 2019\tEpoch:68[90/103],Iteration:6991\tLoss:0.0628\tTLoss:5.3165\t\n",
      "\n",
      "Wed Oct  2 09:33:37 2019\tEpoch:69[30/103],Iteration:7034\tLoss:9.2323\tTLoss:5.1181\t\n",
      "\n",
      "Wed Oct  2 09:33:49 2019\tEpoch:69[60/103],Iteration:7064\tLoss:0.3818\tTLoss:5.3663\t\n",
      "\n",
      "Wed Oct  2 09:34:01 2019\tEpoch:69[90/103],Iteration:7094\tLoss:3.0181\tTLoss:5.3252\t\n",
      "\n",
      "Wed Oct  2 09:34:18 2019\tEpoch:70[30/103],Iteration:7137\tLoss:19.4739\tTLoss:7.3878\t\n",
      "\n",
      "Wed Oct  2 09:34:29 2019\tEpoch:70[60/103],Iteration:7167\tLoss:0.6790\tTLoss:6.1537\t\n",
      "\n",
      "Wed Oct  2 09:34:41 2019\tEpoch:70[90/103],Iteration:7197\tLoss:5.2714\tTLoss:5.5214\t\n",
      "\n",
      "Wed Oct  2 09:34:58 2019\tEpoch:71[30/103],Iteration:7240\tLoss:3.8885\tTLoss:5.7141\t\n",
      "\n",
      "Wed Oct  2 09:35:10 2019\tEpoch:71[60/103],Iteration:7270\tLoss:1.5769\tTLoss:5.6120\t\n",
      "\n",
      "Wed Oct  2 09:35:21 2019\tEpoch:71[90/103],Iteration:7300\tLoss:7.9043\tTLoss:5.1550\t\n",
      "\n",
      "Wed Oct  2 09:35:38 2019\tEpoch:72[30/103],Iteration:7343\tLoss:0.7334\tTLoss:5.2226\t\n",
      "\n",
      "Wed Oct  2 09:35:50 2019\tEpoch:72[60/103],Iteration:7373\tLoss:3.8142\tTLoss:5.1217\t\n",
      "\n",
      "Wed Oct  2 09:36:01 2019\tEpoch:72[90/103],Iteration:7403\tLoss:9.6242\tTLoss:5.7638\t\n",
      "\n",
      "Wed Oct  2 09:36:18 2019\tEpoch:73[30/103],Iteration:7446\tLoss:6.2557\tTLoss:4.8650\t\n",
      "\n",
      "Wed Oct  2 09:36:30 2019\tEpoch:73[60/103],Iteration:7476\tLoss:8.8923\tTLoss:5.7420\t\n",
      "\n",
      "Wed Oct  2 09:36:42 2019\tEpoch:73[90/103],Iteration:7506\tLoss:7.3137\tTLoss:5.6057\t\n",
      "\n",
      "Wed Oct  2 09:36:59 2019\tEpoch:74[30/103],Iteration:7549\tLoss:0.6332\tTLoss:5.3523\t\n",
      "\n",
      "Wed Oct  2 09:37:10 2019\tEpoch:74[60/103],Iteration:7579\tLoss:8.2402\tTLoss:5.6109\t\n",
      "\n",
      "Wed Oct  2 09:37:22 2019\tEpoch:74[90/103],Iteration:7609\tLoss:10.9089\tTLoss:5.7762\t\n",
      "\n",
      "Wed Oct  2 09:37:39 2019\tEpoch:75[30/103],Iteration:7652\tLoss:13.6574\tTLoss:5.4366\t\n",
      "\n",
      "Wed Oct  2 09:37:51 2019\tEpoch:75[60/103],Iteration:7682\tLoss:8.1012\tTLoss:5.8407\t\n",
      "\n",
      "Wed Oct  2 09:38:02 2019\tEpoch:75[90/103],Iteration:7712\tLoss:9.0635\tTLoss:5.7546\t\n",
      "\n",
      "Wed Oct  2 09:38:19 2019\tEpoch:76[30/103],Iteration:7755\tLoss:9.2726\tTLoss:4.8710\t\n",
      "\n",
      "Wed Oct  2 09:38:31 2019\tEpoch:76[60/103],Iteration:7785\tLoss:0.3244\tTLoss:5.1477\t\n",
      "\n",
      "Wed Oct  2 09:38:43 2019\tEpoch:76[90/103],Iteration:7815\tLoss:4.9564\tTLoss:5.5304\t\n",
      "\n",
      "Wed Oct  2 09:39:00 2019\tEpoch:77[30/103],Iteration:7858\tLoss:13.9692\tTLoss:4.6600\t\n",
      "\n",
      "Wed Oct  2 09:39:11 2019\tEpoch:77[60/103],Iteration:7888\tLoss:10.1699\tTLoss:5.2293\t\n",
      "\n",
      "Wed Oct  2 09:39:23 2019\tEpoch:77[90/103],Iteration:7918\tLoss:3.5012\tTLoss:5.4243\t\n",
      "\n",
      "Wed Oct  2 09:39:40 2019\tEpoch:78[30/103],Iteration:7961\tLoss:5.7496\tTLoss:5.7072\t\n",
      "\n",
      "Wed Oct  2 09:39:52 2019\tEpoch:78[60/103],Iteration:7991\tLoss:11.6018\tTLoss:4.8137\t\n",
      "\n",
      "Wed Oct  2 09:40:03 2019\tEpoch:78[90/103],Iteration:8021\tLoss:7.3737\tTLoss:4.9122\t\n",
      "\n",
      "Wed Oct  2 09:40:20 2019\tEpoch:79[30/103],Iteration:8064\tLoss:0.4862\tTLoss:4.4045\t\n",
      "\n",
      "Wed Oct  2 09:40:32 2019\tEpoch:79[60/103],Iteration:8094\tLoss:1.9803\tTLoss:5.4794\t\n",
      "\n",
      "Wed Oct  2 09:40:44 2019\tEpoch:79[90/103],Iteration:8124\tLoss:3.5660\tTLoss:5.1618\t\n",
      "\n",
      "Wed Oct  2 09:41:01 2019\tEpoch:80[30/103],Iteration:8167\tLoss:12.5733\tTLoss:4.1297\t\n",
      "\n",
      "Wed Oct  2 09:41:13 2019\tEpoch:80[60/103],Iteration:8197\tLoss:0.5355\tTLoss:3.8508\t\n",
      "\n",
      "Wed Oct  2 09:41:24 2019\tEpoch:80[90/103],Iteration:8227\tLoss:2.9301\tTLoss:4.3030\t\n",
      "\n",
      "Wed Oct  2 09:41:41 2019\tEpoch:81[30/103],Iteration:8270\tLoss:5.4670\tTLoss:4.9176\t\n",
      "\n",
      "Wed Oct  2 09:41:53 2019\tEpoch:81[60/103],Iteration:8300\tLoss:2.7783\tTLoss:4.5572\t\n",
      "\n",
      "Wed Oct  2 09:42:05 2019\tEpoch:81[90/103],Iteration:8330\tLoss:0.3802\tTLoss:4.3848\t\n",
      "\n",
      "Wed Oct  2 09:42:22 2019\tEpoch:82[30/103],Iteration:8373\tLoss:8.5234\tTLoss:5.6229\t\n",
      "\n",
      "Wed Oct  2 09:42:34 2019\tEpoch:82[60/103],Iteration:8403\tLoss:3.1090\tTLoss:5.1224\t\n",
      "\n",
      "Wed Oct  2 09:42:45 2019\tEpoch:82[90/103],Iteration:8433\tLoss:1.0375\tTLoss:5.1001\t\n",
      "\n",
      "Wed Oct  2 09:43:03 2019\tEpoch:83[30/103],Iteration:8476\tLoss:3.0723\tTLoss:5.8412\t\n",
      "\n",
      "Wed Oct  2 09:43:14 2019\tEpoch:83[60/103],Iteration:8506\tLoss:2.6019\tTLoss:5.3491\t\n",
      "\n",
      "Wed Oct  2 09:43:26 2019\tEpoch:83[90/103],Iteration:8536\tLoss:1.0407\tTLoss:5.1837\t\n",
      "\n",
      "Wed Oct  2 09:43:43 2019\tEpoch:84[30/103],Iteration:8579\tLoss:0.2682\tTLoss:5.1478\t\n",
      "\n",
      "Wed Oct  2 09:43:55 2019\tEpoch:84[60/103],Iteration:8609\tLoss:0.4934\tTLoss:5.4899\t\n",
      "\n",
      "Wed Oct  2 09:44:07 2019\tEpoch:84[90/103],Iteration:8639\tLoss:11.3897\tTLoss:5.1254\t\n",
      "\n",
      "Wed Oct  2 09:44:24 2019\tEpoch:85[30/103],Iteration:8682\tLoss:5.5379\tTLoss:4.5112\t\n",
      "\n",
      "Wed Oct  2 09:44:35 2019\tEpoch:85[60/103],Iteration:8712\tLoss:2.0803\tTLoss:5.3959\t\n",
      "\n",
      "Wed Oct  2 09:44:47 2019\tEpoch:85[90/103],Iteration:8742\tLoss:5.8422\tTLoss:5.0861\t\n",
      "\n",
      "Wed Oct  2 09:45:04 2019\tEpoch:86[30/103],Iteration:8785\tLoss:0.2550\tTLoss:4.6971\t\n",
      "\n",
      "Wed Oct  2 09:45:16 2019\tEpoch:86[60/103],Iteration:8815\tLoss:4.9658\tTLoss:4.2832\t\n",
      "\n",
      "Wed Oct  2 09:45:28 2019\tEpoch:86[90/103],Iteration:8845\tLoss:14.6897\tTLoss:4.0464\t\n",
      "\n",
      "Wed Oct  2 09:45:45 2019\tEpoch:87[30/103],Iteration:8888\tLoss:5.7369\tTLoss:5.2779\t\n",
      "\n",
      "Wed Oct  2 09:45:57 2019\tEpoch:87[60/103],Iteration:8918\tLoss:16.5193\tTLoss:5.2666\t\n",
      "\n",
      "Wed Oct  2 09:46:08 2019\tEpoch:87[90/103],Iteration:8948\tLoss:1.3402\tTLoss:5.4317\t\n",
      "\n",
      "Wed Oct  2 09:46:25 2019\tEpoch:88[30/103],Iteration:8991\tLoss:1.6798\tTLoss:5.9057\t\n",
      "\n",
      "Wed Oct  2 09:46:37 2019\tEpoch:88[60/103],Iteration:9021\tLoss:8.1379\tTLoss:5.4522\t\n",
      "\n",
      "Wed Oct  2 09:46:48 2019\tEpoch:88[90/103],Iteration:9051\tLoss:6.7863\tTLoss:5.1049\t\n",
      "\n",
      "Wed Oct  2 09:47:05 2019\tEpoch:89[30/103],Iteration:9094\tLoss:0.1339\tTLoss:4.8165\t\n",
      "\n",
      "Wed Oct  2 09:47:17 2019\tEpoch:89[60/103],Iteration:9124\tLoss:5.9108\tTLoss:3.8222\t\n",
      "\n",
      "Wed Oct  2 09:47:29 2019\tEpoch:89[90/103],Iteration:9154\tLoss:1.1165\tTLoss:4.2401\t\n",
      "\n",
      "Wed Oct  2 09:47:46 2019\tEpoch:90[30/103],Iteration:9197\tLoss:2.7414\tTLoss:4.9490\t\n",
      "\n",
      "Wed Oct  2 09:47:57 2019\tEpoch:90[60/103],Iteration:9227\tLoss:2.8653\tTLoss:5.4728\t\n",
      "\n",
      "Wed Oct  2 09:48:09 2019\tEpoch:90[90/103],Iteration:9257\tLoss:1.4544\tTLoss:5.3459\t\n",
      "\n",
      "Wed Oct  2 09:48:26 2019\tEpoch:91[30/103],Iteration:9300\tLoss:5.5866\tTLoss:3.6067\t\n",
      "\n",
      "Wed Oct  2 09:48:38 2019\tEpoch:91[60/103],Iteration:9330\tLoss:11.3408\tTLoss:3.9091\t\n",
      "\n",
      "Wed Oct  2 09:48:49 2019\tEpoch:91[90/103],Iteration:9360\tLoss:5.3429\tTLoss:4.3632\t\n",
      "\n",
      "Wed Oct  2 09:49:06 2019\tEpoch:92[30/103],Iteration:9403\tLoss:10.1522\tTLoss:5.3699\t\n",
      "\n",
      "Wed Oct  2 09:49:18 2019\tEpoch:92[60/103],Iteration:9433\tLoss:2.4302\tTLoss:5.3570\t\n",
      "\n",
      "Wed Oct  2 09:49:30 2019\tEpoch:92[90/103],Iteration:9463\tLoss:3.5248\tTLoss:5.1718\t\n",
      "\n",
      "Wed Oct  2 09:49:47 2019\tEpoch:93[30/103],Iteration:9506\tLoss:7.0005\tTLoss:5.2023\t\n",
      "\n",
      "Wed Oct  2 09:49:58 2019\tEpoch:93[60/103],Iteration:9536\tLoss:4.6031\tTLoss:5.0019\t\n",
      "\n",
      "Wed Oct  2 09:50:10 2019\tEpoch:93[90/103],Iteration:9566\tLoss:3.9776\tTLoss:4.5630\t\n",
      "\n",
      "Wed Oct  2 09:50:27 2019\tEpoch:94[30/103],Iteration:9609\tLoss:0.6133\tTLoss:5.2758\t\n",
      "\n",
      "Wed Oct  2 09:50:39 2019\tEpoch:94[60/103],Iteration:9639\tLoss:3.5977\tTLoss:4.8003\t\n",
      "\n",
      "Wed Oct  2 09:50:50 2019\tEpoch:94[90/103],Iteration:9669\tLoss:5.3722\tTLoss:4.6525\t\n",
      "\n",
      "Wed Oct  2 09:51:08 2019\tEpoch:95[30/103],Iteration:9712\tLoss:0.0945\tTLoss:3.0641\t\n",
      "\n",
      "Wed Oct  2 09:51:19 2019\tEpoch:95[60/103],Iteration:9742\tLoss:4.0282\tTLoss:3.3982\t\n",
      "\n",
      "Wed Oct  2 09:51:31 2019\tEpoch:95[90/103],Iteration:9772\tLoss:3.3575\tTLoss:3.4052\t\n",
      "\n",
      "Wed Oct  2 09:51:48 2019\tEpoch:96[30/103],Iteration:9815\tLoss:6.6447\tTLoss:5.7763\t\n",
      "\n",
      "Wed Oct  2 09:52:00 2019\tEpoch:96[60/103],Iteration:9845\tLoss:3.1446\tTLoss:4.9731\t\n",
      "\n",
      "Wed Oct  2 09:52:12 2019\tEpoch:96[90/103],Iteration:9875\tLoss:1.2139\tTLoss:4.7041\t\n",
      "\n",
      "Wed Oct  2 09:52:29 2019\tEpoch:97[30/103],Iteration:9918\tLoss:0.0345\tTLoss:2.8355\t\n",
      "\n",
      "Wed Oct  2 09:52:41 2019\tEpoch:97[60/103],Iteration:9948\tLoss:7.4900\tTLoss:3.7700\t\n",
      "\n",
      "Wed Oct  2 09:52:52 2019\tEpoch:97[90/103],Iteration:9978\tLoss:1.2228\tTLoss:3.9043\t\n",
      "\n",
      "Wed Oct  2 09:53:09 2019\tEpoch:98[30/103],Iteration:10021\tLoss:3.2093\tTLoss:3.7046\t\n",
      "\n",
      "Wed Oct  2 09:53:21 2019\tEpoch:98[60/103],Iteration:10051\tLoss:2.0630\tTLoss:4.4385\t\n",
      "\n",
      "Wed Oct  2 09:53:32 2019\tEpoch:98[90/103],Iteration:10081\tLoss:2.7604\tTLoss:4.5137\t\n",
      "\n",
      "Wed Oct  2 09:53:49 2019\tEpoch:99[30/103],Iteration:10124\tLoss:2.0142\tTLoss:4.3907\t\n",
      "\n",
      "Wed Oct  2 09:54:01 2019\tEpoch:99[60/103],Iteration:10154\tLoss:2.0202\tTLoss:4.1363\t\n",
      "\n",
      "Wed Oct  2 09:54:13 2019\tEpoch:99[90/103],Iteration:10184\tLoss:3.6872\tTLoss:4.0958\t\n",
      "\n",
      "Wed Oct  2 09:54:30 2019\tEpoch:100[30/103],Iteration:10227\tLoss:14.8068\tTLoss:4.6287\t\n",
      "\n",
      "Wed Oct  2 09:54:41 2019\tEpoch:100[60/103],Iteration:10257\tLoss:5.5724\tTLoss:4.9106\t\n",
      "\n",
      "Wed Oct  2 09:54:53 2019\tEpoch:100[90/103],Iteration:10287\tLoss:2.3860\tTLoss:4.9113\t\n",
      "\n",
      "\n",
      "Done, trained model saved at speech_id_checkpoint/final_epoch_100_batch_id_103.model\n"
     ]
    }
   ],
   "source": [
    "import time as time\n",
    "from torch.utils.data import DataLoader\n",
    "def train(model_path,dict1):\n",
    "    device = torch.device(dict1['device'])\n",
    "    device=dict1['device']\n",
    "    if dict1['data_preprocessed']:\n",
    "        train_dataset = SpeakerDatasetTIMITPreprocessed(dict1)\n",
    "    else:\n",
    "        train_dataset = SpeakerDatasetTIMIT(dict1)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=dict1['N'], shuffle=True, num_workers=dict1['num_workers'], drop_last=True) \n",
    "    print(train_loader)\n",
    "    embedder_net = SpeechEmbedder(dict1).to(device)\n",
    "    if dict1['restore']:\n",
    "        embedder_net.load_state_dict(torch.load(model_path))\n",
    "    ge2e_loss = GE2ELoss(device)\n",
    "    #Both net and loss have trainable parameters\n",
    "    optimizer = torch.optim.SGD([\n",
    "                    {'params': embedder_net.parameters()},\n",
    "                    {'params': ge2e_loss.parameters()}\n",
    "                ], lr=dict1['lr'])\n",
    "    \n",
    "    os.makedirs(dict1['checkpoint_dir'], exist_ok=True)\n",
    "    \n",
    "    embedder_net.train()\n",
    "    iteration = 0\n",
    "    for e in range(100):\n",
    "        total_loss = 0\n",
    "        for batch_id, mel_db_batch in enumerate(train_loader): \n",
    "            mel_db_batch = mel_db_batch.to(device)\n",
    "            \n",
    "            mel_db_batch = torch.reshape(mel_db_batch, (dict1['N']*dict1['M'], mel_db_batch.size(2), mel_db_batch.size(3)))\n",
    "            perm = random.sample(range(0, dict1['N']*dict1['M']),dict1['N']*dict1['M'])\n",
    "            unperm = list(perm)\n",
    "            for i,j in enumerate(perm):\n",
    "                unperm[j] = i\n",
    "            mel_db_batch = mel_db_batch[perm]\n",
    "            #gradient accumulates\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            embeddings = embedder_net(mel_db_batch)\n",
    "            embeddings = embeddings[unperm]\n",
    "            embeddings = torch.reshape(embeddings, (dict1['N'],dict1['M'], embeddings.size(1)))\n",
    "            \n",
    "            #get loss, call backward, step optimizer\n",
    "            loss = ge2e_loss(embeddings) #wants (Speaker, Utterances, embedding)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(embedder_net.parameters(), 3.0)\n",
    "            torch.nn.utils.clip_grad_norm_(ge2e_loss.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss = total_loss + loss\n",
    "            iteration += 1\n",
    "            if (batch_id + 1) % dict1['log_interval'] == 0:\n",
    "                mesg = \"{0}\\tEpoch:{1}[{2}/{3}],Iteration:{4}\\tLoss:{5:.4f}\\tTLoss:{6:.4f}\\t\\n\".format(time.ctime(), e+1,\n",
    "                        batch_id+1, len(train_dataset)//dict1['N'], iteration,loss, total_loss / (batch_id + 1))\n",
    "                print(mesg)\n",
    "                if dict1['log_file'] is not None:\n",
    "                    with open(dict1['log_file'],'a') as f:\n",
    "                        f.write(mesg)\n",
    "                    \n",
    "        \n",
    "        if dict1['checkpoint_dir'] is not None and (e + 1) % dict1['checkpoint_interval'] == 0:\n",
    "            embedder_net.eval().cpu()\n",
    "            ckpt_model_filename = \"ckpt_epoch_\" + str(e+1) + \"_batch_id_\" + str(batch_id+1) + \".pth\"\n",
    "            ckpt_model_path = os.path.join(dict1['checkpoint_dir'], ckpt_model_filename)\n",
    "            torch.save(embedder_net.state_dict(), ckpt_model_path)\n",
    "            embedder_net.to(device).train()\n",
    "\n",
    "    #save model\n",
    "    embedder_net.eval().cpu()\n",
    "    save_model_filename = \"final_epoch_\" + str(e + 1) + \"_batch_id_\" + str(batch_id + 1) + \".model\"\n",
    "    save_model_path = os.path.join(dict1['checkpoint_dir'], save_model_filename)\n",
    "    torch.save(embedder_net.state_dict(), save_model_path)\n",
    "    \n",
    "    print(\"\\nDone, trained model saved at\", save_model_path)\n",
    "train(dict1['model_path'],dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2MGRbfQQcZN3"
   },
   "outputs": [],
   "source": [
    "_DEFAULT_OBSERVATION_DIM = 256\n",
    "\n",
    "import argparse\n",
    "def str2bool(value):\n",
    "  \"\"\"A function to convert string to bool value.\"\"\"\n",
    "  if value.lower() in {'yes', 'true', 't', 'y', '1'}:\n",
    "    return True\n",
    "  if value.lower() in {'no', 'false', 'f', 'n', '0'}:\n",
    "    return False\n",
    "# model configurations\n",
    "class arguments:\n",
    "    def __init__(self):\n",
    "        self.observation_dim=_DEFAULT_OBSERVATION_DIM\n",
    "        self.rnn_hidden_size=512\n",
    "        self.rnn_depth=1\n",
    "        self.rnn_dropout=0.2\n",
    "        self.transition_bias=None\n",
    "        self.crp_alpha=1.0\n",
    "        self.sigma2=None\n",
    "        self.verbosity=2\n",
    "        self.enable_cuda=True\n",
    "        self.optimizer='adam'\n",
    "        self.learning_rate=1e-5\n",
    "        self.train_iteration=20000\n",
    "        self.batch_size=20\n",
    "        self.num_permutations=10\n",
    "        self.sigma_alpha=1\n",
    "        self.sigma_beta=1\n",
    "        self.regularization_weight=1e-5\n",
    "        self.grad_max_norm=5\n",
    "        self.enforce_cluster_id_uniqueness=True\n",
    "        self.beam_size=10\n",
    "        self.look_ahead=1\n",
    "        self.test_iteration=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d4h8ecj4cZQ0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def weighted_mse_loss(input_tensor, target_tensor, weight=1):\n",
    "  \"\"\"Compute weighted MSE loss.\n",
    "  Note that we are doing weighted loss that only sum up over non-zero entries.\n",
    "  Args:\n",
    "    input_tensor: input tensor\n",
    "    target_tensor: target tensor\n",
    "    weight: weight tensor, in this case 1/sigma^2\n",
    "  Returns:\n",
    "    the weighted MSE loss\n",
    "  \"\"\"\n",
    "  observation_dim = input_tensor.size()[-1]\n",
    "  streched_tensor = ((input_tensor - target_tensor) ** 2).view(\n",
    "      -1, observation_dim)\n",
    "  entry_num = float(streched_tensor.size()[0])\n",
    "  non_zero_entry_num = torch.sum(streched_tensor[:, 0] != 0).float()\n",
    "  weighted_tensor = torch.mm(\n",
    "      ((input_tensor - target_tensor)**2).view(-1, observation_dim),\n",
    "      (torch.diag(weight.float().view(-1))))\n",
    "  return torch.mean(\n",
    "      weighted_tensor) * weight.nelement() * entry_num / non_zero_entry_num\n",
    "\n",
    "\n",
    "def sigma2_prior_loss(num_non_zero, sigma_alpha, sigma_beta, sigma2):\n",
    "    \"\"\"Compute sigma2 prior loss.\n",
    "  Args:\n",
    "    num_non_zero: since rnn_truth is a collection of different length sequences\n",
    "        padded with zeros to fit them into a tensor, we count the sum of\n",
    "        'real lengths' of all sequences\n",
    "    sigma_alpha: inverse gamma shape\n",
    "    sigma_beta: inverse gamma scale\n",
    "    sigma2: sigma squared\n",
    "  Returns:\n",
    "    the sigma2 prior loss\n",
    "    \"\"\"\n",
    "    return ((2 * sigma_alpha + num_non_zero + 2) /\n",
    "          (2 * num_non_zero) * torch.log(sigma2)).sum() + (\n",
    "              sigma_beta / (sigma2 * num_non_zero)).sum()\n",
    "\n",
    "\n",
    "def regularization_loss(params, weight):\n",
    "  \"\"\"Compute regularization loss.\n",
    "  Args:\n",
    "    params: iterable of all parameters\n",
    "    weight: weight for the regularization term\n",
    "  Returns:\n",
    "    the regularization loss\n",
    "  \"\"\"\n",
    "  l2_reg = 0\n",
    "  for param in params:\n",
    "    l2_reg += torch.norm(param)\n",
    "  return weight * l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H1P-SnW8cZWW"
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_list_inverse_index(unique_ids):\n",
    "  \"\"\"Get value to position index from a list of unique ids.\n",
    "  Args:\n",
    "    unique_ids: A list of unique integers of strings.\n",
    "  Returns:\n",
    "    result: a dict from value to position\n",
    "  Raises:\n",
    "    TypeError: If unique_ids is not a list.\n",
    "  \"\"\"\n",
    "  if not isinstance(unique_ids, list):\n",
    "    raise TypeError('unique_ids must be a list')\n",
    "  result = dict()\n",
    "  for i, unique_id in enumerate(unique_ids):\n",
    "    result[unique_id] = i\n",
    "  return result\n",
    "\n",
    "def compute_sequence_match_accuracy(sequence1, sequence2):\n",
    "  \"\"\"Compute the accuracy between two sequences by finding optimal matching.\n",
    "  Args:\n",
    "    sequence1: A list of integers or strings.\n",
    "    sequence2: A list of integers or strings.\n",
    "  Returns:\n",
    "    accuracy: sequence matching accuracy as a number in [0.0, 1.0]\n",
    "  Raises:\n",
    "    TypeError: If sequence1 or sequence2 is not list.\n",
    "    ValueError: If sequence1 and sequence2 are not same size.\n",
    "  \"\"\"\n",
    "  sequence1=list(sequence1)\n",
    "  if not isinstance(sequence1, list) or not isinstance(sequence2, list):\n",
    "    raise TypeError('sequence1 and sequence2 must be lists')\n",
    "  if not sequence1 or len(sequence1) != len(sequence2):\n",
    "    raise ValueError(\n",
    "        'sequence1 and sequence2 must have the same non-zero length')\n",
    "  # get unique ids from sequences\n",
    "  unique_ids1 = sorted(set(sequence1))\n",
    "  unique_ids2 = sorted(set(sequence2))\n",
    "  inverse_index1 = get_list_inverse_index(unique_ids1)\n",
    "  inverse_index2 = get_list_inverse_index(unique_ids2)\n",
    "  print(sequence1)\n",
    "  print(sequence2)\n",
    "  print(inverse_index1)\n",
    "  print(inverse_index2)\n",
    "  # get the count matrix\n",
    "  count_matrix = np.zeros((len(unique_ids1), len(unique_ids2)))\n",
    "  for item1, item2 in zip(sequence1, sequence2):\n",
    "    index1 = inverse_index1[item1]\n",
    "    index2 = inverse_index2[item2]\n",
    "    count_matrix[index1, index2] += 1.0\n",
    "  row_index, col_index = optimize.linear_sum_assignment(-count_matrix)\n",
    "  optimal_match_count = count_matrix[row_index, col_index].sum()\n",
    "  accuracy = optimal_match_count / len(sequence1)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTyCEgnPcZZU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "\n",
    "class Logger:\n",
    "  \"\"\"A class for printing logging information to screen.\"\"\"\n",
    "\n",
    "  def __init__(self, verbosity):\n",
    "    self._verbosity = verbosity\n",
    "\n",
    "  def print(self, level, message):\n",
    "    \"\"\"Print a message if level is not higher than verbosity.\n",
    "    Args:\n",
    "      level: the level of this message, smaller value means more important\n",
    "      message: the message to be printed\n",
    "    \"\"\"\n",
    "    if level <= self._verbosity:\n",
    "      print(message)\n",
    "\n",
    "\n",
    "def generate_random_string(length=6):\n",
    "  \"\"\"Generate a random string of upper case letters and digits.\n",
    "  Args:\n",
    "    length: length of the generated string\n",
    "  Returns:\n",
    "    the generated string\n",
    "  \"\"\"\n",
    "  return ''.join([\n",
    "      random.choice(string.ascii_uppercase + string.digits)\n",
    "      for _ in range(length)])\n",
    "\n",
    "\n",
    "def enforce_cluster_id_uniqueness(cluster_ids):\n",
    "  \"\"\"Enforce uniqueness of cluster id across sequences.\n",
    "  Args:\n",
    "    cluster_ids: a list of 1-dim list/numpy.ndarray of strings\n",
    "  Returns:\n",
    "    a new list with same length of cluster_ids\n",
    "  Raises:\n",
    "    TypeError: if cluster_ids or its element has wrong type\n",
    "  \"\"\"\n",
    "  if not isinstance(cluster_ids, list):\n",
    "    raise TypeError('cluster_ids must be a list')\n",
    "  new_cluster_ids = []\n",
    "  for cluster_id in cluster_ids:\n",
    "    sequence_id = generate_random_string()\n",
    "    if isinstance(cluster_id, np.ndarray):\n",
    "      cluster_id = cluster_id.tolist()\n",
    "    if not isinstance(cluster_id, list):\n",
    "      raise TypeError('Elements of cluster_ids must be list or numpy.ndarray')\n",
    "    new_cluster_id = ['_'.join([sequence_id, s]) for s in cluster_id]\n",
    "    new_cluster_ids.append(new_cluster_id)\n",
    "  return new_cluster_ids\n",
    "\n",
    "\n",
    "def concatenate_training_data(train_sequences, train_cluster_ids,\n",
    "                              enforce_uniqueness=True, shuffle=True):\n",
    "  \"\"\"Concatenate training data.\n",
    "  Args:\n",
    "    train_sequences: a list of 2-dim numpy arrays to be concatenated\n",
    "    train_cluster_ids: a list of 1-dim list/numpy.ndarray of strings\n",
    "    enforce_uniqueness: a boolean indicated whether we should enfore uniqueness\n",
    "      to train_cluster_ids\n",
    "    shuffle: whether to randomly shuffle input order\n",
    "  Returns:\n",
    "    concatenated_train_sequence: a 2-dim numpy array\n",
    "    concatenated_train_cluster_id: a list of strings\n",
    "  Raises:\n",
    "    TypeError: if input has wrong type\n",
    "    ValueError: if sizes/dimensions of input or their elements are incorrect\n",
    "  \"\"\"\n",
    "  # check input\n",
    "  if not isinstance(train_sequences, list) or not isinstance(\n",
    "      train_cluster_ids, list):\n",
    "    raise TypeError('train_sequences and train_cluster_ids must be lists')\n",
    "  if len(train_sequences) != len(train_cluster_ids):\n",
    "    raise ValueError(\n",
    "        'train_sequences and train_cluster_ids must have same size')\n",
    "  train_cluster_ids = [\n",
    "      x.tolist() if isinstance(x, np.ndarray) else x\n",
    "      for x in train_cluster_ids]\n",
    "  global_observation_dim = None\n",
    "  for i, (train_sequence, train_cluster_id) in enumerate(\n",
    "      zip(train_sequences, train_cluster_ids)):\n",
    "    train_length, observation_dim = train_sequence.shape\n",
    "    if i == 0:\n",
    "      global_observation_dim = observation_dim\n",
    "    elif global_observation_dim != observation_dim:\n",
    "      raise ValueError(\n",
    "          'train_sequences must have consistent observation dimension')\n",
    "    if not isinstance(train_cluster_id, list):\n",
    "      raise TypeError(\n",
    "          'Elements of train_cluster_ids must be list or numpy.ndarray')\n",
    "    if len(train_cluster_id) != train_length:\n",
    "      raise ValueError(\n",
    "          'Each train_sequence and its train_cluster_id must have same length')\n",
    "\n",
    "  # enforce uniqueness\n",
    "  if enforce_uniqueness:\n",
    "    train_cluster_ids = enforce_cluster_id_uniqueness(train_cluster_ids)\n",
    "\n",
    "  # random shuffle\n",
    "  if shuffle:\n",
    "    zipped_input = list(zip(train_sequences, train_cluster_ids))\n",
    "    random.shuffle(zipped_input)\n",
    "    train_sequences, train_cluster_ids = zip(*zipped_input)\n",
    "\n",
    "  # concatenate\n",
    "  concatenated_train_sequence = np.concatenate(train_sequences, axis=0)\n",
    "  concatenated_train_cluster_id = [x for train_cluster_id in train_cluster_ids\n",
    "                                   for x in train_cluster_id]\n",
    "  return concatenated_train_sequence, concatenated_train_cluster_id\n",
    "\n",
    "\n",
    "def sample_permuted_segments(index_sequence, number_samples):\n",
    "  \"\"\"Sample sequences with permuted blocks.\n",
    "  Args:\n",
    "    index_sequence: (integer array, size: L)\n",
    "      - subsequence index\n",
    "      For example, index_sequence = [1,2,6,10,11,12].\n",
    "    number_samples: (integer)\n",
    "      - number of subsampled block-preserving permuted sequences.\n",
    "      For example, number_samples = 5\n",
    "  Returns:\n",
    "    sampled_index_sequences: (a list of numpy arrays) - a list of subsampled\n",
    "      block-preserving permuted sequences. For example,\n",
    "    ```\n",
    "    sampled_index_sequences =\n",
    "    [[10,11,12,1,2,6],\n",
    "     [6,1,2,10,11,12],\n",
    "     [1,2,10,11,12,6],\n",
    "     [6,1,2,10,11,12],\n",
    "     [1,2,6,10,11,12]]\n",
    "    ```\n",
    "      The length of \"sampled_index_sequences\" is \"number_samples\".\n",
    "  \"\"\"\n",
    "  segments = []\n",
    "  if len(index_sequence) == 1:\n",
    "    segments.append(index_sequence)\n",
    "  else:\n",
    "    prev = 0\n",
    "    for i in range(len(index_sequence) - 1):\n",
    "      if index_sequence[i + 1] != index_sequence[i] + 1:\n",
    "        segments.append(index_sequence[prev:(i + 1)])\n",
    "        prev = i + 1\n",
    "      if i + 1 == len(index_sequence) - 1:\n",
    "        segments.append(index_sequence[prev:])\n",
    "  # sample permutations\n",
    "  sampled_index_sequences = []\n",
    "  for _ in range(number_samples):\n",
    "    segments_array = []\n",
    "    permutation = np.random.permutation(len(segments))\n",
    "    for permutation_item in permutation:\n",
    "      segments_array.append(segments[permutation_item])\n",
    "    sampled_index_sequences.append(np.concatenate(segments_array))\n",
    "  return sampled_index_sequences\n",
    "\n",
    "\n",
    "def resize_sequence(sequence, cluster_id, num_permutations=None):\n",
    "  \"\"\"Resize sequences for packing and batching.\n",
    "  Args:\n",
    "    sequence: (real numpy matrix, size: seq_len*obs_size) - observed sequence\n",
    "    cluster_id: (numpy vector, size: seq_len) - cluster indicator sequence\n",
    "    num_permutations: int - Number of permutations per utterance sampled.\n",
    "  Returns:\n",
    "    sub_sequences: A list of numpy array, with obsevation vector from the same\n",
    "      cluster in the same list.\n",
    "    seq_lengths: The length of each cluster (+1).\n",
    "    bias: Flipping coin head probability.\n",
    "    bias_denominator: The denominator of the bias, used for multiple calls to\n",
    "      fit().\n",
    "  \"\"\"\n",
    "  # merge sub-sequences that belong to a single cluster to a single sequence\n",
    "  unique_id = np.unique(cluster_id)\n",
    "  sub_sequences = []\n",
    "  seq_lengths = []\n",
    "  if num_permutations and num_permutations > 1:\n",
    "    for i in unique_id:\n",
    "      idx_set = np.where(cluster_id == i)[0]\n",
    "      sampled_idx_sets = sample_permuted_segments(idx_set, num_permutations)\n",
    "      for j in range(num_permutations):\n",
    "        sub_sequences.append(sequence[sampled_idx_sets[j], :])\n",
    "        seq_lengths.append(len(idx_set) + 1)\n",
    "  else:\n",
    "    for i in unique_id:\n",
    "      idx_set = np.where(cluster_id == i)\n",
    "      sub_sequences.append(sequence[idx_set, :][0])\n",
    "      seq_lengths.append(len(idx_set[0]) + 1)\n",
    "\n",
    "  # compute bias\n",
    "  transit_num = 0\n",
    "  for entry in range(len(cluster_id) - 1):\n",
    "    transit_num += (cluster_id[entry] != cluster_id[entry + 1])\n",
    "  bias_denominator = len(cluster_id)\n",
    "  bias = (transit_num + 1) / bias_denominator\n",
    "  return sub_sequences, seq_lengths, bias, bias_denominator\n",
    "\n",
    "\n",
    "def pack_sequence(\n",
    "    sub_sequences, seq_lengths, batch_size, observation_dim, device):\n",
    "  \"\"\"Pack sequences for training.\n",
    "  Args:\n",
    "    sub_sequences: A list of numpy array, with obsevation vector from the same\n",
    "      cluster in the same list.\n",
    "    seq_lengths: The length of each cluster (+1).\n",
    "    batch_size: int or None - Run batch learning if batch_size is None. Else,\n",
    "      run online learning with specified batch size.\n",
    "    observation_dim: int - dimension for observation vectors\n",
    "    device: str - Your device. E.g., `cuda:0` or `cpu`.\n",
    "  Returns:\n",
    "    packed_rnn_input: (PackedSequence object) packed rnn input\n",
    "    rnn_truth: ground truth\n",
    "  \"\"\"\n",
    "  num_clusters = len(seq_lengths)\n",
    "  sorted_seq_lengths = np.sort(seq_lengths)[::-1]\n",
    "  permute_index = np.argsort(seq_lengths)[::-1]\n",
    "\n",
    "  if batch_size is None:\n",
    "    rnn_input = np.zeros((sorted_seq_lengths[0],\n",
    "                          num_clusters,\n",
    "                          observation_dim))\n",
    "    for i in range(num_clusters):\n",
    "      rnn_input[1:sorted_seq_lengths[i], i,\n",
    "                :] = sub_sequences[permute_index[i]]\n",
    "    rnn_input = autograd.Variable(\n",
    "        torch.from_numpy(rnn_input).float()).to(device)\n",
    "    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "        rnn_input, sorted_seq_lengths, batch_first=False)\n",
    "  else:\n",
    "    mini_batch = np.sort(np.random.choice(num_clusters, batch_size))\n",
    "    rnn_input = np.zeros((sorted_seq_lengths[mini_batch[0]],\n",
    "                          batch_size,\n",
    "                          observation_dim))\n",
    "    for i in range(batch_size):\n",
    "      rnn_input[1:sorted_seq_lengths[mini_batch[i]],\n",
    "                i, :] = sub_sequences[permute_index[mini_batch[i]]]\n",
    "    rnn_input = autograd.Variable(\n",
    "        torch.from_numpy(rnn_input).float()).to(device)\n",
    "    packed_rnn_input = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "        rnn_input, sorted_seq_lengths[mini_batch], batch_first=False)\n",
    "  # ground truth is the shifted input\n",
    "  rnn_truth = rnn_input[1:, :, :]\n",
    "  return packed_rnn_input, rnn_truth\n",
    "\n",
    "\n",
    "def output_result(model_args, training_args, test_record):\n",
    "  \"\"\"Produce a string to summarize the experiment.\"\"\"\n",
    "  accuracy_array, _ = zip(*test_record)\n",
    "  total_accuracy = np.mean(accuracy_array)\n",
    "  output_string = \"\"\"\n",
    "Config:\n",
    "  sigma_alpha: {}\n",
    "  sigma_beta: {}\n",
    "  crp_alpha: {}\n",
    "  learning rate: {}\n",
    "  regularization: {}\n",
    "  batch size: {}\n",
    "Performance:\n",
    "  averaged accuracy: {:.6f}\n",
    "  accuracy numbers for all testing sequences:\n",
    "  \"\"\".strip().format(\n",
    "      training_args.sigma_alpha,\n",
    "      training_args.sigma_beta,\n",
    "      model_args.crp_alpha,\n",
    "      training_args.learning_rate,\n",
    "      training_args.regularization_weight,\n",
    "      training_args.batch_size,\n",
    "      total_accuracy)\n",
    "  for accuracy in accuracy_array:\n",
    "    output_string += '\\n    {:.6f}'.format(accuracy)\n",
    "  output_string += '\\n' + '=' * 80 + '\\n'\n",
    "  filename = 'layer_{}_{}_{:.1f}_result.txt'.format(\n",
    "      model_args.rnn_hidden_size,\n",
    "      model_args.rnn_depth, model_args.rnn_dropout)\n",
    "  with open(filename, 'a') as file_object:\n",
    "    file_object.write(output_string)\n",
    "  return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLIa9B51cZjL"
   },
   "outputs": [],
   "source": [
    "_INITIAL_SIGMA2_VALUE = 0.1\n",
    "\n",
    "from torch import nn,optim\n",
    "class CoreRNN(nn.Module):\n",
    "  \"\"\"The core Recurent Neural Network used by UIS-RNN.\"\"\"\n",
    "\n",
    "  def __init__(self, input_dim, hidden_size, depth, observation_dim, dropout=0):\n",
    "    super(CoreRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    if depth >= 2:\n",
    "      self.gru = nn.GRU(input_dim, hidden_size, depth, dropout=dropout)\n",
    "    else:\n",
    "      self.gru = nn.GRU(input_dim, hidden_size, depth)\n",
    "    self.linear_mean1 = nn.Linear(hidden_size, hidden_size)\n",
    "    self.linear_mean2 = nn.Linear(hidden_size, observation_dim)\n",
    "\n",
    "  def forward(self, input_seq, hidden=None):\n",
    "    output_seq, hidden = self.gru(input_seq, hidden)\n",
    "    if isinstance(output_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "      output_seq, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "          output_seq, batch_first=False)\n",
    "    mean = self.linear_mean2(F.relu(self.linear_mean1(output_seq)))\n",
    "    return mean, hidden\n",
    "\n",
    "\n",
    "class BeamState:\n",
    "  \"\"\"Structure that contains necessary states for beam search.\"\"\"\n",
    "\n",
    "  def __init__(self, source=None):\n",
    "    if not source:\n",
    "      self.mean_set = []\n",
    "      self.hidden_set = []\n",
    "      self.neg_likelihood = 0\n",
    "      self.trace = []\n",
    "      self.block_counts = []\n",
    "    else:\n",
    "      self.mean_set = source.mean_set.copy()\n",
    "      self.hidden_set = source.hidden_set.copy()\n",
    "      self.trace = source.trace.copy()\n",
    "      self.block_counts = source.block_counts.copy()\n",
    "      self.neg_likelihood = source.neg_likelihood\n",
    "\n",
    "  def append(self, mean, hidden, cluster):\n",
    "    \"\"\"Append new item to the BeamState.\"\"\"\n",
    "    self.mean_set.append(mean.clone())\n",
    "    self.hidden_set.append(hidden.clone())\n",
    "    self.block_counts.append(1)\n",
    "    self.trace.append(cluster)\n",
    "\n",
    "\n",
    "class UISRNN:\n",
    "  \"\"\"Unbounded Interleaved-State Recurrent Neural Networks.\"\"\"\n",
    "  \n",
    "  def __init__(self,args):\n",
    "    \"\"\"Construct the UISRNN object.\n",
    "    Args:\n",
    "      args: Model configurations. See `arguments.py` for details.\n",
    "    \"\"\"\n",
    "    self.observation_dim = args.observation_dim\n",
    "    self.device = torch.device(\n",
    "        'cuda:0' if (torch.cuda.is_available() and args.enable_cuda) else 'cpu')\n",
    "    self.rnn_model = CoreRNN(self.observation_dim, args.rnn_hidden_size,\n",
    "                             args.rnn_depth, self.observation_dim,\n",
    "                             args.rnn_dropout).to(self.device)\n",
    "    self.rnn_init_hidden = nn.Parameter(\n",
    "        torch.zeros(args.rnn_depth, 1, args.rnn_hidden_size).to(self.device))\n",
    "    # booleans indicating which variables are trainable\n",
    "    self.estimate_sigma2 = (args.sigma2 is None)\n",
    "    self.estimate_transition_bias = (args.transition_bias is None)\n",
    "    # initial values of variables\n",
    "    sigma2 = _INITIAL_SIGMA2_VALUE if self.estimate_sigma2 else args.sigma2\n",
    "    self.sigma2 = nn.Parameter(\n",
    "        sigma2 * torch.ones(self.observation_dim).to(self.device))\n",
    "    self.transition_bias = args.transition_bias\n",
    "    self.transition_bias_denominator = 0.0\n",
    "    self.crp_alpha = args.crp_alpha\n",
    "    self.logger =Logger(args.verbosity)\n",
    "\n",
    "  def _get_optimizer(self, optimizer, learning_rate):\n",
    "    \"\"\"Get optimizer for UISRNN.\n",
    "    Args:\n",
    "      optimizer: string - name of the optimizer.\n",
    "      learning_rate: - learning rate for the entire model.\n",
    "        We do not customize learning rate for separate parts.\n",
    "    Returns:\n",
    "      a pytorch \"optim\" object\n",
    "    \"\"\"\n",
    "    params = [\n",
    "        {\n",
    "            'params': self.rnn_model.parameters()\n",
    "        },  # rnn parameters\n",
    "        {\n",
    "            'params': self.rnn_init_hidden\n",
    "        }  # rnn initial hidden state\n",
    "    ]\n",
    "    if self.estimate_sigma2:  # train sigma2\n",
    "      params.append({\n",
    "          'params': self.sigma2\n",
    "      })  # variance parameters\n",
    "    assert optimizer == 'adam', 'Only adam optimizer is supported.'\n",
    "    return optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "  def save(self, filepath):\n",
    "    \"\"\"Save the model to a file.\n",
    "    Args:\n",
    "      filepath: the path of the file.\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'rnn_state_dict': self.rnn_model.state_dict(),\n",
    "        'rnn_init_hidden': self.rnn_init_hidden.detach().cpu().numpy(),\n",
    "        'transition_bias': self.transition_bias,\n",
    "        'transition_bias_denominator': self.transition_bias_denominator,\n",
    "        'crp_alpha': self.crp_alpha,\n",
    "        'sigma2': self.sigma2.detach().cpu().numpy()}, filepath)\n",
    "\n",
    "  def load(self, filepath):\n",
    "    \"\"\"Load the model from a file.\n",
    "    Args:\n",
    "      filepath: the path of the file.\n",
    "    \"\"\"\n",
    "    var_dict = torch.load(filepath)\n",
    "    self.rnn_model.load_state_dict(var_dict['rnn_state_dict'])\n",
    "    self.rnn_init_hidden = nn.Parameter(\n",
    "        torch.from_numpy(var_dict['rnn_init_hidden']).to(self.device))\n",
    "    self.transition_bias = float(var_dict['transition_bias'])\n",
    "    self.transition_bias_denominator = float(\n",
    "        var_dict['transition_bias_denominator'])\n",
    "    self.crp_alpha = float(var_dict['crp_alpha'])\n",
    "    self.sigma2 = nn.Parameter(\n",
    "        torch.from_numpy(var_dict['sigma2']).to(self.device))\n",
    "\n",
    "    self.logger.print(\n",
    "        3, 'Loaded model with transition_bias={}, crp_alpha={}, sigma2={}, '\n",
    "        'rnn_init_hidden={}'.format(\n",
    "            self.transition_bias, self.crp_alpha, var_dict['sigma2'],\n",
    "            var_dict['rnn_init_hidden']))\n",
    "\n",
    "  def fit_concatenated(self, train_sequence, train_cluster_id, args):\n",
    "    \"\"\"Fit UISRNN model to concatenated sequence and cluster_id.\n",
    "    Args:\n",
    "      train_sequence: the training observation sequence, which is a\n",
    "        2-dim numpy array of real numbers, of size `N * D`.\n",
    "        - `N`: summation of lengths of all utterances.\n",
    "        - `D`: observation dimension.\n",
    "        For example,\n",
    "      ```\n",
    "      train_sequence =\n",
    "      [[1.2 3.0 -4.1 6.0]    --> an entry of speaker #0 from utterance 'iaaa'\n",
    "       [0.8 -1.1 0.4 0.5]    --> an entry of speaker #1 from utterance 'iaaa'\n",
    "       [-0.2 1.0 3.8 5.7]    --> an entry of speaker #0 from utterance 'iaaa'\n",
    "       [3.8 -0.1 1.5 2.3]    --> an entry of speaker #0 from utterance 'ibbb'\n",
    "       [1.2 1.4 3.6 -2.7]]   --> an entry of speaker #0 from utterance 'ibbb'\n",
    "      ```\n",
    "        Here `N=5`, `D=4`.\n",
    "        We concatenate all training utterances into this single sequence.\n",
    "      train_cluster_id: the speaker id sequence, which is 1-dim list or\n",
    "        numpy array of strings, of size `N`.\n",
    "        For example,\n",
    "      ```\n",
    "      train_cluster_id =\n",
    "        ['iaaa_0', 'iaaa_1', 'iaaa_0', 'ibbb_0', 'ibbb_0']\n",
    "      ```\n",
    "        'iaaa_0' means the entry belongs to speaker #0 in utterance 'iaaa'.\n",
    "        Note that the order of entries within an utterance are preserved,\n",
    "        and all utterances are simply concatenated together.\n",
    "      args: Training configurations. See `arguments.py` for details.\n",
    "    Raises:\n",
    "      TypeError: If train_sequence or train_cluster_id is of wrong type.\n",
    "      ValueError: If train_sequence or train_cluster_id has wrong dimension.\n",
    "    \"\"\"\n",
    "    # check type\n",
    "    if (not isinstance(train_sequence, np.ndarray) or\n",
    "        train_sequence.dtype != float):\n",
    "      raise TypeError('train_sequence should be a numpy array of float type.')\n",
    "    if isinstance(train_cluster_id, list):\n",
    "      train_cluster_id = np.array(train_cluster_id)\n",
    "    if (not isinstance(train_cluster_id, np.ndarray) or\n",
    "        not train_cluster_id.dtype.name.startswith(('str', 'unicode'))):\n",
    "      raise TypeError('train_cluster_id type be a numpy array of strings.')\n",
    "    # check dimension\n",
    "    if train_sequence.ndim != 2:\n",
    "      raise ValueError('train_sequence must be 2-dim array.')\n",
    "    if train_cluster_id.ndim != 1:\n",
    "      raise ValueError('train_cluster_id must be 1-dim array.')\n",
    "    # check length and size\n",
    "    train_total_length, observation_dim = train_sequence.shape\n",
    "    if observation_dim != self.observation_dim:\n",
    "      raise ValueError('train_sequence does not match the dimension specified '\n",
    "                       'by args.observation_dim.')\n",
    "    if train_total_length != len(train_cluster_id):\n",
    "      raise ValueError('train_sequence length is not equal to '\n",
    "                       'train_cluster_id length.')\n",
    "\n",
    "    self.rnn_model.train()\n",
    "    optimizer = self._get_optimizer(optimizer=args.optimizer,\n",
    "                                    learning_rate=args.learning_rate)\n",
    "\n",
    "    (sub_sequences,\n",
    "     seq_lengths,\n",
    "     transition_bias,\n",
    "     transition_bias_denominator) = resize_sequence(\n",
    "         sequence=train_sequence,\n",
    "         cluster_id=train_cluster_id,\n",
    "         num_permutations=args.num_permutations)\n",
    "    if self.estimate_transition_bias:\n",
    "      if self.transition_bias is None:\n",
    "        self.transition_bias = transition_bias\n",
    "        self.transition_bias_denominator = transition_bias_denominator\n",
    "        \n",
    "      else:\n",
    "        \n",
    "        self.transition_bias = (\n",
    "            self.transition_bias * self.transition_bias_denominator +\n",
    "            transition_bias * transition_bias_denominator) / (\n",
    "                self.transition_bias_denominator + transition_bias_denominator)\n",
    "        self.transition_bias_denominator += transition_bias_denominator\n",
    "\n",
    "    # For batch learning, pack the entire dataset.\n",
    "    if args.batch_size is None:\n",
    "      packed_train_sequence, rnn_truth = utils.pack_sequence(\n",
    "          sub_sequences,\n",
    "          seq_lengths,\n",
    "          args.batch_size,\n",
    "          self.observation_dim,\n",
    "          self.device)\n",
    "    train_loss = []\n",
    "    for num_iter in range(args.train_iteration):\n",
    "      optimizer.zero_grad()\n",
    "      # For online learning, pack a subset in each iteration.\n",
    "      if args.batch_size is not None:\n",
    "        packed_train_sequence, rnn_truth = pack_sequence(\n",
    "            sub_sequences,\n",
    "            seq_lengths,\n",
    "            args.batch_size,\n",
    "            self.observation_dim,\n",
    "            self.device)\n",
    "      hidden = self.rnn_init_hidden.repeat(1, args.batch_size, 1)\n",
    "      mean, _ = self.rnn_model(packed_train_sequence, hidden)\n",
    "      # use mean to predict\n",
    "      mean = torch.cumsum(mean, dim=0)\n",
    "      mean_size = mean.size()\n",
    "      mean = torch.mm(\n",
    "          torch.diag(\n",
    "              1.0 / torch.arange(1, mean_size[0] + 1).float().to(self.device)),\n",
    "          mean.view(mean_size[0], -1))\n",
    "      mean = mean.view(mean_size)\n",
    "\n",
    "      # Likelihood part.\n",
    "      loss1 = weighted_mse_loss(\n",
    "          input_tensor=(rnn_truth != 0).float() * mean[:-1, :, :],\n",
    "          target_tensor=rnn_truth,\n",
    "          weight=1 / (2 * self.sigma2))\n",
    "\n",
    "      # Sigma2 prior part.\n",
    "      weight = (((rnn_truth != 0).float() * mean[:-1, :, :] - rnn_truth)\n",
    "                ** 2).view(-1, observation_dim)\n",
    "      num_non_zero = torch.sum((weight != 0).float(), dim=0).squeeze()\n",
    "      loss2 = sigma2_prior_loss(\n",
    "          num_non_zero, args.sigma_alpha, args.sigma_beta, self.sigma2)\n",
    "\n",
    "      # Regularization part.\n",
    "      loss3 = regularization_loss(\n",
    "          self.rnn_model.parameters(), args.regularization_weight)\n",
    "\n",
    "      loss = loss1 + loss2 + loss3\n",
    "      loss.backward()\n",
    "      nn.utils.clip_grad_norm_(self.rnn_model.parameters(), args.grad_max_norm)\n",
    "      optimizer.step()\n",
    "      # avoid numerical issues\n",
    "      self.sigma2.data.clamp_(min=1e-6)\n",
    "\n",
    "      if (np.remainder(num_iter, 10) == 0 or\n",
    "          num_iter == args.train_iteration - 1):\n",
    "        self.logger.print(\n",
    "            2,\n",
    "            'Iter: {:d}  \\t'\n",
    "            'Training Loss: {:.4f}    \\n'\n",
    "            '    Negative Log Likelihood: {:.4f}\\t'\n",
    "            'Sigma2 Prior: {:.4f}\\t'\n",
    "            'Regularization: {:.4f}'.format(\n",
    "                num_iter,\n",
    "                float(loss.data),\n",
    "                float(loss1.data),\n",
    "                float(loss2.data),\n",
    "                float(loss3.data)))\n",
    "      train_loss.append(float(loss1.data))  # only save the likelihood part\n",
    "    self.logger.print(\n",
    "        1, 'Done training with {} iterations'.format(args.train_iteration))\n",
    "\n",
    "  def fit(self, train_sequences, train_cluster_ids, args):\n",
    "    \"\"\"Fit UISRNN model.\n",
    "    Args:\n",
    "      train_sequences: Either a list of training sequences, or a single\n",
    "        concatenated training sequence:\n",
    "        1. train_sequences is list, and each element is a 2-dim numpy array\n",
    "           of real numbers, of size: `length * D`.\n",
    "           The length varies among different sequences, but the D is the same.\n",
    "           In speaker diarization, each sequence is the sequence of speaker\n",
    "           embeddings of one utterance.\n",
    "        2. train_sequences is a single concatenated sequence, which is a\n",
    "           2-dim numpy array of real numbers. See `fit_concatenated()`\n",
    "           for more details.\n",
    "      train_cluster_ids: Ground truth labels for train_sequences:\n",
    "        1. if train_sequences is a list, this must also be a list of the same\n",
    "           size, each element being a 1-dim list or numpy array of strings.\n",
    "        2. if train_sequences is a single concatenated sequence, this\n",
    "           must also be the concatenated 1-dim list or numpy array of strings\n",
    "      args: Training configurations. See `arguments.py` for details.\n",
    "    Raises:\n",
    "      TypeError: If train_sequences or train_cluster_ids is of wrong type.\n",
    "    \"\"\"\n",
    "    if isinstance(train_sequences, np.ndarray):\n",
    "      # train_sequences is already the concatenated sequence\n",
    "      concatenated_train_sequence = train_sequences\n",
    "      concatenated_train_cluster_id = train_cluster_ids\n",
    "    elif isinstance(train_sequences, list):\n",
    "      # train_sequences is a list of un-concatenated sequences,\n",
    "      # then we concatenate them first\n",
    "      (concatenated_train_sequence,\n",
    "       concatenated_train_cluster_id) = utils.concatenate_training_data(\n",
    "           train_sequences,\n",
    "           train_cluster_ids,\n",
    "           args.enforce_cluster_id_uniqueness,\n",
    "           True)\n",
    "    else:\n",
    "      raise TypeError('train_sequences must be a list or numpy.ndarray')\n",
    "\n",
    "    self.fit_concatenated(\n",
    "        concatenated_train_sequence, concatenated_train_cluster_id, args)\n",
    "\n",
    "  def _update_beam_state(self, beam_state, look_ahead_seq, cluster_seq):\n",
    "    \"\"\"Update a beam state given a look ahead sequence and known cluster\n",
    "    assignments.\n",
    "    Args:\n",
    "      beam_state: A BeamState object.\n",
    "      look_ahead_seq: Look ahead sequence, size: look_ahead*D.\n",
    "        look_ahead: number of step to look ahead in the beam search.\n",
    "        D: observation dimension\n",
    "      cluster_seq: Cluster assignment sequence for look_ahead_seq.\n",
    "    Returns:\n",
    "      new_beam_state: An updated BeamState object.\n",
    "    \"\"\"\n",
    "\n",
    "    loss = 0\n",
    "    new_beam_state = BeamState(beam_state)\n",
    "    for sub_idx, cluster in enumerate(cluster_seq):\n",
    "      if cluster > len(new_beam_state.mean_set):  # invalid trace\n",
    "        new_beam_state.neg_likelihood = float('inf')\n",
    "        break\n",
    "      elif cluster < len(new_beam_state.mean_set):  # existing cluster\n",
    "        last_cluster = new_beam_state.trace[-1]\n",
    "        loss = weighted_mse_loss(\n",
    "            input_tensor=torch.squeeze(new_beam_state.mean_set[cluster]),\n",
    "            target_tensor=look_ahead_seq[sub_idx, :],\n",
    "            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()\n",
    "        if cluster == last_cluster:\n",
    "          loss -= np.log(1 - self.transition_bias)\n",
    "        else:\n",
    "          loss -= np.log(self.transition_bias) + np.log(\n",
    "              new_beam_state.block_counts[cluster]) - np.log(\n",
    "                  sum(new_beam_state.block_counts) + self.crp_alpha)\n",
    "        # update new mean and new hidden\n",
    "        mean, hidden = self.rnn_model(\n",
    "            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),\n",
    "            new_beam_state.hidden_set[cluster])\n",
    "        new_beam_state.mean_set[cluster] = (new_beam_state.mean_set[cluster]*(\n",
    "            (np.array(new_beam_state.trace) == cluster).sum() -\n",
    "            1).astype(float) + mean.clone()) / (\n",
    "                np.array(new_beam_state.trace) == cluster).sum().astype(\n",
    "                    float)  # use mean to predict\n",
    "        new_beam_state.hidden_set[cluster] = hidden.clone()\n",
    "        if cluster != last_cluster:\n",
    "          new_beam_state.block_counts[cluster] += 1\n",
    "        new_beam_state.trace.append(cluster)\n",
    "      else:  # new cluster\n",
    "        init_input = autograd.Variable(\n",
    "            torch.zeros(self.observation_dim)\n",
    "        ).unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        mean, hidden = self.rnn_model(init_input,\n",
    "                                      self.rnn_init_hidden)\n",
    "        loss = weighted_mse_loss(\n",
    "            input_tensor=torch.squeeze(mean),\n",
    "            target_tensor=look_ahead_seq[sub_idx, :],\n",
    "            weight=1 / (2 * self.sigma2)).cpu().detach().numpy()\n",
    "        loss -= np.log(self.transition_bias) + np.log(\n",
    "            self.crp_alpha) - np.log(\n",
    "                sum(new_beam_state.block_counts) + self.crp_alpha)\n",
    "        # update new min and new hidden\n",
    "        mean, hidden = self.rnn_model(\n",
    "            look_ahead_seq[sub_idx, :].unsqueeze(0).unsqueeze(0),\n",
    "            hidden)\n",
    "        new_beam_state.append(mean, hidden, cluster)\n",
    "      new_beam_state.neg_likelihood += loss\n",
    "    return new_beam_state\n",
    "\n",
    "  def _calculate_score(self, beam_state, look_ahead_seq):\n",
    "    \"\"\"Calculate negative log likelihoods for all possible state allocations\n",
    "       of a look ahead sequence, according to the current beam state.\n",
    "    Args:\n",
    "      beam_state: A BeamState object.\n",
    "      look_ahead_seq: Look ahead sequence, size: look_ahead*D.\n",
    "        look_ahead: number of step to look ahead in the beam search.\n",
    "        D: observation dimension\n",
    "    Returns:\n",
    "      beam_score_set: a set of scores for each possible state allocation.\n",
    "    \"\"\"\n",
    "\n",
    "    look_ahead, _ = look_ahead_seq.shape\n",
    "    beam_num_clusters = len(beam_state.mean_set)\n",
    "    beam_score_set = float('inf') * np.ones(\n",
    "        beam_num_clusters + 1 + np.arange(look_ahead))\n",
    "    for cluster_seq, _ in np.ndenumerate(beam_score_set):\n",
    "      updated_beam_state = self._update_beam_state(beam_state,\n",
    "                                                   look_ahead_seq, cluster_seq)\n",
    "      beam_score_set[cluster_seq] = updated_beam_state.neg_likelihood\n",
    "    return beam_score_set\n",
    "\n",
    "  def predict_single(self, test_sequence, args):\n",
    "    \"\"\"Predict labels for a single test sequence using UISRNN model.\n",
    "    Args:\n",
    "      test_sequence: the test observation sequence, which is 2-dim numpy array\n",
    "        of real numbers, of size `N * D`.\n",
    "        - `N`: length of one test utterance.\n",
    "        - `D` : observation dimension.\n",
    "        For example:\n",
    "      ```\n",
    "      test_sequence =\n",
    "      [[2.2 -1.0 3.0 5.6]    --> 1st entry of utterance 'iccc'\n",
    "       [0.5 1.8 -3.2 0.4]    --> 2nd entry of utterance 'iccc'\n",
    "       [-2.2 5.0 1.8 3.7]    --> 3rd entry of utterance 'iccc'\n",
    "       [-3.8 0.1 1.4 3.3]    --> 4th entry of utterance 'iccc'\n",
    "       [0.1 2.7 3.5 -1.7]]   --> 5th entry of utterance 'iccc'\n",
    "      ```\n",
    "        Here `N=5`, `D=4`.\n",
    "      args: Inference configurations. See `arguments.py` for details.\n",
    "    Returns:\n",
    "      predicted_cluster_id: predicted speaker id sequence, which is\n",
    "        an array of integers, of size `N`.\n",
    "        For example, `predicted_cluster_id = [0, 1, 0, 0, 1]`\n",
    "    Raises:\n",
    "      TypeError: If test_sequence is of wrong type.\n",
    "      ValueError: If test_sequence has wrong dimension.\n",
    "    \"\"\"\n",
    "    # check type\n",
    "    if (not isinstance(test_sequence, np.ndarray) or\n",
    "        test_sequence.dtype != float):\n",
    "      raise TypeError('test_sequence should be a numpy array of float type.')\n",
    "    # check dimension\n",
    "    if test_sequence.ndim != 2:\n",
    "      raise ValueError('test_sequence must be 2-dim array.')\n",
    "    # check size\n",
    "    test_sequence_length, observation_dim = test_sequence.shape\n",
    "    if observation_dim != self.observation_dim:\n",
    "      raise ValueError('test_sequence does not match the dimension specified '\n",
    "                       'by args.observation_dim.')\n",
    "\n",
    "    self.rnn_model.eval()\n",
    "    test_sequence = np.tile(test_sequence, (args.test_iteration, 1))\n",
    "    test_sequence = autograd.Variable(\n",
    "        torch.from_numpy(test_sequence).float()).to(self.device)\n",
    "    # bookkeeping for beam search\n",
    "    beam_set = [BeamState()]\n",
    "    for num_iter in np.arange(0, args.test_iteration * test_sequence_length,\n",
    "                              args.look_ahead):\n",
    "      max_clusters = max([len(beam_state.mean_set) for beam_state in beam_set])\n",
    "      look_ahead_seq = test_sequence[num_iter:  num_iter + args.look_ahead, :]\n",
    "      look_ahead_seq_length = look_ahead_seq.shape[0]\n",
    "      score_set = float('inf') * np.ones(\n",
    "          np.append(\n",
    "              args.beam_size, max_clusters + 1 + np.arange(\n",
    "                  look_ahead_seq_length)))\n",
    "      for beam_rank, beam_state in enumerate(beam_set):\n",
    "        beam_score_set = self._calculate_score(beam_state, look_ahead_seq)\n",
    "        score_set[beam_rank, :] = np.pad(\n",
    "            beam_score_set,\n",
    "            np.tile([[0, max_clusters - len(beam_state.mean_set)]],\n",
    "                    (look_ahead_seq_length, 1)), 'constant',\n",
    "            constant_values=float('inf'))\n",
    "      # find top scores\n",
    "      score_ranked = np.sort(score_set, axis=None)\n",
    "      score_ranked[score_ranked == float('inf')] = 0\n",
    "      score_ranked = np.trim_zeros(score_ranked)\n",
    "      idx_ranked = np.argsort(score_set, axis=None)\n",
    "      updated_beam_set = []\n",
    "      for new_beam_rank in range(\n",
    "          np.min((len(score_ranked), args.beam_size))):\n",
    "        total_idx = np.unravel_index(idx_ranked[new_beam_rank],\n",
    "                                     score_set.shape)\n",
    "        prev_beam_rank = total_idx[0]\n",
    "        cluster_seq = total_idx[1:]\n",
    "        updated_beam_state = self._update_beam_state(\n",
    "            beam_set[prev_beam_rank], look_ahead_seq, cluster_seq)\n",
    "        updated_beam_set.append(updated_beam_state)\n",
    "      beam_set = updated_beam_set\n",
    "    predicted_cluster_id = beam_set[0].trace[-test_sequence_length:]\n",
    "    return predicted_cluster_id\n",
    "\n",
    "  def predict(self, test_sequences, args):\n",
    "    \"\"\"Predict labels for a single or many test sequences using UISRNN model.\n",
    "    Args:\n",
    "      test_sequences: Either a list of test sequences, or a single test\n",
    "        sequence. Each test sequence is a 2-dim numpy array\n",
    "        of real numbers. See `predict_single()` for details.\n",
    "      args: Inference configurations. See `arguments.py` for details.\n",
    "    Returns:\n",
    "      predicted_cluster_ids: Predicted labels for test_sequences.\n",
    "        1. if test_sequences is a list, predicted_cluster_ids will be a list\n",
    "           of the same size, where each element being a 1-dim list of strings.\n",
    "        2. if test_sequences is a single sequence, predicted_cluster_ids will\n",
    "           be a 1-dim list of strings\n",
    "    Raises:\n",
    "      TypeError: If test_sequences is of wrong type.\n",
    "    \"\"\"\n",
    "    # check type\n",
    "    if isinstance(test_sequences, np.ndarray):\n",
    "      return self.predict_single(test_sequences, args)\n",
    "    if isinstance(test_sequences, list):\n",
    "      return [self.predict_single(test_sequence, args)\n",
    "              for test_sequence in test_sequences]\n",
    "    raise TypeError('test_sequences should be either a list or numpy array.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 876
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2747769,
     "status": "ok",
     "timestamp": 1570013295522,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "fGaYhOsGcZpD",
    "outputId": "f633d5a6-8e66-49d9-8ed3-cf7899727bc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/462 files\n",
      "Processed 200/462 files\n",
      "Processed 300/462 files\n",
      "Processed 400/462 files\n",
      "Processed 500/462 files\n",
      "Processed 600/462 files\n",
      "Processed 700/462 files\n",
      "Processed 800/462 files\n",
      "Processed 900/462 files\n",
      "Processed 1000/462 files\n",
      "Processed 1100/462 files\n",
      "Processed 1200/462 files\n",
      "Processed 1300/462 files\n",
      "Processed 1400/462 files\n",
      "Processed 1500/462 files\n",
      "Processed 1600/462 files\n",
      "Processed 1700/462 files\n",
      "Processed 1800/462 files\n",
      "Processed 1900/462 files\n",
      "Processed 2000/462 files\n",
      "Processed 2100/462 files\n",
      "Processed 2200/462 files\n",
      "Processed 2300/462 files\n",
      "Processed 2400/462 files\n",
      "Processed 2500/462 files\n",
      "Processed 2600/462 files\n",
      "Processed 2700/462 files\n",
      "Processed 2800/462 files\n",
      "Processed 2900/462 files\n",
      "Processed 3000/462 files\n",
      "Processed 3100/462 files\n",
      "Processed 3200/462 files\n",
      "Processed 3300/462 files\n",
      "Processed 3400/462 files\n",
      "Processed 3500/462 files\n",
      "Processed 3600/462 files\n",
      "Processed 3700/462 files\n",
      "Processed 3800/462 files\n",
      "Processed 3900/462 files\n",
      "Processed 4000/462 files\n",
      "Processed 4100/462 files\n",
      "Processed 4200/462 files\n",
      "Processed 4300/462 files\n",
      "Processed 4400/462 files\n",
      "Processed 4500/462 files\n",
      "Processed 4600/462 files\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "def concat_segs(times, segs):\n",
    "    #Concatenate continuous voiced segments\n",
    "    concat_seg = []\n",
    "    seg_concat = segs[0]\n",
    "    for i in range(0, len(times)-1):\n",
    "        if times[i][1] == times[i+1][0]:\n",
    "            seg_concat = np.concatenate((seg_concat, segs[i+1]))\n",
    "        else:\n",
    "            concat_seg.append(seg_concat)\n",
    "            seg_concat = segs[i+1]\n",
    "    else:\n",
    "        concat_seg.append(seg_concat)\n",
    "    return concat_seg\n",
    "\n",
    "def get_STFTs(segs,dict1):\n",
    "    #Get 240ms STFT windows with 50% overlap\n",
    "    sr = 16000\n",
    "    STFT_frames = []\n",
    "    for seg in segs:\n",
    "        S = librosa.core.stft(y=seg, n_fft=dict1['nfft'],\n",
    "                              win_length=int(dict1['window'] * sr), hop_length=int(dict1['hop'] * sr))\n",
    "        S = np.abs(S)**2\n",
    "        mel_basis = librosa.filters.mel(sr, n_fft=dict1['nfft'], n_mels=dict1['nmels'])\n",
    "        S = np.log10(np.dot(mel_basis, S) + 1e-6)           # log mel spectrogram of utterances\n",
    "        for j in range(0, S.shape[1], int(.12/dict1['hop'])):\n",
    "            if j + 24 < S.shape[1]:\n",
    "                STFT_frames.append(S[:,j:j+24])\n",
    "            else:\n",
    "                break\n",
    "    return STFT_frames\n",
    "\n",
    "def align_embeddings(embeddings):\n",
    "    partitions = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    j = 1\n",
    "    for i, embedding in enumerate(embeddings):\n",
    "        if (i*.12)+.24 < j*.401:\n",
    "            end = end + 1\n",
    "        else:\n",
    "            partitions.append((start,end))\n",
    "            start = end\n",
    "            end = end + 1\n",
    "            j += 1\n",
    "    else:\n",
    "        partitions.append((start,end))\n",
    "    avg_embeddings = np.zeros((len(partitions),256))\n",
    "    for i, partition in enumerate(partitions):\n",
    "        avg_embeddings[i] = np.average(embeddings[partition[0]:partition[1]],axis=0) \n",
    "    return avg_embeddings\n",
    "#dataset path\n",
    "audio_path = glob.glob(os.path.dirname(dict1['unprocessed_data']))  \n",
    "\n",
    "total_speaker_num = len(audio_path)\n",
    "train_speaker_num= (total_speaker_num//10)*9            # split total data 90% train and 10% test\n",
    "\n",
    "embedder_net = SpeechEmbedder(dict1)\n",
    "embedder_net.load_state_dict(torch.load(\"speech_id_checkpoint/final_epoch_100_batch_id_103.model\"))\n",
    "embedder_net.eval()\n",
    "\n",
    "train_sequence = []\n",
    "train_cluster_id = []\n",
    "label = 0\n",
    "count = 0\n",
    "train_saved = False\n",
    "for i, folder in enumerate(audio_path):\n",
    "    for file in os.listdir(folder):\n",
    "        if file[-4:] == '.wav':\n",
    "            times, segs = VAD_chunk(2, folder+'/'+file)\n",
    "            if segs == []:\n",
    "                print('No voice activity detected')\n",
    "                continue\n",
    "            concat_seg = concat_segs(times, segs)\n",
    "            STFT_frames = get_STFTs(concat_seg,dict1)\n",
    "            STFT_frames = np.stack(STFT_frames, axis=2)\n",
    "            STFT_frames = torch.tensor(np.transpose(STFT_frames, axes=(2,1,0)))\n",
    "            embeddings = embedder_net(STFT_frames)\n",
    "            aligned_embeddings = align_embeddings(embeddings.detach().numpy())\n",
    "            train_sequence.append(aligned_embeddings)\n",
    "            for embedding in aligned_embeddings:\n",
    "                train_cluster_id.append(str(label))\n",
    "            count = count + 1\n",
    "            if count % 100 == 0:\n",
    "                print('Processed {0}/{1} files'.format(count, len(audio_path)))\n",
    "    label = label + 1\n",
    "    \n",
    "    if not train_saved and i > train_speaker_num:\n",
    "        train_sequence = np.concatenate(train_sequence,axis=0)\n",
    "        train_cluster_id = np.asarray(train_cluster_id)\n",
    "        np.save('train_sequence',train_sequence)\n",
    "        np.save('train_cluster_id',train_cluster_id)\n",
    "        train_saved = True\n",
    "        train_sequence = []\n",
    "        train_cluster_id = []\n",
    "        \n",
    "train_sequence = np.concatenate(train_sequence,axis=0)\n",
    "train_cluster_id = np.asarray(train_cluster_id)\n",
    "np.save('test_sequence',train_sequence)\n",
    "np.save('test_cluster_id',train_cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSwjXVMUcaKr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SAVE_MODEL_NAME = 'saved_model_timit.uisrnn'\n",
    "#model training on timit dataset\n",
    "\n",
    "def diarization_experiment(args):\n",
    "  \"\"\"Experiment pipeline.\n",
    "  Load data --> train model --> test model --> output result\n",
    "  Args:\n",
    "    model_args: model configurations\n",
    "    training_args: training configurations\n",
    "    inference_args: inference configurations\n",
    "  \"\"\"\n",
    "\n",
    "  predicted_cluster_ids = []\n",
    "  test_record = []\n",
    "\n",
    "  train_sequence = np.load('train_sequence.npy')\n",
    "  test_sequence= np.load('test_sequence.npy')\n",
    "  train_cluster_id= np.load('train_cluster_id.npy')\n",
    "  test_cluster_id= np.load('test_cluster_id.npy')\n",
    "  test_cluster_id=test_cluster_id.reshape(-1,1)\n",
    "  model = UISRNN(args)\n",
    "  # training\n",
    "  model.fit(train_sequence, train_cluster_id, args)\n",
    "  model.save(SAVE_MODEL_NAME)\n",
    "  # we can also skip training by calling：\n",
    "  #model.load(SAVED_MODEL_NAME)\n",
    "\n",
    "  # testing\n",
    "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
    "    predicted_cluster_id = model.predict(test_sequence.reshape(1,-1), args)\n",
    "    predicted_cluster_ids.append(predicted_cluster_id)\n",
    "    print(type(predicted_cluster_id))\n",
    "    accuracy = compute_sequence_match_accuracy(\n",
    "        test_cluster_id, predicted_cluster_id)\n",
    "    test_record.append((accuracy, len(test_cluster_id)))\n",
    "    print('Ground truth labels:')\n",
    "    print(test_cluster_id)\n",
    "    print('Predicted labels:')\n",
    "    print(predicted_cluster_id)\n",
    "    print('-' * 80)\n",
    "  output_string = output_result(args,args,test_record)\n",
    "\n",
    "  print('Finished diarization experiment')\n",
    "  print(output_string)\n",
    "args=arguments()\n",
    "diarization_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1303911,
     "status": "ok",
     "timestamp": 1570018734360,
     "user": {
      "displayName": "pranshu rastogi",
      "photoUrl": "",
      "userId": "12209237871877088716"
     },
     "user_tz": -330
    },
    "id": "ofW91kRHcabK",
    "outputId": "bcbf7a75-c88a-40e4-89cc-11d37b75b284"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0  \tTraining Loss: -285.9686    \n",
      "    Negative Log Likelihood: 6.1656\tSigma2 Prior: -292.1349\tRegularization: 0.0006\n",
      "Iter: 10  \tTraining Loss: -286.9801    \n",
      "    Negative Log Likelihood: 6.0607\tSigma2 Prior: -293.0415\tRegularization: 0.0006\n",
      "Iter: 20  \tTraining Loss: -287.5458    \n",
      "    Negative Log Likelihood: 5.8720\tSigma2 Prior: -293.4185\tRegularization: 0.0006\n",
      "Iter: 30  \tTraining Loss: -287.0736    \n",
      "    Negative Log Likelihood: 5.9862\tSigma2 Prior: -293.0605\tRegularization: 0.0006\n",
      "Iter: 40  \tTraining Loss: -287.5175    \n",
      "    Negative Log Likelihood: 5.8229\tSigma2 Prior: -293.3410\tRegularization: 0.0006\n",
      "Iter: 50  \tTraining Loss: -287.4049    \n",
      "    Negative Log Likelihood: 5.7267\tSigma2 Prior: -293.1323\tRegularization: 0.0006\n",
      "Iter: 60  \tTraining Loss: -287.3828    \n",
      "    Negative Log Likelihood: 5.7282\tSigma2 Prior: -293.1116\tRegularization: 0.0006\n",
      "Iter: 70  \tTraining Loss: -287.9607    \n",
      "    Negative Log Likelihood: 5.7012\tSigma2 Prior: -293.6625\tRegularization: 0.0006\n",
      "Iter: 80  \tTraining Loss: -288.1610    \n",
      "    Negative Log Likelihood: 5.6329\tSigma2 Prior: -293.7946\tRegularization: 0.0006\n",
      "Iter: 90  \tTraining Loss: -287.8755    \n",
      "    Negative Log Likelihood: 5.6211\tSigma2 Prior: -293.4973\tRegularization: 0.0006\n",
      "Iter: 100  \tTraining Loss: -288.3328    \n",
      "    Negative Log Likelihood: 5.4720\tSigma2 Prior: -293.8055\tRegularization: 0.0006\n",
      "Iter: 110  \tTraining Loss: -288.8444    \n",
      "    Negative Log Likelihood: 5.4529\tSigma2 Prior: -294.2979\tRegularization: 0.0006\n",
      "Iter: 120  \tTraining Loss: -289.2753    \n",
      "    Negative Log Likelihood: 5.4439\tSigma2 Prior: -294.7199\tRegularization: 0.0006\n",
      "Iter: 130  \tTraining Loss: -288.2878    \n",
      "    Negative Log Likelihood: 5.4607\tSigma2 Prior: -293.7491\tRegularization: 0.0006\n",
      "Iter: 140  \tTraining Loss: -288.8411    \n",
      "    Negative Log Likelihood: 5.3768\tSigma2 Prior: -294.2185\tRegularization: 0.0006\n",
      "Iter: 150  \tTraining Loss: -288.9966    \n",
      "    Negative Log Likelihood: 5.3602\tSigma2 Prior: -294.3575\tRegularization: 0.0006\n",
      "Iter: 160  \tTraining Loss: -288.5078    \n",
      "    Negative Log Likelihood: 5.3176\tSigma2 Prior: -293.8260\tRegularization: 0.0006\n",
      "Iter: 170  \tTraining Loss: -289.1514    \n",
      "    Negative Log Likelihood: 5.3449\tSigma2 Prior: -294.4969\tRegularization: 0.0006\n",
      "Iter: 180  \tTraining Loss: -289.6724    \n",
      "    Negative Log Likelihood: 5.2981\tSigma2 Prior: -294.9711\tRegularization: 0.0006\n",
      "Iter: 190  \tTraining Loss: -289.3808    \n",
      "    Negative Log Likelihood: 5.3067\tSigma2 Prior: -294.6882\tRegularization: 0.0006\n",
      "Iter: 200  \tTraining Loss: -290.4095    \n",
      "    Negative Log Likelihood: 5.2424\tSigma2 Prior: -295.6526\tRegularization: 0.0006\n",
      "Iter: 210  \tTraining Loss: -290.1003    \n",
      "    Negative Log Likelihood: 5.2059\tSigma2 Prior: -295.3068\tRegularization: 0.0006\n",
      "Iter: 220  \tTraining Loss: -289.7048    \n",
      "    Negative Log Likelihood: 5.2368\tSigma2 Prior: -294.9423\tRegularization: 0.0006\n",
      "Iter: 230  \tTraining Loss: -289.9554    \n",
      "    Negative Log Likelihood: 5.1774\tSigma2 Prior: -295.1335\tRegularization: 0.0006\n",
      "Iter: 240  \tTraining Loss: -290.4496    \n",
      "    Negative Log Likelihood: 5.1804\tSigma2 Prior: -295.6306\tRegularization: 0.0006\n",
      "Iter: 250  \tTraining Loss: -290.6793    \n",
      "    Negative Log Likelihood: 5.1988\tSigma2 Prior: -295.8788\tRegularization: 0.0006\n",
      "Iter: 260  \tTraining Loss: -290.5391    \n",
      "    Negative Log Likelihood: 5.2134\tSigma2 Prior: -295.7531\tRegularization: 0.0006\n",
      "Iter: 270  \tTraining Loss: -291.2747    \n",
      "    Negative Log Likelihood: 5.1804\tSigma2 Prior: -296.4557\tRegularization: 0.0006\n",
      "Iter: 280  \tTraining Loss: -291.0761    \n",
      "    Negative Log Likelihood: 5.1767\tSigma2 Prior: -296.2535\tRegularization: 0.0006\n",
      "Iter: 290  \tTraining Loss: -291.3487    \n",
      "    Negative Log Likelihood: 5.1774\tSigma2 Prior: -296.5267\tRegularization: 0.0006\n",
      "Iter: 300  \tTraining Loss: -291.8569    \n",
      "    Negative Log Likelihood: 5.1825\tSigma2 Prior: -297.0401\tRegularization: 0.0006\n",
      "Iter: 310  \tTraining Loss: -291.8879    \n",
      "    Negative Log Likelihood: 5.1888\tSigma2 Prior: -297.0774\tRegularization: 0.0006\n",
      "Iter: 320  \tTraining Loss: -290.8506    \n",
      "    Negative Log Likelihood: 5.2032\tSigma2 Prior: -296.0544\tRegularization: 0.0006\n",
      "Iter: 330  \tTraining Loss: -291.3894    \n",
      "    Negative Log Likelihood: 5.1974\tSigma2 Prior: -296.5875\tRegularization: 0.0006\n",
      "Iter: 340  \tTraining Loss: -292.0772    \n",
      "    Negative Log Likelihood: 5.1804\tSigma2 Prior: -297.2583\tRegularization: 0.0006\n",
      "Iter: 350  \tTraining Loss: -292.1980    \n",
      "    Negative Log Likelihood: 5.1837\tSigma2 Prior: -297.3823\tRegularization: 0.0006\n",
      "Iter: 360  \tTraining Loss: -291.3113    \n",
      "    Negative Log Likelihood: 5.2050\tSigma2 Prior: -296.5170\tRegularization: 0.0006\n",
      "Iter: 370  \tTraining Loss: -292.2608    \n",
      "    Negative Log Likelihood: 5.1791\tSigma2 Prior: -297.4405\tRegularization: 0.0006\n",
      "Iter: 380  \tTraining Loss: -292.4256    \n",
      "    Negative Log Likelihood: 5.1873\tSigma2 Prior: -297.6135\tRegularization: 0.0006\n",
      "Iter: 390  \tTraining Loss: -292.6627    \n",
      "    Negative Log Likelihood: 5.1996\tSigma2 Prior: -297.8630\tRegularization: 0.0006\n",
      "Iter: 400  \tTraining Loss: -292.0699    \n",
      "    Negative Log Likelihood: 5.2179\tSigma2 Prior: -297.2885\tRegularization: 0.0006\n",
      "Iter: 410  \tTraining Loss: -292.5458    \n",
      "    Negative Log Likelihood: 5.2070\tSigma2 Prior: -297.7534\tRegularization: 0.0006\n",
      "Iter: 420  \tTraining Loss: -293.1210    \n",
      "    Negative Log Likelihood: 5.2041\tSigma2 Prior: -298.3258\tRegularization: 0.0006\n",
      "Iter: 430  \tTraining Loss: -293.3774    \n",
      "    Negative Log Likelihood: 5.2124\tSigma2 Prior: -298.5905\tRegularization: 0.0006\n",
      "Iter: 440  \tTraining Loss: -293.5528    \n",
      "    Negative Log Likelihood: 5.2086\tSigma2 Prior: -298.7621\tRegularization: 0.0006\n",
      "Iter: 450  \tTraining Loss: -293.0473    \n",
      "    Negative Log Likelihood: 5.2441\tSigma2 Prior: -298.2920\tRegularization: 0.0006\n",
      "Iter: 460  \tTraining Loss: -293.6129    \n",
      "    Negative Log Likelihood: 5.2253\tSigma2 Prior: -298.8389\tRegularization: 0.0006\n",
      "Iter: 470  \tTraining Loss: -293.9185    \n",
      "    Negative Log Likelihood: 5.2385\tSigma2 Prior: -299.1577\tRegularization: 0.0006\n",
      "Iter: 480  \tTraining Loss: -293.0606    \n",
      "    Negative Log Likelihood: 5.2549\tSigma2 Prior: -298.3162\tRegularization: 0.0006\n",
      "Iter: 490  \tTraining Loss: -294.2455    \n",
      "    Negative Log Likelihood: 5.2428\tSigma2 Prior: -299.4889\tRegularization: 0.0006\n",
      "Iter: 500  \tTraining Loss: -293.8691    \n",
      "    Negative Log Likelihood: 5.2514\tSigma2 Prior: -299.1212\tRegularization: 0.0006\n",
      "Iter: 510  \tTraining Loss: -293.6714    \n",
      "    Negative Log Likelihood: 5.2493\tSigma2 Prior: -298.9213\tRegularization: 0.0006\n",
      "Iter: 520  \tTraining Loss: -294.5582    \n",
      "    Negative Log Likelihood: 5.2489\tSigma2 Prior: -299.8078\tRegularization: 0.0006\n",
      "Iter: 530  \tTraining Loss: -294.8009    \n",
      "    Negative Log Likelihood: 5.2267\tSigma2 Prior: -300.0283\tRegularization: 0.0006\n",
      "Iter: 540  \tTraining Loss: -293.8858    \n",
      "    Negative Log Likelihood: 5.2839\tSigma2 Prior: -299.1703\tRegularization: 0.0006\n",
      "Iter: 550  \tTraining Loss: -295.1245    \n",
      "    Negative Log Likelihood: 5.2589\tSigma2 Prior: -300.3840\tRegularization: 0.0006\n",
      "Iter: 560  \tTraining Loss: -293.8889    \n",
      "    Negative Log Likelihood: 5.2936\tSigma2 Prior: -299.1831\tRegularization: 0.0006\n",
      "Iter: 570  \tTraining Loss: -294.2483    \n",
      "    Negative Log Likelihood: 5.2945\tSigma2 Prior: -299.5434\tRegularization: 0.0006\n",
      "Iter: 580  \tTraining Loss: -294.1635    \n",
      "    Negative Log Likelihood: 5.2756\tSigma2 Prior: -299.4398\tRegularization: 0.0006\n",
      "Iter: 590  \tTraining Loss: -295.1284    \n",
      "    Negative Log Likelihood: 5.2797\tSigma2 Prior: -300.4088\tRegularization: 0.0006\n",
      "Iter: 600  \tTraining Loss: -294.6258    \n",
      "    Negative Log Likelihood: 5.3010\tSigma2 Prior: -299.9274\tRegularization: 0.0006\n",
      "Iter: 610  \tTraining Loss: -295.2520    \n",
      "    Negative Log Likelihood: 5.2863\tSigma2 Prior: -300.5390\tRegularization: 0.0006\n",
      "Iter: 620  \tTraining Loss: -295.8996    \n",
      "    Negative Log Likelihood: 5.2875\tSigma2 Prior: -301.1877\tRegularization: 0.0006\n",
      "Iter: 630  \tTraining Loss: -295.9807    \n",
      "    Negative Log Likelihood: 5.3019\tSigma2 Prior: -301.2832\tRegularization: 0.0006\n",
      "Iter: 640  \tTraining Loss: -296.2120    \n",
      "    Negative Log Likelihood: 5.2908\tSigma2 Prior: -301.5034\tRegularization: 0.0006\n",
      "Iter: 650  \tTraining Loss: -295.9683    \n",
      "    Negative Log Likelihood: 5.3095\tSigma2 Prior: -301.2784\tRegularization: 0.0006\n",
      "Iter: 660  \tTraining Loss: -296.0922    \n",
      "    Negative Log Likelihood: 5.3200\tSigma2 Prior: -301.4129\tRegularization: 0.0006\n",
      "Iter: 670  \tTraining Loss: -296.2863    \n",
      "    Negative Log Likelihood: 5.3254\tSigma2 Prior: -301.6123\tRegularization: 0.0006\n",
      "Iter: 680  \tTraining Loss: -296.1761    \n",
      "    Negative Log Likelihood: 5.3260\tSigma2 Prior: -301.5027\tRegularization: 0.0006\n",
      "Iter: 690  \tTraining Loss: -296.4619    \n",
      "    Negative Log Likelihood: 5.3418\tSigma2 Prior: -301.8044\tRegularization: 0.0006\n",
      "Iter: 700  \tTraining Loss: -296.2422    \n",
      "    Negative Log Likelihood: 5.3370\tSigma2 Prior: -301.5799\tRegularization: 0.0006\n",
      "Iter: 710  \tTraining Loss: -297.1140    \n",
      "    Negative Log Likelihood: 5.3136\tSigma2 Prior: -302.4282\tRegularization: 0.0006\n",
      "Iter: 720  \tTraining Loss: -296.6137    \n",
      "    Negative Log Likelihood: 5.3474\tSigma2 Prior: -301.9617\tRegularization: 0.0006\n",
      "Iter: 730  \tTraining Loss: -296.8695    \n",
      "    Negative Log Likelihood: 5.3538\tSigma2 Prior: -302.2240\tRegularization: 0.0006\n",
      "Iter: 740  \tTraining Loss: -296.7822    \n",
      "    Negative Log Likelihood: 5.3326\tSigma2 Prior: -302.1154\tRegularization: 0.0006\n",
      "Iter: 750  \tTraining Loss: -297.2702    \n",
      "    Negative Log Likelihood: 5.3471\tSigma2 Prior: -302.6179\tRegularization: 0.0006\n",
      "Iter: 760  \tTraining Loss: -297.2730    \n",
      "    Negative Log Likelihood: 5.3454\tSigma2 Prior: -302.6191\tRegularization: 0.0006\n",
      "Iter: 770  \tTraining Loss: -297.6443    \n",
      "    Negative Log Likelihood: 5.3544\tSigma2 Prior: -302.9994\tRegularization: 0.0006\n",
      "Iter: 780  \tTraining Loss: -297.8026    \n",
      "    Negative Log Likelihood: 5.3607\tSigma2 Prior: -303.1639\tRegularization: 0.0006\n",
      "Iter: 790  \tTraining Loss: -297.1133    \n",
      "    Negative Log Likelihood: 5.3784\tSigma2 Prior: -302.4924\tRegularization: 0.0006\n",
      "Iter: 800  \tTraining Loss: -297.9174    \n",
      "    Negative Log Likelihood: 5.3724\tSigma2 Prior: -303.2905\tRegularization: 0.0006\n",
      "Iter: 810  \tTraining Loss: -297.1156    \n",
      "    Negative Log Likelihood: 5.4019\tSigma2 Prior: -302.5182\tRegularization: 0.0006\n",
      "Iter: 820  \tTraining Loss: -298.1394    \n",
      "    Negative Log Likelihood: 5.3894\tSigma2 Prior: -303.5294\tRegularization: 0.0006\n",
      "Iter: 830  \tTraining Loss: -298.6089    \n",
      "    Negative Log Likelihood: 5.3725\tSigma2 Prior: -303.9821\tRegularization: 0.0006\n",
      "Iter: 840  \tTraining Loss: -297.9585    \n",
      "    Negative Log Likelihood: 5.4090\tSigma2 Prior: -303.3682\tRegularization: 0.0006\n",
      "Iter: 850  \tTraining Loss: -298.2972    \n",
      "    Negative Log Likelihood: 5.3955\tSigma2 Prior: -303.6933\tRegularization: 0.0006\n",
      "Iter: 860  \tTraining Loss: -298.9835    \n",
      "    Negative Log Likelihood: 5.3985\tSigma2 Prior: -304.3826\tRegularization: 0.0006\n",
      "Iter: 870  \tTraining Loss: -299.0698    \n",
      "    Negative Log Likelihood: 5.4058\tSigma2 Prior: -304.4762\tRegularization: 0.0006\n",
      "Iter: 880  \tTraining Loss: -298.8108    \n",
      "    Negative Log Likelihood: 5.4327\tSigma2 Prior: -304.2442\tRegularization: 0.0006\n",
      "Iter: 890  \tTraining Loss: -299.1923    \n",
      "    Negative Log Likelihood: 5.4245\tSigma2 Prior: -304.6175\tRegularization: 0.0006\n",
      "Iter: 900  \tTraining Loss: -298.7193    \n",
      "    Negative Log Likelihood: 5.4375\tSigma2 Prior: -304.1575\tRegularization: 0.0006\n",
      "Iter: 910  \tTraining Loss: -299.7563    \n",
      "    Negative Log Likelihood: 5.4254\tSigma2 Prior: -305.1823\tRegularization: 0.0006\n",
      "Iter: 920  \tTraining Loss: -299.6711    \n",
      "    Negative Log Likelihood: 5.4071\tSigma2 Prior: -305.0788\tRegularization: 0.0006\n",
      "Iter: 930  \tTraining Loss: -299.8333    \n",
      "    Negative Log Likelihood: 5.4360\tSigma2 Prior: -305.2700\tRegularization: 0.0006\n",
      "Iter: 940  \tTraining Loss: -299.9856    \n",
      "    Negative Log Likelihood: 5.4427\tSigma2 Prior: -305.4290\tRegularization: 0.0006\n",
      "Iter: 950  \tTraining Loss: -300.1542    \n",
      "    Negative Log Likelihood: 5.4211\tSigma2 Prior: -305.5759\tRegularization: 0.0006\n",
      "Iter: 960  \tTraining Loss: -300.0756    \n",
      "    Negative Log Likelihood: 5.4598\tSigma2 Prior: -305.5359\tRegularization: 0.0006\n",
      "Iter: 970  \tTraining Loss: -299.1670    \n",
      "    Negative Log Likelihood: 5.4675\tSigma2 Prior: -304.6352\tRegularization: 0.0006\n",
      "Iter: 980  \tTraining Loss: -300.1119    \n",
      "    Negative Log Likelihood: 5.4537\tSigma2 Prior: -305.5663\tRegularization: 0.0006\n",
      "Iter: 990  \tTraining Loss: -300.7497    \n",
      "    Negative Log Likelihood: 5.4392\tSigma2 Prior: -306.1895\tRegularization: 0.0006\n",
      "Iter: 1000  \tTraining Loss: -300.1888    \n",
      "    Negative Log Likelihood: 5.4684\tSigma2 Prior: -305.6578\tRegularization: 0.0006\n",
      "Iter: 1010  \tTraining Loss: -300.3755    \n",
      "    Negative Log Likelihood: 5.4761\tSigma2 Prior: -305.8522\tRegularization: 0.0006\n",
      "Iter: 1020  \tTraining Loss: -300.8999    \n",
      "    Negative Log Likelihood: 5.4754\tSigma2 Prior: -306.3759\tRegularization: 0.0006\n",
      "Iter: 1030  \tTraining Loss: -300.2994    \n",
      "    Negative Log Likelihood: 5.4940\tSigma2 Prior: -305.7940\tRegularization: 0.0006\n",
      "Iter: 1040  \tTraining Loss: -301.7448    \n",
      "    Negative Log Likelihood: 5.4670\tSigma2 Prior: -307.2124\tRegularization: 0.0006\n",
      "Iter: 1050  \tTraining Loss: -301.1710    \n",
      "    Negative Log Likelihood: 5.4758\tSigma2 Prior: -306.6475\tRegularization: 0.0006\n",
      "Iter: 1060  \tTraining Loss: -301.9149    \n",
      "    Negative Log Likelihood: 5.4840\tSigma2 Prior: -307.3996\tRegularization: 0.0006\n",
      "Iter: 1070  \tTraining Loss: -301.1518    \n",
      "    Negative Log Likelihood: 5.5171\tSigma2 Prior: -306.6696\tRegularization: 0.0006\n",
      "Iter: 1080  \tTraining Loss: -301.8989    \n",
      "    Negative Log Likelihood: 5.5057\tSigma2 Prior: -307.4052\tRegularization: 0.0006\n",
      "Iter: 1090  \tTraining Loss: -301.9764    \n",
      "    Negative Log Likelihood: 5.4922\tSigma2 Prior: -307.4692\tRegularization: 0.0006\n",
      "Iter: 1100  \tTraining Loss: -302.3770    \n",
      "    Negative Log Likelihood: 5.4766\tSigma2 Prior: -307.8542\tRegularization: 0.0006\n",
      "Iter: 1110  \tTraining Loss: -301.0192    \n",
      "    Negative Log Likelihood: 5.5486\tSigma2 Prior: -306.5685\tRegularization: 0.0006\n",
      "Iter: 1120  \tTraining Loss: -302.7719    \n",
      "    Negative Log Likelihood: 5.5208\tSigma2 Prior: -308.2934\tRegularization: 0.0006\n",
      "Iter: 1130  \tTraining Loss: -302.5198    \n",
      "    Negative Log Likelihood: 5.5268\tSigma2 Prior: -308.0472\tRegularization: 0.0006\n",
      "Iter: 1140  \tTraining Loss: -302.6624    \n",
      "    Negative Log Likelihood: 5.5246\tSigma2 Prior: -308.1877\tRegularization: 0.0006\n",
      "Iter: 1150  \tTraining Loss: -302.3199    \n",
      "    Negative Log Likelihood: 5.5297\tSigma2 Prior: -307.8503\tRegularization: 0.0006\n",
      "Iter: 1160  \tTraining Loss: -303.6178    \n",
      "    Negative Log Likelihood: 5.5049\tSigma2 Prior: -309.1234\tRegularization: 0.0006\n",
      "Iter: 1170  \tTraining Loss: -302.3777    \n",
      "    Negative Log Likelihood: 5.5830\tSigma2 Prior: -307.9614\tRegularization: 0.0006\n",
      "Iter: 1180  \tTraining Loss: -302.9225    \n",
      "    Negative Log Likelihood: 5.5576\tSigma2 Prior: -308.4807\tRegularization: 0.0006\n",
      "Iter: 1190  \tTraining Loss: -303.7906    \n",
      "    Negative Log Likelihood: 5.5057\tSigma2 Prior: -309.2969\tRegularization: 0.0006\n",
      "Iter: 1200  \tTraining Loss: -303.8841    \n",
      "    Negative Log Likelihood: 5.5512\tSigma2 Prior: -309.4359\tRegularization: 0.0006\n",
      "Iter: 1210  \tTraining Loss: -303.8739    \n",
      "    Negative Log Likelihood: 5.5759\tSigma2 Prior: -309.4504\tRegularization: 0.0006\n",
      "Iter: 1220  \tTraining Loss: -303.5775    \n",
      "    Negative Log Likelihood: 5.5831\tSigma2 Prior: -309.1612\tRegularization: 0.0006\n",
      "Iter: 1230  \tTraining Loss: -304.1362    \n",
      "    Negative Log Likelihood: 5.5731\tSigma2 Prior: -309.7100\tRegularization: 0.0006\n",
      "Iter: 1240  \tTraining Loss: -304.3881    \n",
      "    Negative Log Likelihood: 5.5777\tSigma2 Prior: -309.9665\tRegularization: 0.0006\n",
      "Iter: 1250  \tTraining Loss: -304.2558    \n",
      "    Negative Log Likelihood: 5.5788\tSigma2 Prior: -309.8352\tRegularization: 0.0006\n",
      "Iter: 1260  \tTraining Loss: -304.7541    \n",
      "    Negative Log Likelihood: 5.5616\tSigma2 Prior: -310.3164\tRegularization: 0.0006\n",
      "Iter: 1270  \tTraining Loss: -304.4055    \n",
      "    Negative Log Likelihood: 5.5765\tSigma2 Prior: -309.9827\tRegularization: 0.0006\n",
      "Iter: 1280  \tTraining Loss: -305.1041    \n",
      "    Negative Log Likelihood: 5.5605\tSigma2 Prior: -310.6653\tRegularization: 0.0006\n",
      "Iter: 1290  \tTraining Loss: -305.2021    \n",
      "    Negative Log Likelihood: 5.5927\tSigma2 Prior: -310.7954\tRegularization: 0.0006\n",
      "Iter: 1300  \tTraining Loss: -303.7548    \n",
      "    Negative Log Likelihood: 5.6513\tSigma2 Prior: -309.4066\tRegularization: 0.0006\n",
      "Iter: 1310  \tTraining Loss: -305.6149    \n",
      "    Negative Log Likelihood: 5.5760\tSigma2 Prior: -311.1915\tRegularization: 0.0006\n",
      "Iter: 1320  \tTraining Loss: -305.0843    \n",
      "    Negative Log Likelihood: 5.6314\tSigma2 Prior: -310.7163\tRegularization: 0.0006\n",
      "Iter: 1330  \tTraining Loss: -305.8112    \n",
      "    Negative Log Likelihood: 5.6032\tSigma2 Prior: -311.4150\tRegularization: 0.0006\n",
      "Iter: 1340  \tTraining Loss: -305.7081    \n",
      "    Negative Log Likelihood: 5.6126\tSigma2 Prior: -311.3214\tRegularization: 0.0006\n",
      "Iter: 1350  \tTraining Loss: -306.0525    \n",
      "    Negative Log Likelihood: 5.6091\tSigma2 Prior: -311.6623\tRegularization: 0.0006\n",
      "Iter: 1360  \tTraining Loss: -306.4135    \n",
      "    Negative Log Likelihood: 5.5577\tSigma2 Prior: -311.9718\tRegularization: 0.0006\n",
      "Iter: 1370  \tTraining Loss: -306.1542    \n",
      "    Negative Log Likelihood: 5.6255\tSigma2 Prior: -311.7804\tRegularization: 0.0006\n",
      "Iter: 1380  \tTraining Loss: -306.3947    \n",
      "    Negative Log Likelihood: 5.6492\tSigma2 Prior: -312.0446\tRegularization: 0.0006\n",
      "Iter: 1390  \tTraining Loss: -306.7437    \n",
      "    Negative Log Likelihood: 5.6404\tSigma2 Prior: -312.3847\tRegularization: 0.0006\n",
      "Iter: 1400  \tTraining Loss: -306.6981    \n",
      "    Negative Log Likelihood: 5.6600\tSigma2 Prior: -312.3587\tRegularization: 0.0006\n",
      "Iter: 1410  \tTraining Loss: -306.4807    \n",
      "    Negative Log Likelihood: 5.6808\tSigma2 Prior: -312.1622\tRegularization: 0.0006\n",
      "Iter: 1420  \tTraining Loss: -307.1036    \n",
      "    Negative Log Likelihood: 5.6477\tSigma2 Prior: -312.7519\tRegularization: 0.0006\n",
      "Iter: 1430  \tTraining Loss: -307.1660    \n",
      "    Negative Log Likelihood: 5.6530\tSigma2 Prior: -312.8197\tRegularization: 0.0006\n",
      "Iter: 1440  \tTraining Loss: -306.9795    \n",
      "    Negative Log Likelihood: 5.6899\tSigma2 Prior: -312.6700\tRegularization: 0.0006\n",
      "Iter: 1450  \tTraining Loss: -307.3515    \n",
      "    Negative Log Likelihood: 5.6830\tSigma2 Prior: -313.0352\tRegularization: 0.0006\n",
      "Iter: 1460  \tTraining Loss: -307.4553    \n",
      "    Negative Log Likelihood: 5.6573\tSigma2 Prior: -313.1133\tRegularization: 0.0006\n",
      "Iter: 1470  \tTraining Loss: -308.1602    \n",
      "    Negative Log Likelihood: 5.6569\tSigma2 Prior: -313.8177\tRegularization: 0.0006\n",
      "Iter: 1480  \tTraining Loss: -307.2396    \n",
      "    Negative Log Likelihood: 5.6984\tSigma2 Prior: -312.9387\tRegularization: 0.0006\n",
      "Iter: 1490  \tTraining Loss: -307.9855    \n",
      "    Negative Log Likelihood: 5.6754\tSigma2 Prior: -313.6616\tRegularization: 0.0006\n",
      "Iter: 1500  \tTraining Loss: -308.1160    \n",
      "    Negative Log Likelihood: 5.6772\tSigma2 Prior: -313.7938\tRegularization: 0.0006\n",
      "Iter: 1510  \tTraining Loss: -308.1778    \n",
      "    Negative Log Likelihood: 5.7183\tSigma2 Prior: -313.8967\tRegularization: 0.0006\n",
      "Iter: 1520  \tTraining Loss: -308.5901    \n",
      "    Negative Log Likelihood: 5.6583\tSigma2 Prior: -314.2491\tRegularization: 0.0006\n",
      "Iter: 1530  \tTraining Loss: -308.5891    \n",
      "    Negative Log Likelihood: 5.6986\tSigma2 Prior: -314.2883\tRegularization: 0.0006\n",
      "Iter: 1540  \tTraining Loss: -309.0300    \n",
      "    Negative Log Likelihood: 5.7095\tSigma2 Prior: -314.7401\tRegularization: 0.0006\n",
      "Iter: 1550  \tTraining Loss: -308.0506    \n",
      "    Negative Log Likelihood: 5.7347\tSigma2 Prior: -313.7859\tRegularization: 0.0006\n",
      "Iter: 1560  \tTraining Loss: -308.6216    \n",
      "    Negative Log Likelihood: 5.7547\tSigma2 Prior: -314.3769\tRegularization: 0.0006\n",
      "Iter: 1570  \tTraining Loss: -308.3261    \n",
      "    Negative Log Likelihood: 5.7649\tSigma2 Prior: -314.0917\tRegularization: 0.0006\n",
      "Iter: 1580  \tTraining Loss: -309.5237    \n",
      "    Negative Log Likelihood: 5.7350\tSigma2 Prior: -315.2594\tRegularization: 0.0006\n",
      "Iter: 1590  \tTraining Loss: -309.5080    \n",
      "    Negative Log Likelihood: 5.7499\tSigma2 Prior: -315.2585\tRegularization: 0.0006\n",
      "Iter: 1600  \tTraining Loss: -309.9547    \n",
      "    Negative Log Likelihood: 5.7246\tSigma2 Prior: -315.6800\tRegularization: 0.0006\n",
      "Iter: 1610  \tTraining Loss: -308.4155    \n",
      "    Negative Log Likelihood: 5.8055\tSigma2 Prior: -314.2216\tRegularization: 0.0006\n",
      "Iter: 1620  \tTraining Loss: -309.9224    \n",
      "    Negative Log Likelihood: 5.7744\tSigma2 Prior: -315.6974\tRegularization: 0.0006\n",
      "Iter: 1630  \tTraining Loss: -310.4940    \n",
      "    Negative Log Likelihood: 5.7467\tSigma2 Prior: -316.2414\tRegularization: 0.0006\n",
      "Iter: 1640  \tTraining Loss: -310.1366    \n",
      "    Negative Log Likelihood: 5.7875\tSigma2 Prior: -315.9247\tRegularization: 0.0006\n",
      "Iter: 1650  \tTraining Loss: -310.5930    \n",
      "    Negative Log Likelihood: 5.7551\tSigma2 Prior: -316.3487\tRegularization: 0.0006\n",
      "Iter: 1660  \tTraining Loss: -310.5746    \n",
      "    Negative Log Likelihood: 5.7540\tSigma2 Prior: -316.3292\tRegularization: 0.0006\n",
      "Iter: 1670  \tTraining Loss: -309.9246    \n",
      "    Negative Log Likelihood: 5.8355\tSigma2 Prior: -315.7607\tRegularization: 0.0006\n",
      "Iter: 1680  \tTraining Loss: -310.7418    \n",
      "    Negative Log Likelihood: 5.8142\tSigma2 Prior: -316.5567\tRegularization: 0.0006\n",
      "Iter: 1690  \tTraining Loss: -310.7789    \n",
      "    Negative Log Likelihood: 5.8208\tSigma2 Prior: -316.6004\tRegularization: 0.0006\n",
      "Iter: 1700  \tTraining Loss: -310.3990    \n",
      "    Negative Log Likelihood: 5.8096\tSigma2 Prior: -316.2093\tRegularization: 0.0006\n",
      "Iter: 1710  \tTraining Loss: -310.8020    \n",
      "    Negative Log Likelihood: 5.8402\tSigma2 Prior: -316.6428\tRegularization: 0.0006\n",
      "Iter: 1720  \tTraining Loss: -311.4725    \n",
      "    Negative Log Likelihood: 5.8289\tSigma2 Prior: -317.3020\tRegularization: 0.0006\n",
      "Iter: 1730  \tTraining Loss: -310.8327    \n",
      "    Negative Log Likelihood: 5.8620\tSigma2 Prior: -316.6954\tRegularization: 0.0006\n",
      "Iter: 1740  \tTraining Loss: -311.6748    \n",
      "    Negative Log Likelihood: 5.7929\tSigma2 Prior: -317.4683\tRegularization: 0.0006\n",
      "Iter: 1750  \tTraining Loss: -312.2198    \n",
      "    Negative Log Likelihood: 5.7983\tSigma2 Prior: -318.0187\tRegularization: 0.0006\n",
      "Iter: 1760  \tTraining Loss: -311.4427    \n",
      "    Negative Log Likelihood: 5.8470\tSigma2 Prior: -317.2903\tRegularization: 0.0006\n",
      "Iter: 1770  \tTraining Loss: -312.1457    \n",
      "    Negative Log Likelihood: 5.8572\tSigma2 Prior: -318.0034\tRegularization: 0.0006\n",
      "Iter: 1780  \tTraining Loss: -312.2821    \n",
      "    Negative Log Likelihood: 5.8474\tSigma2 Prior: -318.1302\tRegularization: 0.0006\n",
      "Iter: 1790  \tTraining Loss: -311.5644    \n",
      "    Negative Log Likelihood: 5.9026\tSigma2 Prior: -317.4676\tRegularization: 0.0006\n",
      "Iter: 1800  \tTraining Loss: -313.0126    \n",
      "    Negative Log Likelihood: 5.8527\tSigma2 Prior: -318.8660\tRegularization: 0.0006\n",
      "Iter: 1810  \tTraining Loss: -312.2478    \n",
      "    Negative Log Likelihood: 5.9088\tSigma2 Prior: -318.1573\tRegularization: 0.0006\n",
      "Iter: 1820  \tTraining Loss: -313.0286    \n",
      "    Negative Log Likelihood: 5.8797\tSigma2 Prior: -318.9089\tRegularization: 0.0006\n",
      "Iter: 1830  \tTraining Loss: -313.2096    \n",
      "    Negative Log Likelihood: 5.8058\tSigma2 Prior: -319.0160\tRegularization: 0.0006\n",
      "Iter: 1840  \tTraining Loss: -313.7739    \n",
      "    Negative Log Likelihood: 5.8396\tSigma2 Prior: -319.6142\tRegularization: 0.0006\n",
      "Iter: 1850  \tTraining Loss: -312.9743    \n",
      "    Negative Log Likelihood: 5.9023\tSigma2 Prior: -318.8772\tRegularization: 0.0006\n",
      "Iter: 1860  \tTraining Loss: -313.7919    \n",
      "    Negative Log Likelihood: 5.8701\tSigma2 Prior: -319.6626\tRegularization: 0.0006\n",
      "Iter: 1870  \tTraining Loss: -313.8662    \n",
      "    Negative Log Likelihood: 5.9153\tSigma2 Prior: -319.7821\tRegularization: 0.0006\n",
      "Iter: 1880  \tTraining Loss: -314.1795    \n",
      "    Negative Log Likelihood: 5.8761\tSigma2 Prior: -320.0563\tRegularization: 0.0006\n",
      "Iter: 1890  \tTraining Loss: -314.6137    \n",
      "    Negative Log Likelihood: 5.8858\tSigma2 Prior: -320.5002\tRegularization: 0.0006\n",
      "Iter: 1900  \tTraining Loss: -314.6266    \n",
      "    Negative Log Likelihood: 5.9286\tSigma2 Prior: -320.5559\tRegularization: 0.0006\n",
      "Iter: 1910  \tTraining Loss: -314.0133    \n",
      "    Negative Log Likelihood: 5.9278\tSigma2 Prior: -319.9418\tRegularization: 0.0006\n",
      "Iter: 1920  \tTraining Loss: -314.6942    \n",
      "    Negative Log Likelihood: 5.9195\tSigma2 Prior: -320.6144\tRegularization: 0.0006\n",
      "Iter: 1930  \tTraining Loss: -314.6376    \n",
      "    Negative Log Likelihood: 5.9542\tSigma2 Prior: -320.5924\tRegularization: 0.0006\n",
      "Iter: 1940  \tTraining Loss: -314.6920    \n",
      "    Negative Log Likelihood: 5.9295\tSigma2 Prior: -320.6221\tRegularization: 0.0006\n",
      "Iter: 1950  \tTraining Loss: -315.3278    \n",
      "    Negative Log Likelihood: 5.9375\tSigma2 Prior: -321.2659\tRegularization: 0.0006\n",
      "Iter: 1960  \tTraining Loss: -314.6153    \n",
      "    Negative Log Likelihood: 5.9978\tSigma2 Prior: -320.6138\tRegularization: 0.0006\n",
      "Iter: 1970  \tTraining Loss: -315.7167    \n",
      "    Negative Log Likelihood: 5.9018\tSigma2 Prior: -321.6192\tRegularization: 0.0006\n",
      "Iter: 1980  \tTraining Loss: -315.6306    \n",
      "    Negative Log Likelihood: 5.9596\tSigma2 Prior: -321.5909\tRegularization: 0.0006\n",
      "Iter: 1990  \tTraining Loss: -315.8847    \n",
      "    Negative Log Likelihood: 5.9694\tSigma2 Prior: -321.8547\tRegularization: 0.0006\n",
      "Iter: 2000  \tTraining Loss: -315.8578    \n",
      "    Negative Log Likelihood: 5.9383\tSigma2 Prior: -321.7967\tRegularization: 0.0006\n",
      "Iter: 2010  \tTraining Loss: -315.8253    \n",
      "    Negative Log Likelihood: 5.9944\tSigma2 Prior: -321.8203\tRegularization: 0.0006\n",
      "Iter: 2020  \tTraining Loss: -316.4479    \n",
      "    Negative Log Likelihood: 5.9773\tSigma2 Prior: -322.4258\tRegularization: 0.0006\n",
      "Iter: 2030  \tTraining Loss: -315.2206    \n",
      "    Negative Log Likelihood: 6.0374\tSigma2 Prior: -321.2586\tRegularization: 0.0006\n",
      "Iter: 2040  \tTraining Loss: -317.4764    \n",
      "    Negative Log Likelihood: 5.9609\tSigma2 Prior: -323.4380\tRegularization: 0.0006\n",
      "Iter: 2050  \tTraining Loss: -316.7385    \n",
      "    Negative Log Likelihood: 6.0233\tSigma2 Prior: -322.7625\tRegularization: 0.0006\n",
      "Iter: 2060  \tTraining Loss: -317.2580    \n",
      "    Negative Log Likelihood: 6.0010\tSigma2 Prior: -323.2597\tRegularization: 0.0006\n",
      "Iter: 2070  \tTraining Loss: -316.8834    \n",
      "    Negative Log Likelihood: 6.0105\tSigma2 Prior: -322.8945\tRegularization: 0.0006\n",
      "Iter: 2080  \tTraining Loss: -317.1154    \n",
      "    Negative Log Likelihood: 6.0200\tSigma2 Prior: -323.1360\tRegularization: 0.0006\n",
      "Iter: 2090  \tTraining Loss: -316.9609    \n",
      "    Negative Log Likelihood: 6.0399\tSigma2 Prior: -323.0014\tRegularization: 0.0006\n",
      "Iter: 2100  \tTraining Loss: -316.9597    \n",
      "    Negative Log Likelihood: 6.0569\tSigma2 Prior: -323.0172\tRegularization: 0.0006\n",
      "Iter: 2110  \tTraining Loss: -317.4356    \n",
      "    Negative Log Likelihood: 6.0442\tSigma2 Prior: -323.4805\tRegularization: 0.0006\n",
      "Iter: 2120  \tTraining Loss: -317.3584    \n",
      "    Negative Log Likelihood: 6.0349\tSigma2 Prior: -323.3940\tRegularization: 0.0006\n",
      "Iter: 2130  \tTraining Loss: -317.8269    \n",
      "    Negative Log Likelihood: 6.0211\tSigma2 Prior: -323.8486\tRegularization: 0.0006\n",
      "Iter: 2140  \tTraining Loss: -318.6785    \n",
      "    Negative Log Likelihood: 6.0184\tSigma2 Prior: -324.6976\tRegularization: 0.0006\n",
      "Iter: 2150  \tTraining Loss: -317.1873    \n",
      "    Negative Log Likelihood: 6.1199\tSigma2 Prior: -323.3079\tRegularization: 0.0006\n",
      "Iter: 2160  \tTraining Loss: -317.6035    \n",
      "    Negative Log Likelihood: 6.1223\tSigma2 Prior: -323.7265\tRegularization: 0.0006\n",
      "Iter: 2170  \tTraining Loss: -318.2497    \n",
      "    Negative Log Likelihood: 6.0336\tSigma2 Prior: -324.2839\tRegularization: 0.0006\n",
      "Iter: 2180  \tTraining Loss: -319.1584    \n",
      "    Negative Log Likelihood: 5.9637\tSigma2 Prior: -325.1228\tRegularization: 0.0006\n",
      "Iter: 2190  \tTraining Loss: -319.4261    \n",
      "    Negative Log Likelihood: 6.0130\tSigma2 Prior: -325.4398\tRegularization: 0.0006\n",
      "Iter: 2200  \tTraining Loss: -318.2461    \n",
      "    Negative Log Likelihood: 6.0973\tSigma2 Prior: -324.3440\tRegularization: 0.0006\n",
      "Iter: 2210  \tTraining Loss: -319.9442    \n",
      "    Negative Log Likelihood: 6.0236\tSigma2 Prior: -325.9684\tRegularization: 0.0006\n",
      "Iter: 2220  \tTraining Loss: -319.1119    \n",
      "    Negative Log Likelihood: 6.1333\tSigma2 Prior: -325.2458\tRegularization: 0.0006\n",
      "Iter: 2230  \tTraining Loss: -319.9320    \n",
      "    Negative Log Likelihood: 6.0908\tSigma2 Prior: -326.0234\tRegularization: 0.0006\n",
      "Iter: 2240  \tTraining Loss: -319.9431    \n",
      "    Negative Log Likelihood: 6.0652\tSigma2 Prior: -326.0089\tRegularization: 0.0006\n",
      "Iter: 2250  \tTraining Loss: -319.4162    \n",
      "    Negative Log Likelihood: 6.1393\tSigma2 Prior: -325.5562\tRegularization: 0.0006\n",
      "Iter: 2260  \tTraining Loss: -320.1157    \n",
      "    Negative Log Likelihood: 6.0816\tSigma2 Prior: -326.1979\tRegularization: 0.0006\n",
      "Iter: 2270  \tTraining Loss: -320.2822    \n",
      "    Negative Log Likelihood: 6.1052\tSigma2 Prior: -326.3881\tRegularization: 0.0006\n",
      "Iter: 2280  \tTraining Loss: -319.2332    \n",
      "    Negative Log Likelihood: 6.2032\tSigma2 Prior: -325.4371\tRegularization: 0.0006\n",
      "Iter: 2290  \tTraining Loss: -320.7354    \n",
      "    Negative Log Likelihood: 6.0462\tSigma2 Prior: -326.7822\tRegularization: 0.0006\n",
      "Iter: 2300  \tTraining Loss: -321.0893    \n",
      "    Negative Log Likelihood: 6.0998\tSigma2 Prior: -327.1897\tRegularization: 0.0006\n",
      "Iter: 2310  \tTraining Loss: -320.5110    \n",
      "    Negative Log Likelihood: 6.1952\tSigma2 Prior: -326.7068\tRegularization: 0.0006\n",
      "Iter: 2320  \tTraining Loss: -320.6361    \n",
      "    Negative Log Likelihood: 6.1805\tSigma2 Prior: -326.8172\tRegularization: 0.0006\n",
      "Iter: 2330  \tTraining Loss: -321.1747    \n",
      "    Negative Log Likelihood: 6.1935\tSigma2 Prior: -327.3688\tRegularization: 0.0006\n",
      "Iter: 2340  \tTraining Loss: -321.3010    \n",
      "    Negative Log Likelihood: 6.1949\tSigma2 Prior: -327.4965\tRegularization: 0.0006\n",
      "Iter: 2350  \tTraining Loss: -321.0334    \n",
      "    Negative Log Likelihood: 6.1864\tSigma2 Prior: -327.2204\tRegularization: 0.0006\n",
      "Iter: 2360  \tTraining Loss: -321.5935    \n",
      "    Negative Log Likelihood: 6.1802\tSigma2 Prior: -327.7743\tRegularization: 0.0006\n",
      "Iter: 2370  \tTraining Loss: -321.5826    \n",
      "    Negative Log Likelihood: 6.2355\tSigma2 Prior: -327.8187\tRegularization: 0.0006\n",
      "Iter: 2380  \tTraining Loss: -322.0477    \n",
      "    Negative Log Likelihood: 6.2327\tSigma2 Prior: -328.2810\tRegularization: 0.0006\n",
      "Iter: 2390  \tTraining Loss: -322.7306    \n",
      "    Negative Log Likelihood: 6.1822\tSigma2 Prior: -328.9135\tRegularization: 0.0006\n",
      "Iter: 2400  \tTraining Loss: -321.4321    \n",
      "    Negative Log Likelihood: 6.2447\tSigma2 Prior: -327.6774\tRegularization: 0.0006\n",
      "Iter: 2410  \tTraining Loss: -322.4474    \n",
      "    Negative Log Likelihood: 6.2169\tSigma2 Prior: -328.6649\tRegularization: 0.0006\n",
      "Iter: 2420  \tTraining Loss: -322.4904    \n",
      "    Negative Log Likelihood: 6.2530\tSigma2 Prior: -328.7440\tRegularization: 0.0006\n",
      "Iter: 2430  \tTraining Loss: -323.4938    \n",
      "    Negative Log Likelihood: 6.2014\tSigma2 Prior: -329.6959\tRegularization: 0.0007\n",
      "Iter: 2440  \tTraining Loss: -323.3439    \n",
      "    Negative Log Likelihood: 6.1745\tSigma2 Prior: -329.5190\tRegularization: 0.0007\n",
      "Iter: 2450  \tTraining Loss: -323.9364    \n",
      "    Negative Log Likelihood: 6.1888\tSigma2 Prior: -330.1259\tRegularization: 0.0007\n",
      "Iter: 2460  \tTraining Loss: -322.5525    \n",
      "    Negative Log Likelihood: 6.3207\tSigma2 Prior: -328.8738\tRegularization: 0.0007\n",
      "Iter: 2470  \tTraining Loss: -323.4697    \n",
      "    Negative Log Likelihood: 6.2957\tSigma2 Prior: -329.7660\tRegularization: 0.0007\n",
      "Iter: 2480  \tTraining Loss: -323.7192    \n",
      "    Negative Log Likelihood: 6.2711\tSigma2 Prior: -329.9909\tRegularization: 0.0007\n",
      "Iter: 2490  \tTraining Loss: -323.7108    \n",
      "    Negative Log Likelihood: 6.2873\tSigma2 Prior: -329.9988\tRegularization: 0.0007\n",
      "Iter: 2500  \tTraining Loss: -322.8817    \n",
      "    Negative Log Likelihood: 6.3552\tSigma2 Prior: -329.2376\tRegularization: 0.0007\n",
      "Iter: 2510  \tTraining Loss: -324.3311    \n",
      "    Negative Log Likelihood: 6.3452\tSigma2 Prior: -330.6770\tRegularization: 0.0007\n",
      "Iter: 2520  \tTraining Loss: -324.5476    \n",
      "    Negative Log Likelihood: 6.3128\tSigma2 Prior: -330.8610\tRegularization: 0.0007\n",
      "Iter: 2530  \tTraining Loss: -325.0353    \n",
      "    Negative Log Likelihood: 6.2858\tSigma2 Prior: -331.3217\tRegularization: 0.0007\n",
      "Iter: 2540  \tTraining Loss: -325.0934    \n",
      "    Negative Log Likelihood: 6.2918\tSigma2 Prior: -331.3857\tRegularization: 0.0007\n",
      "Iter: 2550  \tTraining Loss: -324.7335    \n",
      "    Negative Log Likelihood: 6.2344\tSigma2 Prior: -330.9685\tRegularization: 0.0007\n",
      "Iter: 2560  \tTraining Loss: -325.3413    \n",
      "    Negative Log Likelihood: 6.2327\tSigma2 Prior: -331.5747\tRegularization: 0.0007\n",
      "Iter: 2570  \tTraining Loss: -325.7600    \n",
      "    Negative Log Likelihood: 6.3443\tSigma2 Prior: -332.1050\tRegularization: 0.0007\n",
      "Iter: 2580  \tTraining Loss: -325.7326    \n",
      "    Negative Log Likelihood: 6.3347\tSigma2 Prior: -332.0680\tRegularization: 0.0007\n",
      "Iter: 2590  \tTraining Loss: -326.2976    \n",
      "    Negative Log Likelihood: 6.2443\tSigma2 Prior: -332.5425\tRegularization: 0.0007\n",
      "Iter: 2600  \tTraining Loss: -326.1253    \n",
      "    Negative Log Likelihood: 6.2376\tSigma2 Prior: -332.3635\tRegularization: 0.0007\n",
      "Iter: 2610  \tTraining Loss: -326.1291    \n",
      "    Negative Log Likelihood: 6.3647\tSigma2 Prior: -332.4944\tRegularization: 0.0007\n",
      "Iter: 2620  \tTraining Loss: -326.7551    \n",
      "    Negative Log Likelihood: 6.2887\tSigma2 Prior: -333.0444\tRegularization: 0.0007\n",
      "Iter: 2630  \tTraining Loss: -326.4745    \n",
      "    Negative Log Likelihood: 6.3989\tSigma2 Prior: -332.8740\tRegularization: 0.0007\n",
      "Iter: 2640  \tTraining Loss: -327.6243    \n",
      "    Negative Log Likelihood: 6.2651\tSigma2 Prior: -333.8901\tRegularization: 0.0007\n",
      "Iter: 2650  \tTraining Loss: -327.2273    \n",
      "    Negative Log Likelihood: 6.3182\tSigma2 Prior: -333.5461\tRegularization: 0.0007\n",
      "Iter: 2660  \tTraining Loss: -326.7658    \n",
      "    Negative Log Likelihood: 6.4434\tSigma2 Prior: -333.2099\tRegularization: 0.0007\n",
      "Iter: 2670  \tTraining Loss: -327.1674    \n",
      "    Negative Log Likelihood: 6.3985\tSigma2 Prior: -333.5665\tRegularization: 0.0007\n",
      "Iter: 2680  \tTraining Loss: -327.4134    \n",
      "    Negative Log Likelihood: 6.3940\tSigma2 Prior: -333.8080\tRegularization: 0.0007\n",
      "Iter: 2690  \tTraining Loss: -327.6747    \n",
      "    Negative Log Likelihood: 6.4034\tSigma2 Prior: -334.0787\tRegularization: 0.0007\n",
      "Iter: 2700  \tTraining Loss: -328.5849    \n",
      "    Negative Log Likelihood: 6.3496\tSigma2 Prior: -334.9352\tRegularization: 0.0007\n",
      "Iter: 2710  \tTraining Loss: -328.1166    \n",
      "    Negative Log Likelihood: 6.4141\tSigma2 Prior: -334.5313\tRegularization: 0.0007\n",
      "Iter: 2720  \tTraining Loss: -328.3601    \n",
      "    Negative Log Likelihood: 6.4585\tSigma2 Prior: -334.8192\tRegularization: 0.0007\n",
      "Iter: 2730  \tTraining Loss: -328.4258    \n",
      "    Negative Log Likelihood: 6.4711\tSigma2 Prior: -334.8976\tRegularization: 0.0007\n",
      "Iter: 2740  \tTraining Loss: -329.1307    \n",
      "    Negative Log Likelihood: 6.3808\tSigma2 Prior: -335.5122\tRegularization: 0.0007\n",
      "Iter: 2750  \tTraining Loss: -329.0110    \n",
      "    Negative Log Likelihood: 6.4349\tSigma2 Prior: -335.4466\tRegularization: 0.0007\n",
      "Iter: 2760  \tTraining Loss: -329.0320    \n",
      "    Negative Log Likelihood: 6.4341\tSigma2 Prior: -335.4667\tRegularization: 0.0007\n",
      "Iter: 2770  \tTraining Loss: -329.1024    \n",
      "    Negative Log Likelihood: 6.4805\tSigma2 Prior: -335.5836\tRegularization: 0.0007\n",
      "Iter: 2780  \tTraining Loss: -329.6740    \n",
      "    Negative Log Likelihood: 6.4393\tSigma2 Prior: -336.1140\tRegularization: 0.0007\n",
      "Iter: 2790  \tTraining Loss: -328.5201    \n",
      "    Negative Log Likelihood: 6.5257\tSigma2 Prior: -335.0465\tRegularization: 0.0007\n",
      "Iter: 2800  \tTraining Loss: -328.4045    \n",
      "    Negative Log Likelihood: 6.5575\tSigma2 Prior: -334.9627\tRegularization: 0.0007\n",
      "Iter: 2810  \tTraining Loss: -330.0082    \n",
      "    Negative Log Likelihood: 6.4743\tSigma2 Prior: -336.4832\tRegularization: 0.0007\n",
      "Iter: 2820  \tTraining Loss: -329.7865    \n",
      "    Negative Log Likelihood: 6.5038\tSigma2 Prior: -336.2909\tRegularization: 0.0007\n",
      "Iter: 2830  \tTraining Loss: -329.9247    \n",
      "    Negative Log Likelihood: 6.5105\tSigma2 Prior: -336.4359\tRegularization: 0.0007\n",
      "Iter: 2840  \tTraining Loss: -329.8790    \n",
      "    Negative Log Likelihood: 6.5821\tSigma2 Prior: -336.4618\tRegularization: 0.0007\n",
      "Iter: 2850  \tTraining Loss: -330.0699    \n",
      "    Negative Log Likelihood: 6.5847\tSigma2 Prior: -336.6552\tRegularization: 0.0007\n",
      "Iter: 2860  \tTraining Loss: -330.9815    \n",
      "    Negative Log Likelihood: 6.4144\tSigma2 Prior: -337.3966\tRegularization: 0.0007\n",
      "Iter: 2870  \tTraining Loss: -331.0123    \n",
      "    Negative Log Likelihood: 6.5539\tSigma2 Prior: -337.5669\tRegularization: 0.0007\n",
      "Iter: 2880  \tTraining Loss: -330.8788    \n",
      "    Negative Log Likelihood: 6.6130\tSigma2 Prior: -337.4924\tRegularization: 0.0007\n",
      "Iter: 2890  \tTraining Loss: -331.8196    \n",
      "    Negative Log Likelihood: 6.4418\tSigma2 Prior: -338.2621\tRegularization: 0.0007\n",
      "Iter: 2900  \tTraining Loss: -331.8150    \n",
      "    Negative Log Likelihood: 6.5142\tSigma2 Prior: -338.3299\tRegularization: 0.0007\n",
      "Iter: 2910  \tTraining Loss: -331.7014    \n",
      "    Negative Log Likelihood: 6.5647\tSigma2 Prior: -338.2668\tRegularization: 0.0007\n",
      "Iter: 2920  \tTraining Loss: -331.9871    \n",
      "    Negative Log Likelihood: 6.6216\tSigma2 Prior: -338.6093\tRegularization: 0.0007\n",
      "Iter: 2930  \tTraining Loss: -332.2505    \n",
      "    Negative Log Likelihood: 6.5660\tSigma2 Prior: -338.8171\tRegularization: 0.0007\n",
      "Iter: 2940  \tTraining Loss: -332.3181    \n",
      "    Negative Log Likelihood: 6.5990\tSigma2 Prior: -338.9178\tRegularization: 0.0007\n",
      "Iter: 2950  \tTraining Loss: -333.2094    \n",
      "    Negative Log Likelihood: 6.4939\tSigma2 Prior: -339.7040\tRegularization: 0.0007\n",
      "Iter: 2960  \tTraining Loss: -331.5742    \n",
      "    Negative Log Likelihood: 6.6554\tSigma2 Prior: -338.2304\tRegularization: 0.0007\n",
      "Iter: 2970  \tTraining Loss: -332.3756    \n",
      "    Negative Log Likelihood: 6.6420\tSigma2 Prior: -339.0184\tRegularization: 0.0007\n",
      "Iter: 2980  \tTraining Loss: -333.8610    \n",
      "    Negative Log Likelihood: 6.4822\tSigma2 Prior: -340.3439\tRegularization: 0.0007\n",
      "Iter: 2990  \tTraining Loss: -333.0360    \n",
      "    Negative Log Likelihood: 6.6361\tSigma2 Prior: -339.6728\tRegularization: 0.0007\n",
      "Iter: 3000  \tTraining Loss: -333.5658    \n",
      "    Negative Log Likelihood: 6.6275\tSigma2 Prior: -340.1940\tRegularization: 0.0007\n",
      "Iter: 3010  \tTraining Loss: -333.5875    \n",
      "    Negative Log Likelihood: 6.6071\tSigma2 Prior: -340.1953\tRegularization: 0.0007\n",
      "Iter: 3020  \tTraining Loss: -333.8707    \n",
      "    Negative Log Likelihood: 6.7086\tSigma2 Prior: -340.5800\tRegularization: 0.0007\n",
      "Iter: 3030  \tTraining Loss: -333.8896    \n",
      "    Negative Log Likelihood: 6.6360\tSigma2 Prior: -340.5263\tRegularization: 0.0007\n",
      "Iter: 3040  \tTraining Loss: -334.2129    \n",
      "    Negative Log Likelihood: 6.6784\tSigma2 Prior: -340.8919\tRegularization: 0.0007\n",
      "Iter: 3050  \tTraining Loss: -334.9714    \n",
      "    Negative Log Likelihood: 6.5911\tSigma2 Prior: -341.5632\tRegularization: 0.0007\n",
      "Iter: 3060  \tTraining Loss: -334.5200    \n",
      "    Negative Log Likelihood: 6.6911\tSigma2 Prior: -341.2117\tRegularization: 0.0007\n",
      "Iter: 3070  \tTraining Loss: -334.6873    \n",
      "    Negative Log Likelihood: 6.7486\tSigma2 Prior: -341.4366\tRegularization: 0.0007\n",
      "Iter: 3080  \tTraining Loss: -334.8691    \n",
      "    Negative Log Likelihood: 6.6641\tSigma2 Prior: -341.5339\tRegularization: 0.0007\n",
      "Iter: 3090  \tTraining Loss: -334.9151    \n",
      "    Negative Log Likelihood: 6.7596\tSigma2 Prior: -341.6754\tRegularization: 0.0007\n",
      "Iter: 3100  \tTraining Loss: -336.1950    \n",
      "    Negative Log Likelihood: 6.6366\tSigma2 Prior: -342.8323\tRegularization: 0.0007\n",
      "Iter: 3110  \tTraining Loss: -334.9328    \n",
      "    Negative Log Likelihood: 6.7649\tSigma2 Prior: -341.6984\tRegularization: 0.0007\n",
      "Iter: 3120  \tTraining Loss: -335.0614    \n",
      "    Negative Log Likelihood: 6.7543\tSigma2 Prior: -341.8163\tRegularization: 0.0007\n",
      "Iter: 3130  \tTraining Loss: -336.1506    \n",
      "    Negative Log Likelihood: 6.6814\tSigma2 Prior: -342.8327\tRegularization: 0.0007\n",
      "Iter: 3140  \tTraining Loss: -334.7462    \n",
      "    Negative Log Likelihood: 6.7293\tSigma2 Prior: -341.4762\tRegularization: 0.0007\n",
      "Iter: 3150  \tTraining Loss: -337.2776    \n",
      "    Negative Log Likelihood: 6.6817\tSigma2 Prior: -343.9601\tRegularization: 0.0007\n",
      "Iter: 3160  \tTraining Loss: -336.7193    \n",
      "    Negative Log Likelihood: 6.7133\tSigma2 Prior: -343.4333\tRegularization: 0.0007\n",
      "Iter: 3170  \tTraining Loss: -337.0084    \n",
      "    Negative Log Likelihood: 6.7134\tSigma2 Prior: -343.7224\tRegularization: 0.0007\n",
      "Iter: 3180  \tTraining Loss: -336.9655    \n",
      "    Negative Log Likelihood: 6.6537\tSigma2 Prior: -343.6199\tRegularization: 0.0007\n",
      "Iter: 3190  \tTraining Loss: -337.1049    \n",
      "    Negative Log Likelihood: 6.8032\tSigma2 Prior: -343.9088\tRegularization: 0.0007\n",
      "Iter: 3200  \tTraining Loss: -336.6264    \n",
      "    Negative Log Likelihood: 6.8941\tSigma2 Prior: -343.5212\tRegularization: 0.0007\n",
      "Iter: 3210  \tTraining Loss: -337.6710    \n",
      "    Negative Log Likelihood: 6.7919\tSigma2 Prior: -344.4636\tRegularization: 0.0007\n",
      "Iter: 3220  \tTraining Loss: -338.4955    \n",
      "    Negative Log Likelihood: 6.7333\tSigma2 Prior: -345.2295\tRegularization: 0.0007\n",
      "Iter: 3230  \tTraining Loss: -336.9853    \n",
      "    Negative Log Likelihood: 6.9090\tSigma2 Prior: -343.8950\tRegularization: 0.0007\n",
      "Iter: 3240  \tTraining Loss: -338.5588    \n",
      "    Negative Log Likelihood: 6.7636\tSigma2 Prior: -345.3231\tRegularization: 0.0007\n",
      "Iter: 3250  \tTraining Loss: -338.7538    \n",
      "    Negative Log Likelihood: 6.7692\tSigma2 Prior: -345.5236\tRegularization: 0.0007\n",
      "Iter: 3260  \tTraining Loss: -338.5902    \n",
      "    Negative Log Likelihood: 6.8688\tSigma2 Prior: -345.4597\tRegularization: 0.0007\n",
      "Iter: 3270  \tTraining Loss: -339.3803    \n",
      "    Negative Log Likelihood: 6.8221\tSigma2 Prior: -346.2031\tRegularization: 0.0007\n",
      "Iter: 3280  \tTraining Loss: -339.3524    \n",
      "    Negative Log Likelihood: 6.7982\tSigma2 Prior: -346.1513\tRegularization: 0.0007\n",
      "Iter: 3290  \tTraining Loss: -337.6417    \n",
      "    Negative Log Likelihood: 6.9380\tSigma2 Prior: -344.5803\tRegularization: 0.0007\n",
      "Iter: 3300  \tTraining Loss: -339.8156    \n",
      "    Negative Log Likelihood: 6.7823\tSigma2 Prior: -346.5987\tRegularization: 0.0007\n",
      "Iter: 3310  \tTraining Loss: -339.7778    \n",
      "    Negative Log Likelihood: 6.8131\tSigma2 Prior: -346.5916\tRegularization: 0.0007\n",
      "Iter: 3320  \tTraining Loss: -340.1569    \n",
      "    Negative Log Likelihood: 6.7412\tSigma2 Prior: -346.8988\tRegularization: 0.0007\n",
      "Iter: 3330  \tTraining Loss: -340.9058    \n",
      "    Negative Log Likelihood: 6.7689\tSigma2 Prior: -347.6754\tRegularization: 0.0007\n",
      "Iter: 3340  \tTraining Loss: -339.4138    \n",
      "    Negative Log Likelihood: 6.9640\tSigma2 Prior: -346.3784\tRegularization: 0.0007\n",
      "Iter: 3350  \tTraining Loss: -340.2023    \n",
      "    Negative Log Likelihood: 6.9190\tSigma2 Prior: -347.1220\tRegularization: 0.0007\n",
      "Iter: 3360  \tTraining Loss: -340.0611    \n",
      "    Negative Log Likelihood: 6.9904\tSigma2 Prior: -347.0522\tRegularization: 0.0007\n",
      "Iter: 3370  \tTraining Loss: -340.8104    \n",
      "    Negative Log Likelihood: 6.9351\tSigma2 Prior: -347.7462\tRegularization: 0.0007\n",
      "Iter: 3380  \tTraining Loss: -342.1096    \n",
      "    Negative Log Likelihood: 6.7542\tSigma2 Prior: -348.8645\tRegularization: 0.0007\n",
      "Iter: 3390  \tTraining Loss: -341.4267    \n",
      "    Negative Log Likelihood: 6.9014\tSigma2 Prior: -348.3288\tRegularization: 0.0007\n",
      "Iter: 3400  \tTraining Loss: -340.5369    \n",
      "    Negative Log Likelihood: 7.0061\tSigma2 Prior: -347.5437\tRegularization: 0.0007\n",
      "Iter: 3410  \tTraining Loss: -342.4736    \n",
      "    Negative Log Likelihood: 6.8929\tSigma2 Prior: -349.3672\tRegularization: 0.0007\n",
      "Iter: 3420  \tTraining Loss: -340.5760    \n",
      "    Negative Log Likelihood: 7.0963\tSigma2 Prior: -347.6730\tRegularization: 0.0007\n",
      "Iter: 3430  \tTraining Loss: -341.1327    \n",
      "    Negative Log Likelihood: 7.0346\tSigma2 Prior: -348.1680\tRegularization: 0.0007\n",
      "Iter: 3440  \tTraining Loss: -342.7430    \n",
      "    Negative Log Likelihood: 6.9711\tSigma2 Prior: -349.7148\tRegularization: 0.0007\n",
      "Iter: 3450  \tTraining Loss: -343.4983    \n",
      "    Negative Log Likelihood: 6.8574\tSigma2 Prior: -350.3563\tRegularization: 0.0007\n",
      "Iter: 3460  \tTraining Loss: -342.4937    \n",
      "    Negative Log Likelihood: 7.0601\tSigma2 Prior: -349.5544\tRegularization: 0.0007\n",
      "Iter: 3470  \tTraining Loss: -343.1821    \n",
      "    Negative Log Likelihood: 6.9316\tSigma2 Prior: -350.1144\tRegularization: 0.0007\n",
      "Iter: 3480  \tTraining Loss: -344.1252    \n",
      "    Negative Log Likelihood: 6.8512\tSigma2 Prior: -350.9771\tRegularization: 0.0007\n",
      "Iter: 3490  \tTraining Loss: -343.7923    \n",
      "    Negative Log Likelihood: 6.9953\tSigma2 Prior: -350.7883\tRegularization: 0.0007\n",
      "Iter: 3500  \tTraining Loss: -342.9510    \n",
      "    Negative Log Likelihood: 7.1423\tSigma2 Prior: -350.0940\tRegularization: 0.0007\n",
      "Iter: 3510  \tTraining Loss: -344.6117    \n",
      "    Negative Log Likelihood: 6.9606\tSigma2 Prior: -351.5730\tRegularization: 0.0007\n",
      "Iter: 3520  \tTraining Loss: -344.0825    \n",
      "    Negative Log Likelihood: 6.9666\tSigma2 Prior: -351.0497\tRegularization: 0.0007\n",
      "Iter: 3530  \tTraining Loss: -342.3041    \n",
      "    Negative Log Likelihood: 7.2291\tSigma2 Prior: -349.5338\tRegularization: 0.0007\n",
      "Iter: 3540  \tTraining Loss: -344.6861    \n",
      "    Negative Log Likelihood: 7.0150\tSigma2 Prior: -351.7018\tRegularization: 0.0007\n",
      "Iter: 3550  \tTraining Loss: -343.9868    \n",
      "    Negative Log Likelihood: 7.0790\tSigma2 Prior: -351.0665\tRegularization: 0.0007\n",
      "Iter: 3560  \tTraining Loss: -344.7664    \n",
      "    Negative Log Likelihood: 7.0894\tSigma2 Prior: -351.8565\tRegularization: 0.0007\n",
      "Iter: 3570  \tTraining Loss: -345.4896    \n",
      "    Negative Log Likelihood: 7.0306\tSigma2 Prior: -352.5209\tRegularization: 0.0007\n",
      "Iter: 3580  \tTraining Loss: -344.9252    \n",
      "    Negative Log Likelihood: 7.1647\tSigma2 Prior: -352.0906\tRegularization: 0.0007\n",
      "Iter: 3590  \tTraining Loss: -344.6016    \n",
      "    Negative Log Likelihood: 7.2580\tSigma2 Prior: -351.8602\tRegularization: 0.0007\n",
      "Iter: 3600  \tTraining Loss: -347.1635    \n",
      "    Negative Log Likelihood: 7.0056\tSigma2 Prior: -354.1699\tRegularization: 0.0007\n",
      "Iter: 3610  \tTraining Loss: -346.1255    \n",
      "    Negative Log Likelihood: 7.1087\tSigma2 Prior: -353.2348\tRegularization: 0.0007\n",
      "Iter: 3620  \tTraining Loss: -346.2055    \n",
      "    Negative Log Likelihood: 7.1687\tSigma2 Prior: -353.3748\tRegularization: 0.0007\n",
      "Iter: 3630  \tTraining Loss: -346.5536    \n",
      "    Negative Log Likelihood: 7.1924\tSigma2 Prior: -353.7467\tRegularization: 0.0007\n",
      "Iter: 3640  \tTraining Loss: -347.0970    \n",
      "    Negative Log Likelihood: 7.2213\tSigma2 Prior: -354.3189\tRegularization: 0.0007\n",
      "Iter: 3650  \tTraining Loss: -347.3426    \n",
      "    Negative Log Likelihood: 7.0427\tSigma2 Prior: -354.3860\tRegularization: 0.0007\n",
      "Iter: 3660  \tTraining Loss: -346.1521    \n",
      "    Negative Log Likelihood: 7.3232\tSigma2 Prior: -353.4760\tRegularization: 0.0007\n",
      "Iter: 3670  \tTraining Loss: -346.6389    \n",
      "    Negative Log Likelihood: 7.3033\tSigma2 Prior: -353.9429\tRegularization: 0.0007\n",
      "Iter: 3680  \tTraining Loss: -347.9555    \n",
      "    Negative Log Likelihood: 7.0998\tSigma2 Prior: -355.0561\tRegularization: 0.0007\n",
      "Iter: 3690  \tTraining Loss: -347.4572    \n",
      "    Negative Log Likelihood: 7.2209\tSigma2 Prior: -354.6788\tRegularization: 0.0007\n",
      "Iter: 3700  \tTraining Loss: -346.7592    \n",
      "    Negative Log Likelihood: 7.3987\tSigma2 Prior: -354.1586\tRegularization: 0.0007\n",
      "Iter: 3710  \tTraining Loss: -349.2953    \n",
      "    Negative Log Likelihood: 7.0828\tSigma2 Prior: -356.3789\tRegularization: 0.0007\n",
      "Iter: 3720  \tTraining Loss: -349.0642    \n",
      "    Negative Log Likelihood: 7.1780\tSigma2 Prior: -356.2430\tRegularization: 0.0007\n",
      "Iter: 3730  \tTraining Loss: -348.8306    \n",
      "    Negative Log Likelihood: 7.2324\tSigma2 Prior: -356.0637\tRegularization: 0.0007\n",
      "Iter: 3740  \tTraining Loss: -349.5419    \n",
      "    Negative Log Likelihood: 7.2027\tSigma2 Prior: -356.7453\tRegularization: 0.0007\n",
      "Iter: 3750  \tTraining Loss: -348.8739    \n",
      "    Negative Log Likelihood: 7.2261\tSigma2 Prior: -356.1008\tRegularization: 0.0007\n",
      "Iter: 3760  \tTraining Loss: -349.9408    \n",
      "    Negative Log Likelihood: 7.2524\tSigma2 Prior: -357.1939\tRegularization: 0.0007\n",
      "Iter: 3770  \tTraining Loss: -349.0833    \n",
      "    Negative Log Likelihood: 7.3611\tSigma2 Prior: -356.4451\tRegularization: 0.0007\n",
      "Iter: 3780  \tTraining Loss: -349.1525    \n",
      "    Negative Log Likelihood: 7.3598\tSigma2 Prior: -356.5129\tRegularization: 0.0007\n",
      "Iter: 3790  \tTraining Loss: -350.0498    \n",
      "    Negative Log Likelihood: 7.3947\tSigma2 Prior: -357.4453\tRegularization: 0.0007\n",
      "Iter: 3800  \tTraining Loss: -350.9535    \n",
      "    Negative Log Likelihood: 7.3323\tSigma2 Prior: -358.2864\tRegularization: 0.0007\n",
      "Iter: 3810  \tTraining Loss: -350.2070    \n",
      "    Negative Log Likelihood: 7.3667\tSigma2 Prior: -357.5743\tRegularization: 0.0007\n",
      "Iter: 3820  \tTraining Loss: -350.4734    \n",
      "    Negative Log Likelihood: 7.3795\tSigma2 Prior: -357.8537\tRegularization: 0.0007\n",
      "Iter: 3830  \tTraining Loss: -350.2434    \n",
      "    Negative Log Likelihood: 7.3684\tSigma2 Prior: -357.6125\tRegularization: 0.0007\n",
      "Iter: 3840  \tTraining Loss: -351.2386    \n",
      "    Negative Log Likelihood: 7.2694\tSigma2 Prior: -358.5087\tRegularization: 0.0007\n",
      "Iter: 3850  \tTraining Loss: -351.2763    \n",
      "    Negative Log Likelihood: 7.2072\tSigma2 Prior: -358.4843\tRegularization: 0.0007\n",
      "Iter: 3860  \tTraining Loss: -351.0171    \n",
      "    Negative Log Likelihood: 7.4371\tSigma2 Prior: -358.4549\tRegularization: 0.0007\n",
      "Iter: 3870  \tTraining Loss: -351.4332    \n",
      "    Negative Log Likelihood: 7.4256\tSigma2 Prior: -358.8594\tRegularization: 0.0007\n",
      "Iter: 3880  \tTraining Loss: -352.8879    \n",
      "    Negative Log Likelihood: 7.2519\tSigma2 Prior: -360.1405\tRegularization: 0.0007\n",
      "Iter: 3890  \tTraining Loss: -353.1262    \n",
      "    Negative Log Likelihood: 7.3075\tSigma2 Prior: -360.4344\tRegularization: 0.0007\n",
      "Iter: 3900  \tTraining Loss: -352.1111    \n",
      "    Negative Log Likelihood: 7.4732\tSigma2 Prior: -359.5849\tRegularization: 0.0007\n",
      "Iter: 3910  \tTraining Loss: -351.9731    \n",
      "    Negative Log Likelihood: 7.5782\tSigma2 Prior: -359.5520\tRegularization: 0.0007\n",
      "Iter: 3920  \tTraining Loss: -352.4455    \n",
      "    Negative Log Likelihood: 7.5209\tSigma2 Prior: -359.9671\tRegularization: 0.0007\n",
      "Iter: 3930  \tTraining Loss: -352.6399    \n",
      "    Negative Log Likelihood: 7.5133\tSigma2 Prior: -360.1539\tRegularization: 0.0007\n",
      "Iter: 3940  \tTraining Loss: -353.2353    \n",
      "    Negative Log Likelihood: 7.4711\tSigma2 Prior: -360.7070\tRegularization: 0.0007\n",
      "Iter: 3950  \tTraining Loss: -353.7264    \n",
      "    Negative Log Likelihood: 7.4479\tSigma2 Prior: -361.1750\tRegularization: 0.0007\n",
      "Iter: 3960  \tTraining Loss: -353.6550    \n",
      "    Negative Log Likelihood: 7.5254\tSigma2 Prior: -361.1811\tRegularization: 0.0007\n",
      "Iter: 3970  \tTraining Loss: -352.0786    \n",
      "    Negative Log Likelihood: 7.6858\tSigma2 Prior: -359.7651\tRegularization: 0.0007\n",
      "Iter: 3980  \tTraining Loss: -353.9436    \n",
      "    Negative Log Likelihood: 7.5801\tSigma2 Prior: -361.5244\tRegularization: 0.0007\n",
      "Iter: 3990  \tTraining Loss: -353.5832    \n",
      "    Negative Log Likelihood: 7.6192\tSigma2 Prior: -361.2031\tRegularization: 0.0007\n",
      "Iter: 4000  \tTraining Loss: -354.6551    \n",
      "    Negative Log Likelihood: 7.5339\tSigma2 Prior: -362.1897\tRegularization: 0.0007\n",
      "Iter: 4010  \tTraining Loss: -354.6794    \n",
      "    Negative Log Likelihood: 7.5867\tSigma2 Prior: -362.2668\tRegularization: 0.0007\n",
      "Iter: 4020  \tTraining Loss: -354.2931    \n",
      "    Negative Log Likelihood: 7.6973\tSigma2 Prior: -361.9911\tRegularization: 0.0007\n",
      "Iter: 4030  \tTraining Loss: -353.4371    \n",
      "    Negative Log Likelihood: 7.8362\tSigma2 Prior: -361.2740\tRegularization: 0.0007\n",
      "Iter: 4040  \tTraining Loss: -354.2144    \n",
      "    Negative Log Likelihood: 7.7733\tSigma2 Prior: -361.9884\tRegularization: 0.0007\n",
      "Iter: 4050  \tTraining Loss: -353.9065    \n",
      "    Negative Log Likelihood: 7.7192\tSigma2 Prior: -361.6265\tRegularization: 0.0007\n",
      "Iter: 4060  \tTraining Loss: -355.6024    \n",
      "    Negative Log Likelihood: 7.6962\tSigma2 Prior: -363.2993\tRegularization: 0.0007\n",
      "Iter: 4070  \tTraining Loss: -354.5116    \n",
      "    Negative Log Likelihood: 7.8252\tSigma2 Prior: -362.3375\tRegularization: 0.0007\n",
      "Iter: 4080  \tTraining Loss: -354.1822    \n",
      "    Negative Log Likelihood: 7.8210\tSigma2 Prior: -362.0039\tRegularization: 0.0007\n",
      "Iter: 4090  \tTraining Loss: -358.1354    \n",
      "    Negative Log Likelihood: 7.5098\tSigma2 Prior: -365.6458\tRegularization: 0.0007\n",
      "Iter: 4100  \tTraining Loss: -356.7929    \n",
      "    Negative Log Likelihood: 7.6540\tSigma2 Prior: -364.4477\tRegularization: 0.0007\n",
      "Iter: 4110  \tTraining Loss: -357.4532    \n",
      "    Negative Log Likelihood: 7.7676\tSigma2 Prior: -365.2214\tRegularization: 0.0007\n",
      "Iter: 4120  \tTraining Loss: -355.8930    \n",
      "    Negative Log Likelihood: 7.9492\tSigma2 Prior: -363.8429\tRegularization: 0.0007\n",
      "Iter: 4130  \tTraining Loss: -356.8389    \n",
      "    Negative Log Likelihood: 7.8321\tSigma2 Prior: -364.6717\tRegularization: 0.0007\n",
      "Iter: 4140  \tTraining Loss: -355.4593    \n",
      "    Negative Log Likelihood: 7.9763\tSigma2 Prior: -363.4363\tRegularization: 0.0007\n",
      "Iter: 4150  \tTraining Loss: -358.5663    \n",
      "    Negative Log Likelihood: 7.7252\tSigma2 Prior: -366.2922\tRegularization: 0.0007\n",
      "Iter: 4160  \tTraining Loss: -357.3133    \n",
      "    Negative Log Likelihood: 7.7498\tSigma2 Prior: -365.0638\tRegularization: 0.0007\n",
      "Iter: 4170  \tTraining Loss: -359.7328    \n",
      "    Negative Log Likelihood: 7.4678\tSigma2 Prior: -367.2013\tRegularization: 0.0007\n",
      "Iter: 4180  \tTraining Loss: -359.7832    \n",
      "    Negative Log Likelihood: 7.7529\tSigma2 Prior: -367.5368\tRegularization: 0.0007\n",
      "Iter: 4190  \tTraining Loss: -357.9406    \n",
      "    Negative Log Likelihood: 7.9669\tSigma2 Prior: -365.9083\tRegularization: 0.0007\n",
      "Iter: 4200  \tTraining Loss: -358.7082    \n",
      "    Negative Log Likelihood: 7.7442\tSigma2 Prior: -366.4531\tRegularization: 0.0007\n",
      "Iter: 4210  \tTraining Loss: -358.2816    \n",
      "    Negative Log Likelihood: 7.9474\tSigma2 Prior: -366.2297\tRegularization: 0.0007\n",
      "Iter: 4220  \tTraining Loss: -360.3963    \n",
      "    Negative Log Likelihood: 7.6654\tSigma2 Prior: -368.0624\tRegularization: 0.0007\n",
      "Iter: 4230  \tTraining Loss: -360.4859    \n",
      "    Negative Log Likelihood: 7.6715\tSigma2 Prior: -368.1581\tRegularization: 0.0007\n",
      "Iter: 4240  \tTraining Loss: -360.1075    \n",
      "    Negative Log Likelihood: 7.8384\tSigma2 Prior: -367.9466\tRegularization: 0.0007\n",
      "Iter: 4250  \tTraining Loss: -359.2773    \n",
      "    Negative Log Likelihood: 8.0604\tSigma2 Prior: -367.3385\tRegularization: 0.0007\n",
      "Iter: 4260  \tTraining Loss: -360.8514    \n",
      "    Negative Log Likelihood: 7.7822\tSigma2 Prior: -368.6343\tRegularization: 0.0007\n",
      "Iter: 4270  \tTraining Loss: -359.2723    \n",
      "    Negative Log Likelihood: 8.0823\tSigma2 Prior: -367.3553\tRegularization: 0.0007\n",
      "Iter: 4280  \tTraining Loss: -361.3479    \n",
      "    Negative Log Likelihood: 7.8884\tSigma2 Prior: -369.2370\tRegularization: 0.0007\n",
      "Iter: 4290  \tTraining Loss: -361.7802    \n",
      "    Negative Log Likelihood: 7.8540\tSigma2 Prior: -369.6350\tRegularization: 0.0007\n",
      "Iter: 4300  \tTraining Loss: -361.4160    \n",
      "    Negative Log Likelihood: 8.0196\tSigma2 Prior: -369.4362\tRegularization: 0.0007\n",
      "Iter: 4310  \tTraining Loss: -360.1841    \n",
      "    Negative Log Likelihood: 8.0868\tSigma2 Prior: -368.2715\tRegularization: 0.0007\n",
      "Iter: 4320  \tTraining Loss: -362.3824    \n",
      "    Negative Log Likelihood: 7.9239\tSigma2 Prior: -370.3070\tRegularization: 0.0007\n",
      "Iter: 4330  \tTraining Loss: -362.2165    \n",
      "    Negative Log Likelihood: 8.0081\tSigma2 Prior: -370.2253\tRegularization: 0.0007\n",
      "Iter: 4340  \tTraining Loss: -360.9868    \n",
      "    Negative Log Likelihood: 8.0942\tSigma2 Prior: -369.0817\tRegularization: 0.0007\n",
      "Iter: 4350  \tTraining Loss: -364.0573    \n",
      "    Negative Log Likelihood: 7.8034\tSigma2 Prior: -371.8614\tRegularization: 0.0007\n",
      "Iter: 4360  \tTraining Loss: -363.4102    \n",
      "    Negative Log Likelihood: 8.0038\tSigma2 Prior: -371.4146\tRegularization: 0.0007\n",
      "Iter: 4370  \tTraining Loss: -363.6459    \n",
      "    Negative Log Likelihood: 8.0876\tSigma2 Prior: -371.7342\tRegularization: 0.0007\n",
      "Iter: 4380  \tTraining Loss: -363.0471    \n",
      "    Negative Log Likelihood: 8.1077\tSigma2 Prior: -371.1556\tRegularization: 0.0007\n",
      "Iter: 4390  \tTraining Loss: -363.0507    \n",
      "    Negative Log Likelihood: 8.1797\tSigma2 Prior: -371.2310\tRegularization: 0.0007\n",
      "Iter: 4400  \tTraining Loss: -364.7542    \n",
      "    Negative Log Likelihood: 7.7906\tSigma2 Prior: -372.5454\tRegularization: 0.0007\n",
      "Iter: 4410  \tTraining Loss: -363.1121    \n",
      "    Negative Log Likelihood: 8.1568\tSigma2 Prior: -371.2696\tRegularization: 0.0007\n",
      "Iter: 4420  \tTraining Loss: -363.9117    \n",
      "    Negative Log Likelihood: 8.2323\tSigma2 Prior: -372.1447\tRegularization: 0.0007\n",
      "Iter: 4430  \tTraining Loss: -364.2340    \n",
      "    Negative Log Likelihood: 8.2057\tSigma2 Prior: -372.4405\tRegularization: 0.0007\n",
      "Iter: 4440  \tTraining Loss: -365.0034    \n",
      "    Negative Log Likelihood: 8.1891\tSigma2 Prior: -373.1932\tRegularization: 0.0007\n",
      "Iter: 4450  \tTraining Loss: -366.2377    \n",
      "    Negative Log Likelihood: 8.0309\tSigma2 Prior: -374.2693\tRegularization: 0.0007\n",
      "Iter: 4460  \tTraining Loss: -365.8862    \n",
      "    Negative Log Likelihood: 8.2016\tSigma2 Prior: -374.0885\tRegularization: 0.0007\n",
      "Iter: 4470  \tTraining Loss: -366.3636    \n",
      "    Negative Log Likelihood: 8.2633\tSigma2 Prior: -374.6277\tRegularization: 0.0007\n",
      "Iter: 4480  \tTraining Loss: -366.7124    \n",
      "    Negative Log Likelihood: 8.1603\tSigma2 Prior: -374.8734\tRegularization: 0.0007\n",
      "Iter: 4490  \tTraining Loss: -365.4984    \n",
      "    Negative Log Likelihood: 8.3440\tSigma2 Prior: -373.8430\tRegularization: 0.0007\n",
      "Iter: 4500  \tTraining Loss: -366.2606    \n",
      "    Negative Log Likelihood: 8.3070\tSigma2 Prior: -374.5684\tRegularization: 0.0007\n",
      "Iter: 4510  \tTraining Loss: -366.6632    \n",
      "    Negative Log Likelihood: 8.3195\tSigma2 Prior: -374.9834\tRegularization: 0.0007\n",
      "Iter: 4520  \tTraining Loss: -367.9415    \n",
      "    Negative Log Likelihood: 8.2039\tSigma2 Prior: -376.1461\tRegularization: 0.0007\n",
      "Iter: 4530  \tTraining Loss: -368.4339    \n",
      "    Negative Log Likelihood: 8.2276\tSigma2 Prior: -376.6622\tRegularization: 0.0007\n",
      "Iter: 4540  \tTraining Loss: -366.7278    \n",
      "    Negative Log Likelihood: 8.4019\tSigma2 Prior: -375.1304\tRegularization: 0.0007\n",
      "Iter: 4550  \tTraining Loss: -367.4325    \n",
      "    Negative Log Likelihood: 8.3650\tSigma2 Prior: -375.7982\tRegularization: 0.0007\n",
      "Iter: 4560  \tTraining Loss: -369.3748    \n",
      "    Negative Log Likelihood: 8.2185\tSigma2 Prior: -377.5941\tRegularization: 0.0007\n",
      "Iter: 4570  \tTraining Loss: -368.2600    \n",
      "    Negative Log Likelihood: 8.3218\tSigma2 Prior: -376.5826\tRegularization: 0.0007\n",
      "Iter: 4580  \tTraining Loss: -367.5193    \n",
      "    Negative Log Likelihood: 8.6359\tSigma2 Prior: -376.1559\tRegularization: 0.0007\n",
      "Iter: 4590  \tTraining Loss: -369.5057    \n",
      "    Negative Log Likelihood: 8.2667\tSigma2 Prior: -377.7731\tRegularization: 0.0007\n",
      "Iter: 4600  \tTraining Loss: -369.6668    \n",
      "    Negative Log Likelihood: 8.2819\tSigma2 Prior: -377.9494\tRegularization: 0.0007\n",
      "Iter: 4610  \tTraining Loss: -369.8767    \n",
      "    Negative Log Likelihood: 8.2831\tSigma2 Prior: -378.1606\tRegularization: 0.0007\n",
      "Iter: 4620  \tTraining Loss: -370.5316    \n",
      "    Negative Log Likelihood: 8.2820\tSigma2 Prior: -378.8143\tRegularization: 0.0007\n",
      "Iter: 4630  \tTraining Loss: -370.5816    \n",
      "    Negative Log Likelihood: 8.2609\tSigma2 Prior: -378.8433\tRegularization: 0.0007\n",
      "Iter: 4640  \tTraining Loss: -369.9134    \n",
      "    Negative Log Likelihood: 8.5594\tSigma2 Prior: -378.4735\tRegularization: 0.0007\n",
      "Iter: 4650  \tTraining Loss: -372.6862    \n",
      "    Negative Log Likelihood: 8.3197\tSigma2 Prior: -381.0067\tRegularization: 0.0007\n",
      "Iter: 4660  \tTraining Loss: -370.2884    \n",
      "    Negative Log Likelihood: 8.5721\tSigma2 Prior: -378.8612\tRegularization: 0.0007\n",
      "Iter: 4670  \tTraining Loss: -367.6456    \n",
      "    Negative Log Likelihood: 8.8483\tSigma2 Prior: -376.4946\tRegularization: 0.0007\n",
      "Iter: 4680  \tTraining Loss: -371.9703    \n",
      "    Negative Log Likelihood: 8.4915\tSigma2 Prior: -380.4625\tRegularization: 0.0007\n",
      "Iter: 4690  \tTraining Loss: -372.3598    \n",
      "    Negative Log Likelihood: 8.3742\tSigma2 Prior: -380.7346\tRegularization: 0.0007\n",
      "Iter: 4700  \tTraining Loss: -372.5005    \n",
      "    Negative Log Likelihood: 8.5061\tSigma2 Prior: -381.0074\tRegularization: 0.0007\n",
      "Iter: 4710  \tTraining Loss: -372.3775    \n",
      "    Negative Log Likelihood: 8.6168\tSigma2 Prior: -380.9951\tRegularization: 0.0007\n",
      "Iter: 4720  \tTraining Loss: -372.7301    \n",
      "    Negative Log Likelihood: 8.5909\tSigma2 Prior: -381.3218\tRegularization: 0.0007\n",
      "Iter: 4730  \tTraining Loss: -371.9225    \n",
      "    Negative Log Likelihood: 8.4484\tSigma2 Prior: -380.3716\tRegularization: 0.0007\n",
      "Iter: 4740  \tTraining Loss: -372.9481    \n",
      "    Negative Log Likelihood: 8.4978\tSigma2 Prior: -381.4466\tRegularization: 0.0007\n",
      "Iter: 4750  \tTraining Loss: -373.1245    \n",
      "    Negative Log Likelihood: 8.7112\tSigma2 Prior: -381.8365\tRegularization: 0.0007\n",
      "Iter: 4760  \tTraining Loss: -373.9427    \n",
      "    Negative Log Likelihood: 8.4977\tSigma2 Prior: -382.4411\tRegularization: 0.0007\n",
      "Iter: 4770  \tTraining Loss: -374.2337    \n",
      "    Negative Log Likelihood: 8.5934\tSigma2 Prior: -382.8278\tRegularization: 0.0007\n",
      "Iter: 4780  \tTraining Loss: -374.3029    \n",
      "    Negative Log Likelihood: 8.6888\tSigma2 Prior: -382.9924\tRegularization: 0.0007\n",
      "Iter: 4790  \tTraining Loss: -375.1181    \n",
      "    Negative Log Likelihood: 8.5069\tSigma2 Prior: -383.6257\tRegularization: 0.0007\n",
      "Iter: 4800  \tTraining Loss: -374.4641    \n",
      "    Negative Log Likelihood: 8.7631\tSigma2 Prior: -383.2279\tRegularization: 0.0007\n",
      "Iter: 4810  \tTraining Loss: -374.7329    \n",
      "    Negative Log Likelihood: 8.7216\tSigma2 Prior: -383.4552\tRegularization: 0.0007\n",
      "Iter: 4820  \tTraining Loss: -375.3632    \n",
      "    Negative Log Likelihood: 8.7202\tSigma2 Prior: -384.0842\tRegularization: 0.0007\n",
      "Iter: 4830  \tTraining Loss: -375.3656    \n",
      "    Negative Log Likelihood: 8.6963\tSigma2 Prior: -384.0626\tRegularization: 0.0007\n",
      "Iter: 4840  \tTraining Loss: -375.0497    \n",
      "    Negative Log Likelihood: 8.8085\tSigma2 Prior: -383.8590\tRegularization: 0.0007\n",
      "Iter: 4850  \tTraining Loss: -375.4560    \n",
      "    Negative Log Likelihood: 8.8723\tSigma2 Prior: -384.3290\tRegularization: 0.0007\n",
      "Iter: 4860  \tTraining Loss: -376.8749    \n",
      "    Negative Log Likelihood: 8.8207\tSigma2 Prior: -385.6964\tRegularization: 0.0007\n",
      "Iter: 4870  \tTraining Loss: -376.9597    \n",
      "    Negative Log Likelihood: 8.8507\tSigma2 Prior: -385.8111\tRegularization: 0.0007\n",
      "Iter: 4880  \tTraining Loss: -379.5643    \n",
      "    Negative Log Likelihood: 8.5900\tSigma2 Prior: -388.1550\tRegularization: 0.0007\n",
      "Iter: 4890  \tTraining Loss: -378.2050    \n",
      "    Negative Log Likelihood: 8.8161\tSigma2 Prior: -387.0219\tRegularization: 0.0007\n",
      "Iter: 4900  \tTraining Loss: -376.6243    \n",
      "    Negative Log Likelihood: 8.8501\tSigma2 Prior: -385.4751\tRegularization: 0.0007\n",
      "Iter: 4910  \tTraining Loss: -379.3544    \n",
      "    Negative Log Likelihood: 8.6069\tSigma2 Prior: -387.9621\tRegularization: 0.0007\n",
      "Iter: 4920  \tTraining Loss: -379.0369    \n",
      "    Negative Log Likelihood: 8.7261\tSigma2 Prior: -387.7637\tRegularization: 0.0007\n",
      "Iter: 4930  \tTraining Loss: -378.7558    \n",
      "    Negative Log Likelihood: 8.8374\tSigma2 Prior: -387.5939\tRegularization: 0.0007\n",
      "Iter: 4940  \tTraining Loss: -378.1738    \n",
      "    Negative Log Likelihood: 9.0404\tSigma2 Prior: -387.2150\tRegularization: 0.0007\n",
      "Iter: 4950  \tTraining Loss: -378.7095    \n",
      "    Negative Log Likelihood: 9.1044\tSigma2 Prior: -387.8147\tRegularization: 0.0007\n",
      "Iter: 4960  \tTraining Loss: -378.7897    \n",
      "    Negative Log Likelihood: 9.0617\tSigma2 Prior: -387.8521\tRegularization: 0.0007\n",
      "Iter: 4970  \tTraining Loss: -379.7474    \n",
      "    Negative Log Likelihood: 8.9147\tSigma2 Prior: -388.6628\tRegularization: 0.0007\n",
      "Iter: 4980  \tTraining Loss: -377.9915    \n",
      "    Negative Log Likelihood: 9.3053\tSigma2 Prior: -387.2975\tRegularization: 0.0007\n",
      "Iter: 4990  \tTraining Loss: -378.8459    \n",
      "    Negative Log Likelihood: 9.1909\tSigma2 Prior: -388.0376\tRegularization: 0.0007\n",
      "Iter: 5000  \tTraining Loss: -380.3848    \n",
      "    Negative Log Likelihood: 9.1198\tSigma2 Prior: -389.5052\tRegularization: 0.0007\n",
      "Iter: 5010  \tTraining Loss: -380.6115    \n",
      "    Negative Log Likelihood: 8.9339\tSigma2 Prior: -389.5461\tRegularization: 0.0007\n",
      "Iter: 5020  \tTraining Loss: -380.3232    \n",
      "    Negative Log Likelihood: 9.1940\tSigma2 Prior: -389.5179\tRegularization: 0.0007\n",
      "Iter: 5030  \tTraining Loss: -380.8103    \n",
      "    Negative Log Likelihood: 9.1466\tSigma2 Prior: -389.9577\tRegularization: 0.0007\n",
      "Iter: 5040  \tTraining Loss: -382.3035    \n",
      "    Negative Log Likelihood: 8.9810\tSigma2 Prior: -391.2852\tRegularization: 0.0007\n",
      "Iter: 5050  \tTraining Loss: -383.2121    \n",
      "    Negative Log Likelihood: 8.9812\tSigma2 Prior: -392.1940\tRegularization: 0.0007\n",
      "Iter: 5060  \tTraining Loss: -379.9612    \n",
      "    Negative Log Likelihood: 9.4841\tSigma2 Prior: -389.4461\tRegularization: 0.0007\n",
      "Iter: 5070  \tTraining Loss: -382.1098    \n",
      "    Negative Log Likelihood: 9.2331\tSigma2 Prior: -391.3437\tRegularization: 0.0007\n",
      "Iter: 5080  \tTraining Loss: -383.1888    \n",
      "    Negative Log Likelihood: 9.2091\tSigma2 Prior: -392.3987\tRegularization: 0.0007\n",
      "Iter: 5090  \tTraining Loss: -382.4076    \n",
      "    Negative Log Likelihood: 9.2916\tSigma2 Prior: -391.6999\tRegularization: 0.0007\n",
      "Iter: 5100  \tTraining Loss: -382.0400    \n",
      "    Negative Log Likelihood: 9.4076\tSigma2 Prior: -391.4483\tRegularization: 0.0007\n",
      "Iter: 5110  \tTraining Loss: -384.6131    \n",
      "    Negative Log Likelihood: 9.2060\tSigma2 Prior: -393.8199\tRegularization: 0.0007\n",
      "Iter: 5120  \tTraining Loss: -385.6287    \n",
      "    Negative Log Likelihood: 9.1271\tSigma2 Prior: -394.7565\tRegularization: 0.0007\n",
      "Iter: 5130  \tTraining Loss: -384.2973    \n",
      "    Negative Log Likelihood: 9.4088\tSigma2 Prior: -393.7068\tRegularization: 0.0007\n",
      "Iter: 5140  \tTraining Loss: -385.0323    \n",
      "    Negative Log Likelihood: 9.3433\tSigma2 Prior: -394.3764\tRegularization: 0.0007\n",
      "Iter: 5150  \tTraining Loss: -383.5993    \n",
      "    Negative Log Likelihood: 9.5882\tSigma2 Prior: -393.1883\tRegularization: 0.0007\n",
      "Iter: 5160  \tTraining Loss: -385.3391    \n",
      "    Negative Log Likelihood: 9.4887\tSigma2 Prior: -394.8285\tRegularization: 0.0007\n",
      "Iter: 5170  \tTraining Loss: -386.6294    \n",
      "    Negative Log Likelihood: 9.3986\tSigma2 Prior: -396.0287\tRegularization: 0.0007\n",
      "Iter: 5180  \tTraining Loss: -384.7144    \n",
      "    Negative Log Likelihood: 9.6538\tSigma2 Prior: -394.3689\tRegularization: 0.0007\n",
      "Iter: 5190  \tTraining Loss: -386.1821    \n",
      "    Negative Log Likelihood: 9.5450\tSigma2 Prior: -395.7278\tRegularization: 0.0007\n",
      "Iter: 5200  \tTraining Loss: -387.9022    \n",
      "    Negative Log Likelihood: 9.3934\tSigma2 Prior: -397.2963\tRegularization: 0.0007\n",
      "Iter: 5210  \tTraining Loss: -388.5825    \n",
      "    Negative Log Likelihood: 9.2679\tSigma2 Prior: -397.8512\tRegularization: 0.0007\n",
      "Iter: 5220  \tTraining Loss: -387.2452    \n",
      "    Negative Log Likelihood: 9.4507\tSigma2 Prior: -396.6966\tRegularization: 0.0007\n",
      "Iter: 5230  \tTraining Loss: -388.0601    \n",
      "    Negative Log Likelihood: 9.4643\tSigma2 Prior: -397.5251\tRegularization: 0.0007\n",
      "Iter: 5240  \tTraining Loss: -387.8616    \n",
      "    Negative Log Likelihood: 9.6499\tSigma2 Prior: -397.5122\tRegularization: 0.0007\n",
      "Iter: 5250  \tTraining Loss: -385.0074    \n",
      "    Negative Log Likelihood: 9.8702\tSigma2 Prior: -394.8784\tRegularization: 0.0007\n",
      "Iter: 5260  \tTraining Loss: -387.8433    \n",
      "    Negative Log Likelihood: 9.8624\tSigma2 Prior: -397.7064\tRegularization: 0.0007\n",
      "Iter: 5270  \tTraining Loss: -387.5747    \n",
      "    Negative Log Likelihood: 9.8981\tSigma2 Prior: -397.4735\tRegularization: 0.0007\n",
      "Iter: 5280  \tTraining Loss: -386.0898    \n",
      "    Negative Log Likelihood: 9.9889\tSigma2 Prior: -396.0795\tRegularization: 0.0007\n",
      "Iter: 5290  \tTraining Loss: -389.4252    \n",
      "    Negative Log Likelihood: 9.6660\tSigma2 Prior: -399.0919\tRegularization: 0.0007\n",
      "Iter: 5300  \tTraining Loss: -388.2606    \n",
      "    Negative Log Likelihood: 9.8188\tSigma2 Prior: -398.0801\tRegularization: 0.0007\n",
      "Iter: 5310  \tTraining Loss: -389.0029    \n",
      "    Negative Log Likelihood: 9.7883\tSigma2 Prior: -398.7920\tRegularization: 0.0007\n",
      "Iter: 5320  \tTraining Loss: -390.6930    \n",
      "    Negative Log Likelihood: 9.6816\tSigma2 Prior: -400.3753\tRegularization: 0.0007\n",
      "Iter: 5330  \tTraining Loss: -388.2054    \n",
      "    Negative Log Likelihood: 10.1501\tSigma2 Prior: -398.3563\tRegularization: 0.0007\n",
      "Iter: 5340  \tTraining Loss: -391.4434    \n",
      "    Negative Log Likelihood: 9.8406\tSigma2 Prior: -401.2848\tRegularization: 0.0007\n",
      "Iter: 5350  \tTraining Loss: -391.9395    \n",
      "    Negative Log Likelihood: 9.8387\tSigma2 Prior: -401.7789\tRegularization: 0.0007\n",
      "Iter: 5360  \tTraining Loss: -390.6252    \n",
      "    Negative Log Likelihood: 9.9630\tSigma2 Prior: -400.5889\tRegularization: 0.0007\n",
      "Iter: 5370  \tTraining Loss: -391.6591    \n",
      "    Negative Log Likelihood: 10.0045\tSigma2 Prior: -401.6644\tRegularization: 0.0007\n",
      "Iter: 5380  \tTraining Loss: -392.9934    \n",
      "    Negative Log Likelihood: 9.9080\tSigma2 Prior: -402.9022\tRegularization: 0.0007\n",
      "Iter: 5390  \tTraining Loss: -394.1468    \n",
      "    Negative Log Likelihood: 9.7986\tSigma2 Prior: -403.9462\tRegularization: 0.0007\n",
      "Iter: 5400  \tTraining Loss: -393.2414    \n",
      "    Negative Log Likelihood: 10.0697\tSigma2 Prior: -403.3118\tRegularization: 0.0007\n",
      "Iter: 5410  \tTraining Loss: -393.3973    \n",
      "    Negative Log Likelihood: 9.9802\tSigma2 Prior: -403.3782\tRegularization: 0.0008\n",
      "Iter: 5420  \tTraining Loss: -390.4988    \n",
      "    Negative Log Likelihood: 10.3115\tSigma2 Prior: -400.8111\tRegularization: 0.0008\n",
      "Iter: 5430  \tTraining Loss: -394.1792    \n",
      "    Negative Log Likelihood: 9.9439\tSigma2 Prior: -404.1239\tRegularization: 0.0008\n",
      "Iter: 5440  \tTraining Loss: -393.0698    \n",
      "    Negative Log Likelihood: 10.1547\tSigma2 Prior: -403.2253\tRegularization: 0.0008\n",
      "Iter: 5450  \tTraining Loss: -394.7921    \n",
      "    Negative Log Likelihood: 10.0148\tSigma2 Prior: -404.8077\tRegularization: 0.0008\n",
      "Iter: 5460  \tTraining Loss: -396.5325    \n",
      "    Negative Log Likelihood: 9.9514\tSigma2 Prior: -406.4847\tRegularization: 0.0008\n",
      "Iter: 5470  \tTraining Loss: -395.5684    \n",
      "    Negative Log Likelihood: 10.1397\tSigma2 Prior: -405.7089\tRegularization: 0.0008\n",
      "Iter: 5480  \tTraining Loss: -394.7979    \n",
      "    Negative Log Likelihood: 10.3456\tSigma2 Prior: -405.1443\tRegularization: 0.0008\n",
      "Iter: 5490  \tTraining Loss: -390.7990    \n",
      "    Negative Log Likelihood: 10.5727\tSigma2 Prior: -401.3724\tRegularization: 0.0008\n",
      "Iter: 5500  \tTraining Loss: -394.3732    \n",
      "    Negative Log Likelihood: 10.5051\tSigma2 Prior: -404.8790\tRegularization: 0.0008\n",
      "Iter: 5510  \tTraining Loss: -396.1399    \n",
      "    Negative Log Likelihood: 10.2404\tSigma2 Prior: -406.3810\tRegularization: 0.0008\n",
      "Iter: 5520  \tTraining Loss: -397.4364    \n",
      "    Negative Log Likelihood: 10.1299\tSigma2 Prior: -407.5670\tRegularization: 0.0008\n",
      "Iter: 5530  \tTraining Loss: -397.7971    \n",
      "    Negative Log Likelihood: 10.1252\tSigma2 Prior: -407.9230\tRegularization: 0.0008\n",
      "Iter: 5540  \tTraining Loss: -395.1267    \n",
      "    Negative Log Likelihood: 10.5458\tSigma2 Prior: -405.6732\tRegularization: 0.0008\n",
      "Iter: 5550  \tTraining Loss: -397.5906    \n",
      "    Negative Log Likelihood: 10.3495\tSigma2 Prior: -407.9409\tRegularization: 0.0008\n",
      "Iter: 5560  \tTraining Loss: -398.3683    \n",
      "    Negative Log Likelihood: 10.4400\tSigma2 Prior: -408.8091\tRegularization: 0.0008\n",
      "Iter: 5570  \tTraining Loss: -397.5934    \n",
      "    Negative Log Likelihood: 10.5792\tSigma2 Prior: -408.1734\tRegularization: 0.0008\n",
      "Iter: 5580  \tTraining Loss: -400.0881    \n",
      "    Negative Log Likelihood: 10.3235\tSigma2 Prior: -410.4124\tRegularization: 0.0008\n",
      "Iter: 5590  \tTraining Loss: -398.1813    \n",
      "    Negative Log Likelihood: 10.7349\tSigma2 Prior: -408.9170\tRegularization: 0.0008\n",
      "Iter: 5600  \tTraining Loss: -400.5246    \n",
      "    Negative Log Likelihood: 10.3217\tSigma2 Prior: -410.8471\tRegularization: 0.0008\n",
      "Iter: 5610  \tTraining Loss: -399.2648    \n",
      "    Negative Log Likelihood: 10.5970\tSigma2 Prior: -409.8625\tRegularization: 0.0008\n",
      "Iter: 5620  \tTraining Loss: -399.8685    \n",
      "    Negative Log Likelihood: 10.5067\tSigma2 Prior: -410.3759\tRegularization: 0.0008\n",
      "Iter: 5630  \tTraining Loss: -397.9132    \n",
      "    Negative Log Likelihood: 10.8732\tSigma2 Prior: -408.7872\tRegularization: 0.0008\n",
      "Iter: 5640  \tTraining Loss: -398.9012    \n",
      "    Negative Log Likelihood: 10.8570\tSigma2 Prior: -409.7589\tRegularization: 0.0008\n",
      "Iter: 5650  \tTraining Loss: -399.9983    \n",
      "    Negative Log Likelihood: 10.7096\tSigma2 Prior: -410.7086\tRegularization: 0.0008\n",
      "Iter: 5660  \tTraining Loss: -399.0416    \n",
      "    Negative Log Likelihood: 10.9499\tSigma2 Prior: -409.9922\tRegularization: 0.0008\n",
      "Iter: 5670  \tTraining Loss: -402.6693    \n",
      "    Negative Log Likelihood: 10.5498\tSigma2 Prior: -413.2199\tRegularization: 0.0008\n",
      "Iter: 5680  \tTraining Loss: -402.0179    \n",
      "    Negative Log Likelihood: 10.7626\tSigma2 Prior: -412.7812\tRegularization: 0.0008\n",
      "Iter: 5690  \tTraining Loss: -401.7222    \n",
      "    Negative Log Likelihood: 10.8427\tSigma2 Prior: -412.5656\tRegularization: 0.0008\n",
      "Iter: 5700  \tTraining Loss: -404.0833    \n",
      "    Negative Log Likelihood: 10.6287\tSigma2 Prior: -414.7127\tRegularization: 0.0008\n",
      "Iter: 5710  \tTraining Loss: -404.1234    \n",
      "    Negative Log Likelihood: 10.6948\tSigma2 Prior: -414.8190\tRegularization: 0.0008\n",
      "Iter: 5720  \tTraining Loss: -402.2147    \n",
      "    Negative Log Likelihood: 11.1502\tSigma2 Prior: -413.3656\tRegularization: 0.0008\n",
      "Iter: 5730  \tTraining Loss: -404.5722    \n",
      "    Negative Log Likelihood: 10.7331\tSigma2 Prior: -415.3060\tRegularization: 0.0008\n",
      "Iter: 5740  \tTraining Loss: -405.9980    \n",
      "    Negative Log Likelihood: 10.6052\tSigma2 Prior: -416.6040\tRegularization: 0.0008\n",
      "Iter: 5750  \tTraining Loss: -405.2431    \n",
      "    Negative Log Likelihood: 11.0384\tSigma2 Prior: -416.2823\tRegularization: 0.0008\n",
      "Iter: 5760  \tTraining Loss: -405.7958    \n",
      "    Negative Log Likelihood: 11.1180\tSigma2 Prior: -416.9145\tRegularization: 0.0008\n",
      "Iter: 5770  \tTraining Loss: -405.8383    \n",
      "    Negative Log Likelihood: 11.0138\tSigma2 Prior: -416.8529\tRegularization: 0.0008\n",
      "Iter: 5780  \tTraining Loss: -401.1591    \n",
      "    Negative Log Likelihood: 11.6446\tSigma2 Prior: -412.8045\tRegularization: 0.0008\n",
      "Iter: 5790  \tTraining Loss: -407.5348    \n",
      "    Negative Log Likelihood: 10.9202\tSigma2 Prior: -418.4558\tRegularization: 0.0008\n",
      "Iter: 5800  \tTraining Loss: -405.7422    \n",
      "    Negative Log Likelihood: 11.2397\tSigma2 Prior: -416.9826\tRegularization: 0.0008\n",
      "Iter: 5810  \tTraining Loss: -403.7387    \n",
      "    Negative Log Likelihood: 11.7043\tSigma2 Prior: -415.4437\tRegularization: 0.0008\n",
      "Iter: 5820  \tTraining Loss: -405.4094    \n",
      "    Negative Log Likelihood: 11.5778\tSigma2 Prior: -416.9879\tRegularization: 0.0008\n",
      "Iter: 5830  \tTraining Loss: -405.9645    \n",
      "    Negative Log Likelihood: 11.3169\tSigma2 Prior: -417.2822\tRegularization: 0.0008\n",
      "Iter: 5840  \tTraining Loss: -407.2695    \n",
      "    Negative Log Likelihood: 11.3052\tSigma2 Prior: -418.5755\tRegularization: 0.0008\n",
      "Iter: 5850  \tTraining Loss: -404.2359    \n",
      "    Negative Log Likelihood: 11.7957\tSigma2 Prior: -416.0323\tRegularization: 0.0008\n",
      "Iter: 5860  \tTraining Loss: -405.5789    \n",
      "    Negative Log Likelihood: 11.6839\tSigma2 Prior: -417.2637\tRegularization: 0.0008\n",
      "Iter: 5870  \tTraining Loss: -408.7965    \n",
      "    Negative Log Likelihood: 11.3350\tSigma2 Prior: -420.1324\tRegularization: 0.0008\n",
      "Iter: 5880  \tTraining Loss: -411.1056    \n",
      "    Negative Log Likelihood: 11.0630\tSigma2 Prior: -422.1693\tRegularization: 0.0008\n",
      "Iter: 5890  \tTraining Loss: -408.9061    \n",
      "    Negative Log Likelihood: 11.5819\tSigma2 Prior: -420.4887\tRegularization: 0.0008\n",
      "Iter: 5900  \tTraining Loss: -409.1619    \n",
      "    Negative Log Likelihood: 11.5433\tSigma2 Prior: -420.7060\tRegularization: 0.0008\n",
      "Iter: 5910  \tTraining Loss: -407.8236    \n",
      "    Negative Log Likelihood: 11.8970\tSigma2 Prior: -419.7214\tRegularization: 0.0008\n",
      "Iter: 5920  \tTraining Loss: -413.1676    \n",
      "    Negative Log Likelihood: 11.3613\tSigma2 Prior: -424.5296\tRegularization: 0.0008\n",
      "Iter: 5930  \tTraining Loss: -412.2451    \n",
      "    Negative Log Likelihood: 11.6822\tSigma2 Prior: -423.9281\tRegularization: 0.0008\n",
      "Iter: 5940  \tTraining Loss: -412.0950    \n",
      "    Negative Log Likelihood: 11.7898\tSigma2 Prior: -423.8855\tRegularization: 0.0008\n",
      "Iter: 5950  \tTraining Loss: -412.6134    \n",
      "    Negative Log Likelihood: 11.6693\tSigma2 Prior: -424.2835\tRegularization: 0.0008\n",
      "Iter: 5960  \tTraining Loss: -412.3999    \n",
      "    Negative Log Likelihood: 11.6800\tSigma2 Prior: -424.0807\tRegularization: 0.0008\n",
      "Iter: 5970  \tTraining Loss: -412.1035    \n",
      "    Negative Log Likelihood: 11.9472\tSigma2 Prior: -424.0515\tRegularization: 0.0008\n",
      "Iter: 5980  \tTraining Loss: -413.6921    \n",
      "    Negative Log Likelihood: 11.7737\tSigma2 Prior: -425.4666\tRegularization: 0.0008\n",
      "Iter: 5990  \tTraining Loss: -411.8175    \n",
      "    Negative Log Likelihood: 12.0598\tSigma2 Prior: -423.8781\tRegularization: 0.0008\n",
      "Iter: 6000  \tTraining Loss: -413.3246    \n",
      "    Negative Log Likelihood: 11.9677\tSigma2 Prior: -425.2932\tRegularization: 0.0008\n",
      "Iter: 6010  \tTraining Loss: -415.3097    \n",
      "    Negative Log Likelihood: 11.9920\tSigma2 Prior: -427.3025\tRegularization: 0.0008\n",
      "Iter: 6020  \tTraining Loss: -415.4196    \n",
      "    Negative Log Likelihood: 11.9587\tSigma2 Prior: -427.3792\tRegularization: 0.0008\n",
      "Iter: 6030  \tTraining Loss: -414.4340    \n",
      "    Negative Log Likelihood: 12.0316\tSigma2 Prior: -426.4663\tRegularization: 0.0008\n",
      "Iter: 6040  \tTraining Loss: -416.2329    \n",
      "    Negative Log Likelihood: 11.7707\tSigma2 Prior: -428.0043\tRegularization: 0.0008\n",
      "Iter: 6050  \tTraining Loss: -416.4995    \n",
      "    Negative Log Likelihood: 12.1629\tSigma2 Prior: -428.6632\tRegularization: 0.0008\n",
      "Iter: 6060  \tTraining Loss: -418.1663    \n",
      "    Negative Log Likelihood: 11.9662\tSigma2 Prior: -430.1333\tRegularization: 0.0008\n",
      "Iter: 6070  \tTraining Loss: -416.2561    \n",
      "    Negative Log Likelihood: 12.1807\tSigma2 Prior: -428.4376\tRegularization: 0.0008\n",
      "Iter: 6080  \tTraining Loss: -417.2584    \n",
      "    Negative Log Likelihood: 12.2103\tSigma2 Prior: -429.4695\tRegularization: 0.0008\n",
      "Iter: 6090  \tTraining Loss: -416.7841    \n",
      "    Negative Log Likelihood: 12.3116\tSigma2 Prior: -429.0965\tRegularization: 0.0008\n",
      "Iter: 6100  \tTraining Loss: -417.5891    \n",
      "    Negative Log Likelihood: 12.4665\tSigma2 Prior: -430.0564\tRegularization: 0.0008\n",
      "Iter: 6110  \tTraining Loss: -419.1419    \n",
      "    Negative Log Likelihood: 12.2831\tSigma2 Prior: -431.4257\tRegularization: 0.0008\n",
      "Iter: 6120  \tTraining Loss: -419.0495    \n",
      "    Negative Log Likelihood: 12.1786\tSigma2 Prior: -431.2288\tRegularization: 0.0008\n",
      "Iter: 6130  \tTraining Loss: -417.1226    \n",
      "    Negative Log Likelihood: 12.6935\tSigma2 Prior: -429.8168\tRegularization: 0.0008\n",
      "Iter: 6140  \tTraining Loss: -417.7141    \n",
      "    Negative Log Likelihood: 12.7935\tSigma2 Prior: -430.5084\tRegularization: 0.0008\n",
      "Iter: 6150  \tTraining Loss: -421.3411    \n",
      "    Negative Log Likelihood: 12.4805\tSigma2 Prior: -433.8224\tRegularization: 0.0008\n",
      "Iter: 6160  \tTraining Loss: -422.8499    \n",
      "    Negative Log Likelihood: 12.0514\tSigma2 Prior: -434.9021\tRegularization: 0.0008\n",
      "Iter: 6170  \tTraining Loss: -419.7143    \n",
      "    Negative Log Likelihood: 12.7521\tSigma2 Prior: -432.4671\tRegularization: 0.0008\n",
      "Iter: 6180  \tTraining Loss: -420.9679    \n",
      "    Negative Log Likelihood: 12.5943\tSigma2 Prior: -433.5630\tRegularization: 0.0008\n",
      "Iter: 6190  \tTraining Loss: -419.7740    \n",
      "    Negative Log Likelihood: 13.0343\tSigma2 Prior: -432.8090\tRegularization: 0.0008\n",
      "Iter: 6200  \tTraining Loss: -421.8947    \n",
      "    Negative Log Likelihood: 12.5071\tSigma2 Prior: -434.4025\tRegularization: 0.0008\n",
      "Iter: 6210  \tTraining Loss: -424.0486    \n",
      "    Negative Log Likelihood: 12.5434\tSigma2 Prior: -436.5928\tRegularization: 0.0008\n",
      "Iter: 6220  \tTraining Loss: -423.2367    \n",
      "    Negative Log Likelihood: 12.9092\tSigma2 Prior: -436.1467\tRegularization: 0.0008\n",
      "Iter: 6230  \tTraining Loss: -423.3189    \n",
      "    Negative Log Likelihood: 12.9511\tSigma2 Prior: -436.2708\tRegularization: 0.0008\n",
      "Iter: 6240  \tTraining Loss: -422.4265    \n",
      "    Negative Log Likelihood: 12.9962\tSigma2 Prior: -435.4234\tRegularization: 0.0008\n",
      "Iter: 6250  \tTraining Loss: -424.3519    \n",
      "    Negative Log Likelihood: 12.8151\tSigma2 Prior: -437.1678\tRegularization: 0.0008\n",
      "Iter: 6260  \tTraining Loss: -423.4404    \n",
      "    Negative Log Likelihood: 13.0649\tSigma2 Prior: -436.5060\tRegularization: 0.0008\n",
      "Iter: 6270  \tTraining Loss: -425.8655    \n",
      "    Negative Log Likelihood: 13.0303\tSigma2 Prior: -438.8966\tRegularization: 0.0008\n",
      "Iter: 6280  \tTraining Loss: -425.7946    \n",
      "    Negative Log Likelihood: 13.1736\tSigma2 Prior: -438.9691\tRegularization: 0.0008\n",
      "Iter: 6290  \tTraining Loss: -426.2847    \n",
      "    Negative Log Likelihood: 12.9057\tSigma2 Prior: -439.1912\tRegularization: 0.0008\n",
      "Iter: 6300  \tTraining Loss: -427.9292    \n",
      "    Negative Log Likelihood: 12.8548\tSigma2 Prior: -440.7848\tRegularization: 0.0008\n",
      "Iter: 6310  \tTraining Loss: -427.0836    \n",
      "    Negative Log Likelihood: 13.2392\tSigma2 Prior: -440.3236\tRegularization: 0.0008\n",
      "Iter: 6320  \tTraining Loss: -426.9930    \n",
      "    Negative Log Likelihood: 13.2439\tSigma2 Prior: -440.2377\tRegularization: 0.0008\n",
      "Iter: 6330  \tTraining Loss: -425.0422    \n",
      "    Negative Log Likelihood: 13.8045\tSigma2 Prior: -438.8475\tRegularization: 0.0008\n",
      "Iter: 6340  \tTraining Loss: -429.9860    \n",
      "    Negative Log Likelihood: 13.1081\tSigma2 Prior: -443.0948\tRegularization: 0.0008\n",
      "Iter: 6350  \tTraining Loss: -428.5463    \n",
      "    Negative Log Likelihood: 13.2333\tSigma2 Prior: -441.7803\tRegularization: 0.0008\n",
      "Iter: 6360  \tTraining Loss: -431.6189    \n",
      "    Negative Log Likelihood: 12.9724\tSigma2 Prior: -444.5921\tRegularization: 0.0008\n",
      "Iter: 6370  \tTraining Loss: -427.3903    \n",
      "    Negative Log Likelihood: 13.5959\tSigma2 Prior: -440.9870\tRegularization: 0.0008\n",
      "Iter: 6380  \tTraining Loss: -430.7429    \n",
      "    Negative Log Likelihood: 13.5474\tSigma2 Prior: -444.2911\tRegularization: 0.0008\n",
      "Iter: 6390  \tTraining Loss: -429.7868    \n",
      "    Negative Log Likelihood: 13.9420\tSigma2 Prior: -443.7296\tRegularization: 0.0008\n",
      "Iter: 6400  \tTraining Loss: -429.8111    \n",
      "    Negative Log Likelihood: 14.0872\tSigma2 Prior: -443.8990\tRegularization: 0.0008\n",
      "Iter: 6410  \tTraining Loss: -432.9564    \n",
      "    Negative Log Likelihood: 13.4627\tSigma2 Prior: -446.4199\tRegularization: 0.0008\n",
      "Iter: 6420  \tTraining Loss: -431.1634    \n",
      "    Negative Log Likelihood: 13.9693\tSigma2 Prior: -445.1335\tRegularization: 0.0008\n",
      "Iter: 6430  \tTraining Loss: -432.8103    \n",
      "    Negative Log Likelihood: 13.9044\tSigma2 Prior: -446.7155\tRegularization: 0.0008\n",
      "Iter: 6440  \tTraining Loss: -432.6178    \n",
      "    Negative Log Likelihood: 14.0472\tSigma2 Prior: -446.6658\tRegularization: 0.0008\n",
      "Iter: 6450  \tTraining Loss: -430.1352    \n",
      "    Negative Log Likelihood: 14.3647\tSigma2 Prior: -444.5007\tRegularization: 0.0008\n",
      "Iter: 6460  \tTraining Loss: -432.2808    \n",
      "    Negative Log Likelihood: 14.2574\tSigma2 Prior: -446.5389\tRegularization: 0.0008\n",
      "Iter: 6470  \tTraining Loss: -433.5249    \n",
      "    Negative Log Likelihood: 14.2955\tSigma2 Prior: -447.8212\tRegularization: 0.0008\n",
      "Iter: 6480  \tTraining Loss: -431.3723    \n",
      "    Negative Log Likelihood: 14.8213\tSigma2 Prior: -446.1944\tRegularization: 0.0008\n",
      "Iter: 6490  \tTraining Loss: -428.6340    \n",
      "    Negative Log Likelihood: 14.9085\tSigma2 Prior: -443.5433\tRegularization: 0.0008\n",
      "Iter: 6500  \tTraining Loss: -433.4044    \n",
      "    Negative Log Likelihood: 14.3407\tSigma2 Prior: -447.7459\tRegularization: 0.0008\n",
      "Iter: 6510  \tTraining Loss: -434.9684    \n",
      "    Negative Log Likelihood: 14.4607\tSigma2 Prior: -449.4299\tRegularization: 0.0008\n",
      "Iter: 6520  \tTraining Loss: -435.6200    \n",
      "    Negative Log Likelihood: 14.4681\tSigma2 Prior: -450.0889\tRegularization: 0.0008\n",
      "Iter: 6530  \tTraining Loss: -438.3196    \n",
      "    Negative Log Likelihood: 14.3641\tSigma2 Prior: -452.6845\tRegularization: 0.0008\n",
      "Iter: 6540  \tTraining Loss: -437.9972    \n",
      "    Negative Log Likelihood: 14.4796\tSigma2 Prior: -452.4776\tRegularization: 0.0008\n",
      "Iter: 6550  \tTraining Loss: -435.6899    \n",
      "    Negative Log Likelihood: 14.7798\tSigma2 Prior: -450.4705\tRegularization: 0.0008\n",
      "Iter: 6560  \tTraining Loss: -436.3114    \n",
      "    Negative Log Likelihood: 14.6660\tSigma2 Prior: -450.9782\tRegularization: 0.0008\n",
      "Iter: 6570  \tTraining Loss: -437.4201    \n",
      "    Negative Log Likelihood: 14.8234\tSigma2 Prior: -452.2443\tRegularization: 0.0008\n",
      "Iter: 6580  \tTraining Loss: -437.7612    \n",
      "    Negative Log Likelihood: 14.9977\tSigma2 Prior: -452.7597\tRegularization: 0.0008\n",
      "Iter: 6590  \tTraining Loss: -441.1819    \n",
      "    Negative Log Likelihood: 14.6648\tSigma2 Prior: -455.8475\tRegularization: 0.0008\n",
      "Iter: 6600  \tTraining Loss: -441.5641    \n",
      "    Negative Log Likelihood: 14.6783\tSigma2 Prior: -456.2432\tRegularization: 0.0008\n",
      "Iter: 6610  \tTraining Loss: -439.3517    \n",
      "    Negative Log Likelihood: 15.3290\tSigma2 Prior: -454.6815\tRegularization: 0.0008\n",
      "Iter: 6620  \tTraining Loss: -439.0387    \n",
      "    Negative Log Likelihood: 15.3378\tSigma2 Prior: -454.3772\tRegularization: 0.0008\n",
      "Iter: 6630  \tTraining Loss: -443.6403    \n",
      "    Negative Log Likelihood: 14.8832\tSigma2 Prior: -458.5244\tRegularization: 0.0008\n",
      "Iter: 6640  \tTraining Loss: -442.1407    \n",
      "    Negative Log Likelihood: 15.4751\tSigma2 Prior: -457.6165\tRegularization: 0.0008\n",
      "Iter: 6650  \tTraining Loss: -443.1274    \n",
      "    Negative Log Likelihood: 15.5546\tSigma2 Prior: -458.6828\tRegularization: 0.0008\n",
      "Iter: 6660  \tTraining Loss: -439.3755    \n",
      "    Negative Log Likelihood: 15.5714\tSigma2 Prior: -454.9478\tRegularization: 0.0008\n",
      "Iter: 6670  \tTraining Loss: -441.4298    \n",
      "    Negative Log Likelihood: 15.6732\tSigma2 Prior: -457.1038\tRegularization: 0.0008\n",
      "Iter: 6680  \tTraining Loss: -444.3932    \n",
      "    Negative Log Likelihood: 15.6496\tSigma2 Prior: -460.0436\tRegularization: 0.0008\n",
      "Iter: 6690  \tTraining Loss: -444.0411    \n",
      "    Negative Log Likelihood: 15.4563\tSigma2 Prior: -459.4981\tRegularization: 0.0008\n",
      "Iter: 6700  \tTraining Loss: -442.0484    \n",
      "    Negative Log Likelihood: 16.1543\tSigma2 Prior: -458.2035\tRegularization: 0.0008\n",
      "Iter: 6710  \tTraining Loss: -444.8154    \n",
      "    Negative Log Likelihood: 15.6976\tSigma2 Prior: -460.5138\tRegularization: 0.0008\n",
      "Iter: 6720  \tTraining Loss: -444.3134    \n",
      "    Negative Log Likelihood: 15.8870\tSigma2 Prior: -460.2012\tRegularization: 0.0008\n",
      "Iter: 6730  \tTraining Loss: -442.4434    \n",
      "    Negative Log Likelihood: 16.1667\tSigma2 Prior: -458.6108\tRegularization: 0.0008\n",
      "Iter: 6740  \tTraining Loss: -446.4747    \n",
      "    Negative Log Likelihood: 16.0087\tSigma2 Prior: -462.4841\tRegularization: 0.0008\n",
      "Iter: 6750  \tTraining Loss: -447.9513    \n",
      "    Negative Log Likelihood: 16.2862\tSigma2 Prior: -464.2383\tRegularization: 0.0008\n",
      "Iter: 6760  \tTraining Loss: -453.0758    \n",
      "    Negative Log Likelihood: 15.3352\tSigma2 Prior: -468.4118\tRegularization: 0.0008\n",
      "Iter: 6770  \tTraining Loss: -448.1285    \n",
      "    Negative Log Likelihood: 15.9070\tSigma2 Prior: -464.0363\tRegularization: 0.0008\n",
      "Iter: 6780  \tTraining Loss: -449.2394    \n",
      "    Negative Log Likelihood: 16.0867\tSigma2 Prior: -465.3269\tRegularization: 0.0008\n",
      "Iter: 6790  \tTraining Loss: -451.9727    \n",
      "    Negative Log Likelihood: 16.3938\tSigma2 Prior: -468.3673\tRegularization: 0.0008\n",
      "Iter: 6800  \tTraining Loss: -449.3569    \n",
      "    Negative Log Likelihood: 16.5849\tSigma2 Prior: -465.9426\tRegularization: 0.0008\n",
      "Iter: 6810  \tTraining Loss: -446.5484    \n",
      "    Negative Log Likelihood: 17.1738\tSigma2 Prior: -463.7230\tRegularization: 0.0008\n",
      "Iter: 6820  \tTraining Loss: -448.1702    \n",
      "    Negative Log Likelihood: 16.7785\tSigma2 Prior: -464.9495\tRegularization: 0.0008\n",
      "Iter: 6830  \tTraining Loss: -453.2462    \n",
      "    Negative Log Likelihood: 16.3923\tSigma2 Prior: -469.6393\tRegularization: 0.0008\n",
      "Iter: 6840  \tTraining Loss: -448.2624    \n",
      "    Negative Log Likelihood: 17.0835\tSigma2 Prior: -465.3467\tRegularization: 0.0008\n",
      "Iter: 6850  \tTraining Loss: -453.5781    \n",
      "    Negative Log Likelihood: 16.7159\tSigma2 Prior: -470.2948\tRegularization: 0.0008\n",
      "Iter: 6860  \tTraining Loss: -449.6980    \n",
      "    Negative Log Likelihood: 17.4772\tSigma2 Prior: -467.1760\tRegularization: 0.0008\n",
      "Iter: 6870  \tTraining Loss: -451.1464    \n",
      "    Negative Log Likelihood: 17.3093\tSigma2 Prior: -468.4565\tRegularization: 0.0008\n",
      "Iter: 6880  \tTraining Loss: -453.2934    \n",
      "    Negative Log Likelihood: 17.2507\tSigma2 Prior: -470.5450\tRegularization: 0.0008\n",
      "Iter: 6890  \tTraining Loss: -453.6982    \n",
      "    Negative Log Likelihood: 17.3899\tSigma2 Prior: -471.0888\tRegularization: 0.0008\n",
      "Iter: 6900  \tTraining Loss: -454.2939    \n",
      "    Negative Log Likelihood: 17.3778\tSigma2 Prior: -471.6725\tRegularization: 0.0008\n",
      "Iter: 6910  \tTraining Loss: -455.3092    \n",
      "    Negative Log Likelihood: 17.3364\tSigma2 Prior: -472.6464\tRegularization: 0.0008\n",
      "Iter: 6920  \tTraining Loss: -455.5868    \n",
      "    Negative Log Likelihood: 17.8633\tSigma2 Prior: -473.4508\tRegularization: 0.0008\n",
      "Iter: 6930  \tTraining Loss: -457.0431    \n",
      "    Negative Log Likelihood: 17.7380\tSigma2 Prior: -474.7819\tRegularization: 0.0008\n",
      "Iter: 6940  \tTraining Loss: -456.6541    \n",
      "    Negative Log Likelihood: 18.2191\tSigma2 Prior: -474.8740\tRegularization: 0.0008\n",
      "Iter: 6950  \tTraining Loss: -455.0119    \n",
      "    Negative Log Likelihood: 18.2656\tSigma2 Prior: -473.2783\tRegularization: 0.0008\n",
      "Iter: 6960  \tTraining Loss: -459.2775    \n",
      "    Negative Log Likelihood: 17.6974\tSigma2 Prior: -476.9757\tRegularization: 0.0008\n",
      "Iter: 6970  \tTraining Loss: -454.4792    \n",
      "    Negative Log Likelihood: 18.4107\tSigma2 Prior: -472.8907\tRegularization: 0.0008\n",
      "Iter: 6980  \tTraining Loss: -454.6292    \n",
      "    Negative Log Likelihood: 18.5782\tSigma2 Prior: -473.2081\tRegularization: 0.0008\n",
      "Iter: 6990  \tTraining Loss: -459.3284    \n",
      "    Negative Log Likelihood: 18.0819\tSigma2 Prior: -477.4111\tRegularization: 0.0008\n",
      "Iter: 7000  \tTraining Loss: -461.3277    \n",
      "    Negative Log Likelihood: 18.3860\tSigma2 Prior: -479.7145\tRegularization: 0.0008\n",
      "Iter: 7010  \tTraining Loss: -464.2687    \n",
      "    Negative Log Likelihood: 18.2814\tSigma2 Prior: -482.5509\tRegularization: 0.0008\n",
      "Iter: 7020  \tTraining Loss: -463.8303    \n",
      "    Negative Log Likelihood: 18.3776\tSigma2 Prior: -482.2087\tRegularization: 0.0008\n",
      "Iter: 7030  \tTraining Loss: -463.7753    \n",
      "    Negative Log Likelihood: 18.2635\tSigma2 Prior: -482.0396\tRegularization: 0.0008\n",
      "Iter: 7040  \tTraining Loss: -460.9924    \n",
      "    Negative Log Likelihood: 18.8759\tSigma2 Prior: -479.8691\tRegularization: 0.0008\n",
      "Iter: 7050  \tTraining Loss: -461.6119    \n",
      "    Negative Log Likelihood: 19.3833\tSigma2 Prior: -480.9960\tRegularization: 0.0008\n",
      "Iter: 7060  \tTraining Loss: -459.4190    \n",
      "    Negative Log Likelihood: 19.7177\tSigma2 Prior: -479.1374\tRegularization: 0.0008\n",
      "Iter: 7070  \tTraining Loss: -467.3648    \n",
      "    Negative Log Likelihood: 18.8533\tSigma2 Prior: -486.2189\tRegularization: 0.0008\n",
      "Iter: 7080  \tTraining Loss: -467.7000    \n",
      "    Negative Log Likelihood: 18.9420\tSigma2 Prior: -486.6428\tRegularization: 0.0008\n",
      "Iter: 7090  \tTraining Loss: -466.3322    \n",
      "    Negative Log Likelihood: 19.1638\tSigma2 Prior: -485.4968\tRegularization: 0.0008\n",
      "Iter: 7100  \tTraining Loss: -463.0685    \n",
      "    Negative Log Likelihood: 19.8606\tSigma2 Prior: -482.9300\tRegularization: 0.0008\n",
      "Iter: 7110  \tTraining Loss: -462.7443    \n",
      "    Negative Log Likelihood: 20.3162\tSigma2 Prior: -483.0612\tRegularization: 0.0008\n",
      "Iter: 7120  \tTraining Loss: -463.2566    \n",
      "    Negative Log Likelihood: 20.3213\tSigma2 Prior: -483.5786\tRegularization: 0.0008\n",
      "Iter: 7130  \tTraining Loss: -464.5759    \n",
      "    Negative Log Likelihood: 20.3006\tSigma2 Prior: -484.8772\tRegularization: 0.0008\n",
      "Iter: 7140  \tTraining Loss: -471.4760    \n",
      "    Negative Log Likelihood: 19.7527\tSigma2 Prior: -491.2295\tRegularization: 0.0008\n",
      "Iter: 7150  \tTraining Loss: -472.4158    \n",
      "    Negative Log Likelihood: 19.5843\tSigma2 Prior: -492.0009\tRegularization: 0.0008\n",
      "Iter: 7160  \tTraining Loss: -471.6287    \n",
      "    Negative Log Likelihood: 20.1715\tSigma2 Prior: -491.8009\tRegularization: 0.0008\n",
      "Iter: 7170  \tTraining Loss: -467.8628    \n",
      "    Negative Log Likelihood: 20.8359\tSigma2 Prior: -488.6994\tRegularization: 0.0008\n",
      "Iter: 7180  \tTraining Loss: -474.2895    \n",
      "    Negative Log Likelihood: 20.2421\tSigma2 Prior: -494.5324\tRegularization: 0.0008\n",
      "Iter: 7190  \tTraining Loss: -470.0944    \n",
      "    Negative Log Likelihood: 21.2873\tSigma2 Prior: -491.3825\tRegularization: 0.0008\n",
      "Iter: 7200  \tTraining Loss: -472.5684    \n",
      "    Negative Log Likelihood: 20.7765\tSigma2 Prior: -493.3457\tRegularization: 0.0008\n",
      "Iter: 7210  \tTraining Loss: -473.8865    \n",
      "    Negative Log Likelihood: 20.3818\tSigma2 Prior: -494.2690\tRegularization: 0.0008\n",
      "Iter: 7220  \tTraining Loss: -470.1517    \n",
      "    Negative Log Likelihood: 21.0169\tSigma2 Prior: -491.1693\tRegularization: 0.0008\n",
      "Iter: 7230  \tTraining Loss: -473.9780    \n",
      "    Negative Log Likelihood: 21.1684\tSigma2 Prior: -495.1472\tRegularization: 0.0008\n",
      "Iter: 7240  \tTraining Loss: -476.4149    \n",
      "    Negative Log Likelihood: 21.3912\tSigma2 Prior: -497.8069\tRegularization: 0.0008\n",
      "Iter: 7250  \tTraining Loss: -474.2585    \n",
      "    Negative Log Likelihood: 21.9255\tSigma2 Prior: -496.1848\tRegularization: 0.0008\n",
      "Iter: 7260  \tTraining Loss: -466.3067    \n",
      "    Negative Log Likelihood: 22.7023\tSigma2 Prior: -489.0098\tRegularization: 0.0008\n",
      "Iter: 7270  \tTraining Loss: -476.0970    \n",
      "    Negative Log Likelihood: 21.8057\tSigma2 Prior: -497.9034\tRegularization: 0.0008\n",
      "Iter: 7280  \tTraining Loss: -481.1890    \n",
      "    Negative Log Likelihood: 21.4463\tSigma2 Prior: -502.6360\tRegularization: 0.0008\n",
      "Iter: 7290  \tTraining Loss: -480.5374    \n",
      "    Negative Log Likelihood: 21.4420\tSigma2 Prior: -501.9802\tRegularization: 0.0008\n",
      "Iter: 7300  \tTraining Loss: -465.9852    \n",
      "    Negative Log Likelihood: 23.6493\tSigma2 Prior: -489.6353\tRegularization: 0.0008\n",
      "Iter: 7310  \tTraining Loss: -476.6026    \n",
      "    Negative Log Likelihood: 23.1544\tSigma2 Prior: -499.7578\tRegularization: 0.0008\n",
      "Iter: 7320  \tTraining Loss: -477.5138    \n",
      "    Negative Log Likelihood: 23.2552\tSigma2 Prior: -500.7698\tRegularization: 0.0008\n",
      "Iter: 7330  \tTraining Loss: -477.3437    \n",
      "    Negative Log Likelihood: 23.0673\tSigma2 Prior: -500.4118\tRegularization: 0.0008\n",
      "Iter: 7340  \tTraining Loss: -482.4287    \n",
      "    Negative Log Likelihood: 22.9006\tSigma2 Prior: -505.3300\tRegularization: 0.0008\n",
      "Iter: 7350  \tTraining Loss: -480.0870    \n",
      "    Negative Log Likelihood: 23.2222\tSigma2 Prior: -503.3101\tRegularization: 0.0008\n",
      "Iter: 7360  \tTraining Loss: -474.9234    \n",
      "    Negative Log Likelihood: 24.5191\tSigma2 Prior: -499.4433\tRegularization: 0.0008\n",
      "Iter: 7370  \tTraining Loss: -488.1527    \n",
      "    Negative Log Likelihood: 22.6187\tSigma2 Prior: -510.7722\tRegularization: 0.0008\n",
      "Iter: 7380  \tTraining Loss: -484.4777    \n",
      "    Negative Log Likelihood: 23.9858\tSigma2 Prior: -508.4643\tRegularization: 0.0008\n",
      "Iter: 7390  \tTraining Loss: -489.3434    \n",
      "    Negative Log Likelihood: 23.1397\tSigma2 Prior: -512.4838\tRegularization: 0.0008\n",
      "Iter: 7400  \tTraining Loss: -480.1769    \n",
      "    Negative Log Likelihood: 24.7748\tSigma2 Prior: -504.9525\tRegularization: 0.0008\n",
      "Iter: 7410  \tTraining Loss: -481.6776    \n",
      "    Negative Log Likelihood: 24.3151\tSigma2 Prior: -505.9936\tRegularization: 0.0008\n",
      "Iter: 7420  \tTraining Loss: -482.7210    \n",
      "    Negative Log Likelihood: 24.9051\tSigma2 Prior: -507.6270\tRegularization: 0.0008\n",
      "Iter: 7430  \tTraining Loss: -485.8160    \n",
      "    Negative Log Likelihood: 24.8060\tSigma2 Prior: -510.6227\tRegularization: 0.0008\n",
      "Iter: 7440  \tTraining Loss: -487.4688    \n",
      "    Negative Log Likelihood: 24.9929\tSigma2 Prior: -512.4626\tRegularization: 0.0008\n",
      "Iter: 7450  \tTraining Loss: -486.3567    \n",
      "    Negative Log Likelihood: 24.7111\tSigma2 Prior: -511.0685\tRegularization: 0.0008\n",
      "Iter: 7460  \tTraining Loss: -483.8446    \n",
      "    Negative Log Likelihood: 25.8855\tSigma2 Prior: -509.7309\tRegularization: 0.0008\n",
      "Iter: 7470  \tTraining Loss: -491.9004    \n",
      "    Negative Log Likelihood: 25.3664\tSigma2 Prior: -517.2676\tRegularization: 0.0008\n",
      "Iter: 7480  \tTraining Loss: -489.8656    \n",
      "    Negative Log Likelihood: 26.1169\tSigma2 Prior: -515.9833\tRegularization: 0.0008\n",
      "Iter: 7490  \tTraining Loss: -483.9835    \n",
      "    Negative Log Likelihood: 26.7945\tSigma2 Prior: -510.7788\tRegularization: 0.0008\n",
      "Iter: 7500  \tTraining Loss: -491.2328    \n",
      "    Negative Log Likelihood: 26.3398\tSigma2 Prior: -517.5735\tRegularization: 0.0008\n",
      "Iter: 7510  \tTraining Loss: -491.2468    \n",
      "    Negative Log Likelihood: 26.4086\tSigma2 Prior: -517.6562\tRegularization: 0.0008\n",
      "Iter: 7520  \tTraining Loss: -493.6234    \n",
      "    Negative Log Likelihood: 26.4893\tSigma2 Prior: -520.1135\tRegularization: 0.0008\n",
      "Iter: 7530  \tTraining Loss: -492.9100    \n",
      "    Negative Log Likelihood: 26.1865\tSigma2 Prior: -519.0973\tRegularization: 0.0008\n",
      "Iter: 7540  \tTraining Loss: -497.4563    \n",
      "    Negative Log Likelihood: 26.7418\tSigma2 Prior: -524.1989\tRegularization: 0.0008\n",
      "Iter: 7550  \tTraining Loss: -493.3217    \n",
      "    Negative Log Likelihood: 28.1824\tSigma2 Prior: -521.5049\tRegularization: 0.0008\n",
      "Iter: 7560  \tTraining Loss: -499.6969    \n",
      "    Negative Log Likelihood: 27.6713\tSigma2 Prior: -527.3690\tRegularization: 0.0008\n",
      "Iter: 7570  \tTraining Loss: -499.9298    \n",
      "    Negative Log Likelihood: 27.4699\tSigma2 Prior: -527.4005\tRegularization: 0.0008\n",
      "Iter: 7580  \tTraining Loss: -497.4226    \n",
      "    Negative Log Likelihood: 28.0094\tSigma2 Prior: -525.4328\tRegularization: 0.0008\n",
      "Iter: 7590  \tTraining Loss: -495.1451    \n",
      "    Negative Log Likelihood: 28.7750\tSigma2 Prior: -523.9210\tRegularization: 0.0008\n",
      "Iter: 7600  \tTraining Loss: -499.2998    \n",
      "    Negative Log Likelihood: 28.1584\tSigma2 Prior: -527.4590\tRegularization: 0.0008\n",
      "Iter: 7610  \tTraining Loss: -496.6311    \n",
      "    Negative Log Likelihood: 28.8244\tSigma2 Prior: -525.4563\tRegularization: 0.0008\n",
      "Iter: 7620  \tTraining Loss: -498.4848    \n",
      "    Negative Log Likelihood: 29.5860\tSigma2 Prior: -528.0717\tRegularization: 0.0008\n",
      "Iter: 7630  \tTraining Loss: -496.4753    \n",
      "    Negative Log Likelihood: 29.9571\tSigma2 Prior: -526.4332\tRegularization: 0.0008\n",
      "Iter: 7640  \tTraining Loss: -502.4148    \n",
      "    Negative Log Likelihood: 29.5297\tSigma2 Prior: -531.9453\tRegularization: 0.0008\n",
      "Iter: 7650  \tTraining Loss: -506.6827    \n",
      "    Negative Log Likelihood: 29.4777\tSigma2 Prior: -536.1612\tRegularization: 0.0008\n",
      "Iter: 7660  \tTraining Loss: -493.3700    \n",
      "    Negative Log Likelihood: 31.4285\tSigma2 Prior: -524.7993\tRegularization: 0.0008\n",
      "Iter: 7670  \tTraining Loss: -503.6860    \n",
      "    Negative Log Likelihood: 30.4622\tSigma2 Prior: -534.1490\tRegularization: 0.0008\n",
      "Iter: 7680  \tTraining Loss: -500.9244    \n",
      "    Negative Log Likelihood: 31.5429\tSigma2 Prior: -532.4681\tRegularization: 0.0008\n",
      "Iter: 7690  \tTraining Loss: -505.1615    \n",
      "    Negative Log Likelihood: 31.2495\tSigma2 Prior: -536.4118\tRegularization: 0.0008\n",
      "Iter: 7700  \tTraining Loss: -509.9626    \n",
      "    Negative Log Likelihood: 30.9642\tSigma2 Prior: -540.9276\tRegularization: 0.0008\n",
      "Iter: 7710  \tTraining Loss: -510.3414    \n",
      "    Negative Log Likelihood: 30.5455\tSigma2 Prior: -540.8878\tRegularization: 0.0008\n",
      "Iter: 7720  \tTraining Loss: -507.0603    \n",
      "    Negative Log Likelihood: 31.5916\tSigma2 Prior: -538.6528\tRegularization: 0.0008\n",
      "Iter: 7730  \tTraining Loss: -507.6477    \n",
      "    Negative Log Likelihood: 31.7840\tSigma2 Prior: -539.4325\tRegularization: 0.0008\n",
      "Iter: 7740  \tTraining Loss: -508.1450    \n",
      "    Negative Log Likelihood: 32.7602\tSigma2 Prior: -540.9059\tRegularization: 0.0008\n",
      "Iter: 7750  \tTraining Loss: -510.8228    \n",
      "    Negative Log Likelihood: 33.2623\tSigma2 Prior: -544.0859\tRegularization: 0.0008\n",
      "Iter: 7760  \tTraining Loss: -508.0487    \n",
      "    Negative Log Likelihood: 33.1112\tSigma2 Prior: -541.1608\tRegularization: 0.0008\n",
      "Iter: 7770  \tTraining Loss: -505.7500    \n",
      "    Negative Log Likelihood: 33.2316\tSigma2 Prior: -538.9824\tRegularization: 0.0008\n",
      "Iter: 7780  \tTraining Loss: -507.5557    \n",
      "    Negative Log Likelihood: 34.9573\tSigma2 Prior: -542.5139\tRegularization: 0.0008\n",
      "Iter: 7790  \tTraining Loss: -509.4287    \n",
      "    Negative Log Likelihood: 34.4048\tSigma2 Prior: -543.8344\tRegularization: 0.0008\n",
      "Iter: 7800  \tTraining Loss: -511.2632    \n",
      "    Negative Log Likelihood: 35.3030\tSigma2 Prior: -546.5670\tRegularization: 0.0008\n",
      "Iter: 7810  \tTraining Loss: -509.3025    \n",
      "    Negative Log Likelihood: 35.7116\tSigma2 Prior: -545.0149\tRegularization: 0.0008\n",
      "Iter: 7820  \tTraining Loss: -507.8972    \n",
      "    Negative Log Likelihood: 37.0925\tSigma2 Prior: -544.9905\tRegularization: 0.0008\n",
      "Iter: 7830  \tTraining Loss: -516.9987    \n",
      "    Negative Log Likelihood: 35.3794\tSigma2 Prior: -552.3788\tRegularization: 0.0008\n",
      "Iter: 7840  \tTraining Loss: -514.5318    \n",
      "    Negative Log Likelihood: 36.6455\tSigma2 Prior: -551.1782\tRegularization: 0.0008\n",
      "Iter: 7850  \tTraining Loss: -506.6818    \n",
      "    Negative Log Likelihood: 38.5479\tSigma2 Prior: -545.2305\tRegularization: 0.0008\n",
      "Iter: 7860  \tTraining Loss: -518.7228    \n",
      "    Negative Log Likelihood: 36.8875\tSigma2 Prior: -555.6110\tRegularization: 0.0008\n",
      "Iter: 7870  \tTraining Loss: -523.4825    \n",
      "    Negative Log Likelihood: 36.0123\tSigma2 Prior: -559.4957\tRegularization: 0.0008\n",
      "Iter: 7880  \tTraining Loss: -503.2878    \n",
      "    Negative Log Likelihood: 39.7074\tSigma2 Prior: -542.9960\tRegularization: 0.0008\n",
      "Iter: 7890  \tTraining Loss: -513.3273    \n",
      "    Negative Log Likelihood: 38.4094\tSigma2 Prior: -551.7375\tRegularization: 0.0008\n",
      "Iter: 7900  \tTraining Loss: -513.9199    \n",
      "    Negative Log Likelihood: 39.0042\tSigma2 Prior: -552.9249\tRegularization: 0.0008\n",
      "Iter: 7910  \tTraining Loss: -520.4996    \n",
      "    Negative Log Likelihood: 39.3428\tSigma2 Prior: -559.8433\tRegularization: 0.0008\n",
      "Iter: 7920  \tTraining Loss: -518.3019    \n",
      "    Negative Log Likelihood: 39.2510\tSigma2 Prior: -557.5538\tRegularization: 0.0008\n",
      "Iter: 7930  \tTraining Loss: -523.1007    \n",
      "    Negative Log Likelihood: 39.0500\tSigma2 Prior: -562.1516\tRegularization: 0.0008\n",
      "Iter: 7940  \tTraining Loss: -507.0349    \n",
      "    Negative Log Likelihood: 42.0431\tSigma2 Prior: -549.0789\tRegularization: 0.0008\n",
      "Iter: 7950  \tTraining Loss: -502.7255    \n",
      "    Negative Log Likelihood: 42.5279\tSigma2 Prior: -545.2543\tRegularization: 0.0008\n",
      "Iter: 7960  \tTraining Loss: -521.3173    \n",
      "    Negative Log Likelihood: 41.9645\tSigma2 Prior: -563.2827\tRegularization: 0.0008\n",
      "Iter: 7970  \tTraining Loss: -520.8646    \n",
      "    Negative Log Likelihood: 42.2197\tSigma2 Prior: -563.0852\tRegularization: 0.0008\n",
      "Iter: 7980  \tTraining Loss: -514.4730    \n",
      "    Negative Log Likelihood: 43.5827\tSigma2 Prior: -558.0565\tRegularization: 0.0008\n",
      "Iter: 7990  \tTraining Loss: -535.2881    \n",
      "    Negative Log Likelihood: 41.0748\tSigma2 Prior: -576.3638\tRegularization: 0.0008\n",
      "Iter: 8000  \tTraining Loss: -522.1721    \n",
      "    Negative Log Likelihood: 43.2374\tSigma2 Prior: -565.4103\tRegularization: 0.0008\n",
      "Iter: 8010  \tTraining Loss: -518.8193    \n",
      "    Negative Log Likelihood: 46.1711\tSigma2 Prior: -564.9913\tRegularization: 0.0008\n",
      "Iter: 8020  \tTraining Loss: -512.8181    \n",
      "    Negative Log Likelihood: 45.8979\tSigma2 Prior: -558.7168\tRegularization: 0.0008\n",
      "Iter: 8030  \tTraining Loss: -526.5997    \n",
      "    Negative Log Likelihood: 45.3675\tSigma2 Prior: -571.9681\tRegularization: 0.0008\n",
      "Iter: 8040  \tTraining Loss: -509.8159    \n",
      "    Negative Log Likelihood: 47.8378\tSigma2 Prior: -557.6546\tRegularization: 0.0008\n",
      "Iter: 8050  \tTraining Loss: -529.9626    \n",
      "    Negative Log Likelihood: 45.4497\tSigma2 Prior: -575.4132\tRegularization: 0.0008\n",
      "Iter: 8060  \tTraining Loss: -525.2143    \n",
      "    Negative Log Likelihood: 46.5852\tSigma2 Prior: -571.8004\tRegularization: 0.0008\n",
      "Iter: 8070  \tTraining Loss: -525.8268    \n",
      "    Negative Log Likelihood: 47.9523\tSigma2 Prior: -573.7799\tRegularization: 0.0008\n",
      "Iter: 8080  \tTraining Loss: -525.9070    \n",
      "    Negative Log Likelihood: 48.3845\tSigma2 Prior: -574.2923\tRegularization: 0.0008\n",
      "Iter: 8090  \tTraining Loss: -526.7793    \n",
      "    Negative Log Likelihood: 47.5062\tSigma2 Prior: -574.2863\tRegularization: 0.0008\n",
      "Iter: 8100  \tTraining Loss: -526.0745    \n",
      "    Negative Log Likelihood: 49.6768\tSigma2 Prior: -575.7522\tRegularization: 0.0008\n",
      "Iter: 8110  \tTraining Loss: -519.1619    \n",
      "    Negative Log Likelihood: 50.7675\tSigma2 Prior: -569.9302\tRegularization: 0.0008\n",
      "Iter: 8120  \tTraining Loss: -530.2352    \n",
      "    Negative Log Likelihood: 50.2920\tSigma2 Prior: -580.5281\tRegularization: 0.0008\n",
      "Iter: 8130  \tTraining Loss: -509.6199    \n",
      "    Negative Log Likelihood: 53.0924\tSigma2 Prior: -562.7131\tRegularization: 0.0008\n",
      "Iter: 8140  \tTraining Loss: -522.0236    \n",
      "    Negative Log Likelihood: 51.8818\tSigma2 Prior: -573.9062\tRegularization: 0.0008\n",
      "Iter: 8150  \tTraining Loss: -520.6866    \n",
      "    Negative Log Likelihood: 53.0195\tSigma2 Prior: -573.7070\tRegularization: 0.0008\n",
      "Iter: 8160  \tTraining Loss: -529.1658    \n",
      "    Negative Log Likelihood: 53.5913\tSigma2 Prior: -582.7580\tRegularization: 0.0008\n",
      "Iter: 8170  \tTraining Loss: -533.8320    \n",
      "    Negative Log Likelihood: 52.4995\tSigma2 Prior: -586.3324\tRegularization: 0.0008\n",
      "Iter: 8180  \tTraining Loss: -527.8946    \n",
      "    Negative Log Likelihood: 53.6967\tSigma2 Prior: -581.5921\tRegularization: 0.0008\n",
      "Iter: 8190  \tTraining Loss: -515.5690    \n",
      "    Negative Log Likelihood: 55.8985\tSigma2 Prior: -571.4683\tRegularization: 0.0008\n",
      "Iter: 8200  \tTraining Loss: -521.8054    \n",
      "    Negative Log Likelihood: 55.9752\tSigma2 Prior: -577.7815\tRegularization: 0.0008\n",
      "Iter: 8210  \tTraining Loss: -518.8818    \n",
      "    Negative Log Likelihood: 56.4941\tSigma2 Prior: -575.3768\tRegularization: 0.0008\n",
      "Iter: 8220  \tTraining Loss: -523.5565    \n",
      "    Negative Log Likelihood: 55.3484\tSigma2 Prior: -578.9058\tRegularization: 0.0008\n",
      "Iter: 8230  \tTraining Loss: -525.1852    \n",
      "    Negative Log Likelihood: 57.6538\tSigma2 Prior: -582.8398\tRegularization: 0.0008\n",
      "Iter: 8240  \tTraining Loss: -526.6528    \n",
      "    Negative Log Likelihood: 57.3749\tSigma2 Prior: -584.0286\tRegularization: 0.0008\n",
      "Iter: 8250  \tTraining Loss: -538.7926    \n",
      "    Negative Log Likelihood: 56.0777\tSigma2 Prior: -594.8712\tRegularization: 0.0008\n",
      "Iter: 8260  \tTraining Loss: -513.8559    \n",
      "    Negative Log Likelihood: 59.1455\tSigma2 Prior: -573.0023\tRegularization: 0.0008\n",
      "Iter: 8270  \tTraining Loss: -529.6083    \n",
      "    Negative Log Likelihood: 56.9200\tSigma2 Prior: -586.5292\tRegularization: 0.0008\n",
      "Iter: 8280  \tTraining Loss: -536.3551    \n",
      "    Negative Log Likelihood: 56.7033\tSigma2 Prior: -593.0592\tRegularization: 0.0008\n",
      "Iter: 8290  \tTraining Loss: -519.5558    \n",
      "    Negative Log Likelihood: 59.2411\tSigma2 Prior: -578.7979\tRegularization: 0.0008\n",
      "Iter: 8300  \tTraining Loss: -532.9496    \n",
      "    Negative Log Likelihood: 58.6789\tSigma2 Prior: -591.6293\tRegularization: 0.0008\n",
      "Iter: 8310  \tTraining Loss: -524.0582    \n",
      "    Negative Log Likelihood: 59.4854\tSigma2 Prior: -583.5444\tRegularization: 0.0008\n",
      "Iter: 8320  \tTraining Loss: -539.0248    \n",
      "    Negative Log Likelihood: 57.2951\tSigma2 Prior: -596.3208\tRegularization: 0.0008\n",
      "Iter: 8330  \tTraining Loss: -527.9788    \n",
      "    Negative Log Likelihood: 58.9575\tSigma2 Prior: -586.9372\tRegularization: 0.0008\n",
      "Iter: 8340  \tTraining Loss: -518.0388    \n",
      "    Negative Log Likelihood: 59.6839\tSigma2 Prior: -577.7236\tRegularization: 0.0008\n",
      "Iter: 8350  \tTraining Loss: -527.9258    \n",
      "    Negative Log Likelihood: 59.8357\tSigma2 Prior: -587.7624\tRegularization: 0.0008\n",
      "Iter: 8360  \tTraining Loss: -530.8737    \n",
      "    Negative Log Likelihood: 59.8084\tSigma2 Prior: -590.6829\tRegularization: 0.0008\n",
      "Iter: 8370  \tTraining Loss: -516.1691    \n",
      "    Negative Log Likelihood: 61.3646\tSigma2 Prior: -577.5345\tRegularization: 0.0008\n",
      "Iter: 8380  \tTraining Loss: -543.4888    \n",
      "    Negative Log Likelihood: 58.5051\tSigma2 Prior: -601.9948\tRegularization: 0.0008\n",
      "Iter: 8390  \tTraining Loss: -515.2777    \n",
      "    Negative Log Likelihood: 62.4885\tSigma2 Prior: -577.7670\tRegularization: 0.0008\n",
      "Iter: 8400  \tTraining Loss: -518.7816    \n",
      "    Negative Log Likelihood: 60.8902\tSigma2 Prior: -579.6727\tRegularization: 0.0008\n",
      "Iter: 8410  \tTraining Loss: -532.2280    \n",
      "    Negative Log Likelihood: 59.9354\tSigma2 Prior: -592.1643\tRegularization: 0.0008\n",
      "Iter: 8420  \tTraining Loss: -531.1653    \n",
      "    Negative Log Likelihood: 60.8224\tSigma2 Prior: -591.9886\tRegularization: 0.0008\n",
      "Iter: 8430  \tTraining Loss: -529.1939    \n",
      "    Negative Log Likelihood: 61.7003\tSigma2 Prior: -590.8950\tRegularization: 0.0008\n",
      "Iter: 8440  \tTraining Loss: -536.0627    \n",
      "    Negative Log Likelihood: 60.4574\tSigma2 Prior: -596.5209\tRegularization: 0.0008\n",
      "Iter: 8450  \tTraining Loss: -540.7092    \n",
      "    Negative Log Likelihood: 60.8977\tSigma2 Prior: -601.6077\tRegularization: 0.0008\n",
      "Iter: 8460  \tTraining Loss: -518.3503    \n",
      "    Negative Log Likelihood: 62.6722\tSigma2 Prior: -581.0234\tRegularization: 0.0008\n",
      "Iter: 8470  \tTraining Loss: -547.2742    \n",
      "    Negative Log Likelihood: 60.4021\tSigma2 Prior: -607.6771\tRegularization: 0.0008\n",
      "Iter: 8480  \tTraining Loss: -532.2439    \n",
      "    Negative Log Likelihood: 62.1405\tSigma2 Prior: -594.3853\tRegularization: 0.0008\n",
      "Iter: 8490  \tTraining Loss: -543.3287    \n",
      "    Negative Log Likelihood: 60.9331\tSigma2 Prior: -604.2627\tRegularization: 0.0008\n",
      "Iter: 8500  \tTraining Loss: -542.7121    \n",
      "    Negative Log Likelihood: 61.6894\tSigma2 Prior: -604.4023\tRegularization: 0.0008\n",
      "Iter: 8510  \tTraining Loss: -533.9792    \n",
      "    Negative Log Likelihood: 61.4807\tSigma2 Prior: -595.4608\tRegularization: 0.0008\n",
      "Iter: 8520  \tTraining Loss: -533.0532    \n",
      "    Negative Log Likelihood: 63.1201\tSigma2 Prior: -596.1741\tRegularization: 0.0008\n",
      "Iter: 8530  \tTraining Loss: -521.6757    \n",
      "    Negative Log Likelihood: 63.3352\tSigma2 Prior: -585.0117\tRegularization: 0.0008\n",
      "Iter: 8540  \tTraining Loss: -525.6013    \n",
      "    Negative Log Likelihood: 63.2182\tSigma2 Prior: -588.8204\tRegularization: 0.0008\n",
      "Iter: 8550  \tTraining Loss: -547.0889    \n",
      "    Negative Log Likelihood: 60.5449\tSigma2 Prior: -607.6346\tRegularization: 0.0008\n",
      "Iter: 8560  \tTraining Loss: -534.4561    \n",
      "    Negative Log Likelihood: 62.5964\tSigma2 Prior: -597.0534\tRegularization: 0.0008\n",
      "Iter: 8570  \tTraining Loss: -543.8294    \n",
      "    Negative Log Likelihood: 61.3564\tSigma2 Prior: -605.1867\tRegularization: 0.0008\n",
      "Iter: 8580  \tTraining Loss: -516.3391    \n",
      "    Negative Log Likelihood: 65.7114\tSigma2 Prior: -582.0513\tRegularization: 0.0008\n",
      "Iter: 8590  \tTraining Loss: -541.2551    \n",
      "    Negative Log Likelihood: 62.1915\tSigma2 Prior: -603.4474\tRegularization: 0.0008\n",
      "Iter: 8600  \tTraining Loss: -545.5098    \n",
      "    Negative Log Likelihood: 62.0319\tSigma2 Prior: -607.5425\tRegularization: 0.0008\n",
      "Iter: 8610  \tTraining Loss: -536.3596    \n",
      "    Negative Log Likelihood: 62.4845\tSigma2 Prior: -598.8450\tRegularization: 0.0008\n",
      "Iter: 8620  \tTraining Loss: -533.0104    \n",
      "    Negative Log Likelihood: 62.6186\tSigma2 Prior: -595.6298\tRegularization: 0.0008\n",
      "Iter: 8630  \tTraining Loss: -539.4971    \n",
      "    Negative Log Likelihood: 61.1120\tSigma2 Prior: -600.6100\tRegularization: 0.0008\n",
      "Iter: 8640  \tTraining Loss: -544.8472    \n",
      "    Negative Log Likelihood: 61.4514\tSigma2 Prior: -606.2994\tRegularization: 0.0008\n",
      "Iter: 8650  \tTraining Loss: -514.8593    \n",
      "    Negative Log Likelihood: 64.9262\tSigma2 Prior: -579.7864\tRegularization: 0.0008\n",
      "Iter: 8660  \tTraining Loss: -531.6279    \n",
      "    Negative Log Likelihood: 62.2450\tSigma2 Prior: -593.8738\tRegularization: 0.0008\n",
      "Iter: 8670  \tTraining Loss: -521.6337    \n",
      "    Negative Log Likelihood: 63.6901\tSigma2 Prior: -585.3246\tRegularization: 0.0008\n",
      "Iter: 8680  \tTraining Loss: -527.2727    \n",
      "    Negative Log Likelihood: 63.9781\tSigma2 Prior: -591.2516\tRegularization: 0.0008\n",
      "Iter: 8690  \tTraining Loss: -544.9839    \n",
      "    Negative Log Likelihood: 61.3090\tSigma2 Prior: -606.2938\tRegularization: 0.0008\n",
      "Iter: 8700  \tTraining Loss: -524.9966    \n",
      "    Negative Log Likelihood: 63.6863\tSigma2 Prior: -588.6838\tRegularization: 0.0008\n",
      "Iter: 8710  \tTraining Loss: -548.8032    \n",
      "    Negative Log Likelihood: 61.0597\tSigma2 Prior: -609.8638\tRegularization: 0.0008\n",
      "Iter: 8720  \tTraining Loss: -532.9551    \n",
      "    Negative Log Likelihood: 62.3422\tSigma2 Prior: -595.2982\tRegularization: 0.0008\n",
      "Iter: 8730  \tTraining Loss: -537.4990    \n",
      "    Negative Log Likelihood: 62.7885\tSigma2 Prior: -600.2883\tRegularization: 0.0008\n",
      "Iter: 8740  \tTraining Loss: -533.2784    \n",
      "    Negative Log Likelihood: 62.3338\tSigma2 Prior: -595.6131\tRegularization: 0.0008\n",
      "Iter: 8750  \tTraining Loss: -513.4326    \n",
      "    Negative Log Likelihood: 64.7591\tSigma2 Prior: -578.1925\tRegularization: 0.0008\n",
      "Iter: 8760  \tTraining Loss: -514.7095    \n",
      "    Negative Log Likelihood: 64.1320\tSigma2 Prior: -578.8423\tRegularization: 0.0008\n",
      "Iter: 8770  \tTraining Loss: -534.7921    \n",
      "    Negative Log Likelihood: 61.7673\tSigma2 Prior: -596.5602\tRegularization: 0.0008\n",
      "Iter: 8780  \tTraining Loss: -526.8453    \n",
      "    Negative Log Likelihood: 62.0561\tSigma2 Prior: -588.9022\tRegularization: 0.0009\n",
      "Iter: 8790  \tTraining Loss: -538.7130    \n",
      "    Negative Log Likelihood: 62.8349\tSigma2 Prior: -601.5488\tRegularization: 0.0009\n",
      "Iter: 8800  \tTraining Loss: -515.2181    \n",
      "    Negative Log Likelihood: 65.3452\tSigma2 Prior: -580.5641\tRegularization: 0.0009\n",
      "Iter: 8810  \tTraining Loss: -536.9644    \n",
      "    Negative Log Likelihood: 62.7862\tSigma2 Prior: -599.7515\tRegularization: 0.0009\n",
      "Iter: 8820  \tTraining Loss: -535.8556    \n",
      "    Negative Log Likelihood: 63.0347\tSigma2 Prior: -598.8911\tRegularization: 0.0009\n",
      "Iter: 8830  \tTraining Loss: -523.9181    \n",
      "    Negative Log Likelihood: 63.2460\tSigma2 Prior: -587.1650\tRegularization: 0.0009\n",
      "Iter: 8840  \tTraining Loss: -533.1015    \n",
      "    Negative Log Likelihood: 63.6265\tSigma2 Prior: -596.7289\tRegularization: 0.0009\n",
      "Iter: 8850  \tTraining Loss: -523.3009    \n",
      "    Negative Log Likelihood: 63.5859\tSigma2 Prior: -586.8876\tRegularization: 0.0009\n",
      "Iter: 8860  \tTraining Loss: -536.8414    \n",
      "    Negative Log Likelihood: 63.3374\tSigma2 Prior: -600.1796\tRegularization: 0.0009\n",
      "Iter: 8870  \tTraining Loss: -530.3327    \n",
      "    Negative Log Likelihood: 64.1711\tSigma2 Prior: -594.5046\tRegularization: 0.0009\n",
      "Iter: 8880  \tTraining Loss: -544.3479    \n",
      "    Negative Log Likelihood: 61.0432\tSigma2 Prior: -605.3920\tRegularization: 0.0009\n",
      "Iter: 8890  \tTraining Loss: -539.9716    \n",
      "    Negative Log Likelihood: 61.3062\tSigma2 Prior: -601.2786\tRegularization: 0.0009\n",
      "Iter: 8900  \tTraining Loss: -543.8375    \n",
      "    Negative Log Likelihood: 62.6435\tSigma2 Prior: -606.4819\tRegularization: 0.0009\n",
      "Iter: 8910  \tTraining Loss: -536.1035    \n",
      "    Negative Log Likelihood: 63.1208\tSigma2 Prior: -599.2252\tRegularization: 0.0009\n",
      "Iter: 8920  \tTraining Loss: -519.4147    \n",
      "    Negative Log Likelihood: 64.8981\tSigma2 Prior: -584.3137\tRegularization: 0.0009\n",
      "Iter: 8930  \tTraining Loss: -540.5588    \n",
      "    Negative Log Likelihood: 60.9016\tSigma2 Prior: -601.4613\tRegularization: 0.0009\n",
      "Iter: 8940  \tTraining Loss: -524.8460    \n",
      "    Negative Log Likelihood: 65.4886\tSigma2 Prior: -590.3355\tRegularization: 0.0009\n",
      "Iter: 8950  \tTraining Loss: -545.0308    \n",
      "    Negative Log Likelihood: 61.0308\tSigma2 Prior: -606.0625\tRegularization: 0.0009\n",
      "Iter: 8960  \tTraining Loss: -533.4261    \n",
      "    Negative Log Likelihood: 62.0557\tSigma2 Prior: -595.4827\tRegularization: 0.0009\n",
      "Iter: 8970  \tTraining Loss: -533.2598    \n",
      "    Negative Log Likelihood: 62.4401\tSigma2 Prior: -595.7007\tRegularization: 0.0009\n",
      "Iter: 8980  \tTraining Loss: -547.4197    \n",
      "    Negative Log Likelihood: 60.7240\tSigma2 Prior: -608.1446\tRegularization: 0.0009\n",
      "Iter: 8990  \tTraining Loss: -534.3686    \n",
      "    Negative Log Likelihood: 62.9993\tSigma2 Prior: -597.3688\tRegularization: 0.0009\n",
      "Iter: 9000  \tTraining Loss: -521.6529    \n",
      "    Negative Log Likelihood: 63.6370\tSigma2 Prior: -585.2907\tRegularization: 0.0009\n",
      "Iter: 9010  \tTraining Loss: -548.2490    \n",
      "    Negative Log Likelihood: 60.0926\tSigma2 Prior: -608.3425\tRegularization: 0.0009\n",
      "Iter: 9020  \tTraining Loss: -526.1407    \n",
      "    Negative Log Likelihood: 64.2884\tSigma2 Prior: -590.4299\tRegularization: 0.0009\n",
      "Iter: 9030  \tTraining Loss: -531.1256    \n",
      "    Negative Log Likelihood: 62.7104\tSigma2 Prior: -593.8369\tRegularization: 0.0009\n",
      "Iter: 9040  \tTraining Loss: -521.8896    \n",
      "    Negative Log Likelihood: 63.0696\tSigma2 Prior: -584.9601\tRegularization: 0.0009\n",
      "Iter: 9050  \tTraining Loss: -535.7182    \n",
      "    Negative Log Likelihood: 62.1007\tSigma2 Prior: -597.8197\tRegularization: 0.0009\n",
      "Iter: 9060  \tTraining Loss: -523.5575    \n",
      "    Negative Log Likelihood: 63.8128\tSigma2 Prior: -587.3712\tRegularization: 0.0009\n",
      "Iter: 9070  \tTraining Loss: -525.6238    \n",
      "    Negative Log Likelihood: 63.5189\tSigma2 Prior: -589.1436\tRegularization: 0.0009\n",
      "Iter: 9080  \tTraining Loss: -532.4913    \n",
      "    Negative Log Likelihood: 64.5844\tSigma2 Prior: -597.0765\tRegularization: 0.0009\n",
      "Iter: 9090  \tTraining Loss: -550.6224    \n",
      "    Negative Log Likelihood: 61.2399\tSigma2 Prior: -611.8632\tRegularization: 0.0009\n",
      "Iter: 9100  \tTraining Loss: -533.6000    \n",
      "    Negative Log Likelihood: 62.7955\tSigma2 Prior: -596.3964\tRegularization: 0.0009\n",
      "Iter: 9110  \tTraining Loss: -544.6425    \n",
      "    Negative Log Likelihood: 62.3398\tSigma2 Prior: -606.9832\tRegularization: 0.0009\n",
      "Iter: 9120  \tTraining Loss: -529.0747    \n",
      "    Negative Log Likelihood: 63.8444\tSigma2 Prior: -592.9200\tRegularization: 0.0009\n",
      "Iter: 9130  \tTraining Loss: -537.5125    \n",
      "    Negative Log Likelihood: 62.0788\tSigma2 Prior: -599.5920\tRegularization: 0.0009\n",
      "Iter: 9140  \tTraining Loss: -527.6281    \n",
      "    Negative Log Likelihood: 64.9276\tSigma2 Prior: -592.5565\tRegularization: 0.0009\n",
      "Iter: 9150  \tTraining Loss: -531.9170    \n",
      "    Negative Log Likelihood: 63.1324\tSigma2 Prior: -595.0503\tRegularization: 0.0009\n",
      "Iter: 9160  \tTraining Loss: -527.8699    \n",
      "    Negative Log Likelihood: 64.9621\tSigma2 Prior: -592.8329\tRegularization: 0.0009\n",
      "Iter: 9170  \tTraining Loss: -528.2713    \n",
      "    Negative Log Likelihood: 64.8517\tSigma2 Prior: -593.1238\tRegularization: 0.0009\n",
      "Iter: 9180  \tTraining Loss: -527.2952    \n",
      "    Negative Log Likelihood: 64.7138\tSigma2 Prior: -592.0099\tRegularization: 0.0009\n",
      "Iter: 9190  \tTraining Loss: -511.2100    \n",
      "    Negative Log Likelihood: 64.9405\tSigma2 Prior: -576.1513\tRegularization: 0.0009\n",
      "Iter: 9200  \tTraining Loss: -506.8627    \n",
      "    Negative Log Likelihood: 65.7624\tSigma2 Prior: -572.6260\tRegularization: 0.0009\n",
      "Iter: 9210  \tTraining Loss: -529.6143    \n",
      "    Negative Log Likelihood: 63.6915\tSigma2 Prior: -593.3067\tRegularization: 0.0009\n",
      "Iter: 9220  \tTraining Loss: -532.9031    \n",
      "    Negative Log Likelihood: 61.8240\tSigma2 Prior: -594.7280\tRegularization: 0.0009\n",
      "Iter: 9230  \tTraining Loss: -522.9006    \n",
      "    Negative Log Likelihood: 62.7013\tSigma2 Prior: -585.6028\tRegularization: 0.0009\n",
      "Iter: 9240  \tTraining Loss: -519.6122    \n",
      "    Negative Log Likelihood: 62.9437\tSigma2 Prior: -582.5568\tRegularization: 0.0009\n",
      "Iter: 9250  \tTraining Loss: -534.2671    \n",
      "    Negative Log Likelihood: 61.2432\tSigma2 Prior: -595.5111\tRegularization: 0.0009\n",
      "Iter: 9260  \tTraining Loss: -543.4583    \n",
      "    Negative Log Likelihood: 60.7215\tSigma2 Prior: -604.1806\tRegularization: 0.0009\n",
      "Iter: 9270  \tTraining Loss: -542.8501    \n",
      "    Negative Log Likelihood: 62.2680\tSigma2 Prior: -605.1190\tRegularization: 0.0009\n",
      "Iter: 9280  \tTraining Loss: -537.7767    \n",
      "    Negative Log Likelihood: 62.3585\tSigma2 Prior: -600.1360\tRegularization: 0.0009\n",
      "Iter: 9290  \tTraining Loss: -523.0936    \n",
      "    Negative Log Likelihood: 63.0782\tSigma2 Prior: -586.1727\tRegularization: 0.0009\n",
      "Iter: 9300  \tTraining Loss: -539.1503    \n",
      "    Negative Log Likelihood: 62.1037\tSigma2 Prior: -601.2548\tRegularization: 0.0009\n",
      "Iter: 9310  \tTraining Loss: -537.9177    \n",
      "    Negative Log Likelihood: 63.0795\tSigma2 Prior: -600.9981\tRegularization: 0.0009\n",
      "Iter: 9320  \tTraining Loss: -551.7729    \n",
      "    Negative Log Likelihood: 60.4643\tSigma2 Prior: -612.2380\tRegularization: 0.0009\n",
      "Iter: 9330  \tTraining Loss: -520.9064    \n",
      "    Negative Log Likelihood: 64.7070\tSigma2 Prior: -585.6142\tRegularization: 0.0009\n",
      "Iter: 9340  \tTraining Loss: -510.6171    \n",
      "    Negative Log Likelihood: 63.0111\tSigma2 Prior: -573.6290\tRegularization: 0.0009\n",
      "Iter: 9350  \tTraining Loss: -538.6969    \n",
      "    Negative Log Likelihood: 61.5840\tSigma2 Prior: -600.2818\tRegularization: 0.0009\n",
      "Iter: 9360  \tTraining Loss: -543.4890    \n",
      "    Negative Log Likelihood: 60.6541\tSigma2 Prior: -604.1440\tRegularization: 0.0009\n",
      "Iter: 9370  \tTraining Loss: -522.2598    \n",
      "    Negative Log Likelihood: 62.9649\tSigma2 Prior: -585.2256\tRegularization: 0.0009\n",
      "Iter: 9380  \tTraining Loss: -534.0893    \n",
      "    Negative Log Likelihood: 63.4643\tSigma2 Prior: -597.5544\tRegularization: 0.0009\n",
      "Iter: 9390  \tTraining Loss: -540.4137    \n",
      "    Negative Log Likelihood: 62.6558\tSigma2 Prior: -603.0703\tRegularization: 0.0009\n",
      "Iter: 9400  \tTraining Loss: -537.4601    \n",
      "    Negative Log Likelihood: 61.2628\tSigma2 Prior: -598.7238\tRegularization: 0.0009\n",
      "Iter: 9410  \tTraining Loss: -539.8366    \n",
      "    Negative Log Likelihood: 61.2990\tSigma2 Prior: -601.1364\tRegularization: 0.0009\n",
      "Iter: 9420  \tTraining Loss: -526.3804    \n",
      "    Negative Log Likelihood: 62.8381\tSigma2 Prior: -589.2194\tRegularization: 0.0009\n",
      "Iter: 9430  \tTraining Loss: -545.8258    \n",
      "    Negative Log Likelihood: 60.4064\tSigma2 Prior: -606.2330\tRegularization: 0.0009\n",
      "Iter: 9440  \tTraining Loss: -529.9826    \n",
      "    Negative Log Likelihood: 63.2961\tSigma2 Prior: -593.2795\tRegularization: 0.0009\n",
      "Iter: 9450  \tTraining Loss: -523.5828    \n",
      "    Negative Log Likelihood: 64.1404\tSigma2 Prior: -587.7241\tRegularization: 0.0009\n",
      "Iter: 9460  \tTraining Loss: -531.4532    \n",
      "    Negative Log Likelihood: 61.8317\tSigma2 Prior: -593.2858\tRegularization: 0.0009\n",
      "Iter: 9470  \tTraining Loss: -532.8715    \n",
      "    Negative Log Likelihood: 62.5532\tSigma2 Prior: -595.4255\tRegularization: 0.0009\n",
      "Iter: 9480  \tTraining Loss: -524.5373    \n",
      "    Negative Log Likelihood: 62.7476\tSigma2 Prior: -587.2858\tRegularization: 0.0009\n",
      "Iter: 9490  \tTraining Loss: -527.9212    \n",
      "    Negative Log Likelihood: 62.6044\tSigma2 Prior: -590.5265\tRegularization: 0.0009\n",
      "Iter: 9500  \tTraining Loss: -509.7111    \n",
      "    Negative Log Likelihood: 65.8257\tSigma2 Prior: -575.5377\tRegularization: 0.0009\n",
      "Iter: 9510  \tTraining Loss: -532.9894    \n",
      "    Negative Log Likelihood: 63.1100\tSigma2 Prior: -596.1003\tRegularization: 0.0009\n",
      "Iter: 9520  \tTraining Loss: -538.7985    \n",
      "    Negative Log Likelihood: 60.4877\tSigma2 Prior: -599.2870\tRegularization: 0.0009\n",
      "Iter: 9530  \tTraining Loss: -540.7678    \n",
      "    Negative Log Likelihood: 60.5042\tSigma2 Prior: -601.2728\tRegularization: 0.0009\n",
      "Iter: 9540  \tTraining Loss: -535.6788    \n",
      "    Negative Log Likelihood: 62.0583\tSigma2 Prior: -597.7380\tRegularization: 0.0009\n",
      "Iter: 9550  \tTraining Loss: -540.2462    \n",
      "    Negative Log Likelihood: 62.4748\tSigma2 Prior: -602.7219\tRegularization: 0.0009\n",
      "Iter: 9560  \tTraining Loss: -505.2227    \n",
      "    Negative Log Likelihood: 65.7552\tSigma2 Prior: -570.9788\tRegularization: 0.0009\n",
      "Iter: 9570  \tTraining Loss: -542.9092    \n",
      "    Negative Log Likelihood: 62.5354\tSigma2 Prior: -605.4455\tRegularization: 0.0009\n",
      "Iter: 9580  \tTraining Loss: -526.7171    \n",
      "    Negative Log Likelihood: 62.9811\tSigma2 Prior: -589.6990\tRegularization: 0.0009\n",
      "Iter: 9590  \tTraining Loss: -525.4983    \n",
      "    Negative Log Likelihood: 64.4383\tSigma2 Prior: -589.9375\tRegularization: 0.0009\n",
      "Iter: 9600  \tTraining Loss: -538.4502    \n",
      "    Negative Log Likelihood: 61.7863\tSigma2 Prior: -600.2374\tRegularization: 0.0009\n",
      "Iter: 9610  \tTraining Loss: -534.5669    \n",
      "    Negative Log Likelihood: 63.3859\tSigma2 Prior: -597.9536\tRegularization: 0.0009\n",
      "Iter: 9620  \tTraining Loss: -541.9218    \n",
      "    Negative Log Likelihood: 61.8599\tSigma2 Prior: -603.7825\tRegularization: 0.0009\n",
      "Iter: 9630  \tTraining Loss: -507.8305    \n",
      "    Negative Log Likelihood: 64.1884\tSigma2 Prior: -572.0198\tRegularization: 0.0009\n",
      "Iter: 9640  \tTraining Loss: -502.6644    \n",
      "    Negative Log Likelihood: 66.2534\tSigma2 Prior: -568.9187\tRegularization: 0.0009\n",
      "Iter: 9650  \tTraining Loss: -554.1396    \n",
      "    Negative Log Likelihood: 59.6208\tSigma2 Prior: -613.7613\tRegularization: 0.0009\n",
      "Iter: 9660  \tTraining Loss: -519.7299    \n",
      "    Negative Log Likelihood: 64.3360\tSigma2 Prior: -584.0667\tRegularization: 0.0009\n",
      "Iter: 9670  \tTraining Loss: -535.0179    \n",
      "    Negative Log Likelihood: 62.1936\tSigma2 Prior: -597.2124\tRegularization: 0.0009\n",
      "Iter: 9680  \tTraining Loss: -523.6680    \n",
      "    Negative Log Likelihood: 63.1244\tSigma2 Prior: -586.7932\tRegularization: 0.0009\n",
      "Iter: 9690  \tTraining Loss: -526.4509    \n",
      "    Negative Log Likelihood: 62.4621\tSigma2 Prior: -588.9138\tRegularization: 0.0009\n",
      "Iter: 9700  \tTraining Loss: -550.6055    \n",
      "    Negative Log Likelihood: 59.8935\tSigma2 Prior: -610.4999\tRegularization: 0.0009\n",
      "Iter: 9710  \tTraining Loss: -526.3752    \n",
      "    Negative Log Likelihood: 62.7870\tSigma2 Prior: -589.1631\tRegularization: 0.0009\n",
      "Iter: 9720  \tTraining Loss: -534.5007    \n",
      "    Negative Log Likelihood: 63.4824\tSigma2 Prior: -597.9839\tRegularization: 0.0009\n",
      "Iter: 9730  \tTraining Loss: -544.4564    \n",
      "    Negative Log Likelihood: 60.3641\tSigma2 Prior: -604.8214\tRegularization: 0.0009\n",
      "Iter: 9740  \tTraining Loss: -538.3727    \n",
      "    Negative Log Likelihood: 62.2598\tSigma2 Prior: -600.6334\tRegularization: 0.0009\n",
      "Iter: 9750  \tTraining Loss: -542.1524    \n",
      "    Negative Log Likelihood: 62.1879\tSigma2 Prior: -604.3412\tRegularization: 0.0009\n",
      "Iter: 9760  \tTraining Loss: -538.1885    \n",
      "    Negative Log Likelihood: 61.3507\tSigma2 Prior: -599.5402\tRegularization: 0.0009\n",
      "Iter: 9770  \tTraining Loss: -543.1859    \n",
      "    Negative Log Likelihood: 60.9495\tSigma2 Prior: -604.1362\tRegularization: 0.0009\n",
      "Iter: 9780  \tTraining Loss: -523.3240    \n",
      "    Negative Log Likelihood: 63.0391\tSigma2 Prior: -586.3640\tRegularization: 0.0009\n",
      "Iter: 9790  \tTraining Loss: -550.3033    \n",
      "    Negative Log Likelihood: 60.4140\tSigma2 Prior: -610.7181\tRegularization: 0.0009\n",
      "Iter: 9800  \tTraining Loss: -528.9587    \n",
      "    Negative Log Likelihood: 63.6822\tSigma2 Prior: -592.6417\tRegularization: 0.0009\n",
      "Iter: 9810  \tTraining Loss: -531.8860    \n",
      "    Negative Log Likelihood: 62.0785\tSigma2 Prior: -593.9654\tRegularization: 0.0009\n",
      "Iter: 9820  \tTraining Loss: -536.9678    \n",
      "    Negative Log Likelihood: 61.3117\tSigma2 Prior: -598.2803\tRegularization: 0.0009\n",
      "Iter: 9830  \tTraining Loss: -547.0794    \n",
      "    Negative Log Likelihood: 62.6202\tSigma2 Prior: -609.7005\tRegularization: 0.0009\n",
      "Iter: 9840  \tTraining Loss: -543.4485    \n",
      "    Negative Log Likelihood: 61.8222\tSigma2 Prior: -605.2715\tRegularization: 0.0009\n",
      "Iter: 9850  \tTraining Loss: -548.1519    \n",
      "    Negative Log Likelihood: 61.2519\tSigma2 Prior: -609.4046\tRegularization: 0.0009\n",
      "Iter: 9860  \tTraining Loss: -538.6707    \n",
      "    Negative Log Likelihood: 62.6237\tSigma2 Prior: -601.2952\tRegularization: 0.0009\n",
      "Iter: 9870  \tTraining Loss: -528.7518    \n",
      "    Negative Log Likelihood: 64.1516\tSigma2 Prior: -592.9042\tRegularization: 0.0009\n",
      "Iter: 9880  \tTraining Loss: -501.0075    \n",
      "    Negative Log Likelihood: 66.7578\tSigma2 Prior: -567.7662\tRegularization: 0.0009\n",
      "Iter: 9890  \tTraining Loss: -520.9998    \n",
      "    Negative Log Likelihood: 64.0798\tSigma2 Prior: -585.0804\tRegularization: 0.0009\n",
      "Iter: 9900  \tTraining Loss: -531.1962    \n",
      "    Negative Log Likelihood: 62.4580\tSigma2 Prior: -593.6550\tRegularization: 0.0009\n",
      "Iter: 9910  \tTraining Loss: -545.8986    \n",
      "    Negative Log Likelihood: 60.9953\tSigma2 Prior: -606.8948\tRegularization: 0.0009\n",
      "Iter: 9920  \tTraining Loss: -532.6991    \n",
      "    Negative Log Likelihood: 62.9052\tSigma2 Prior: -595.6051\tRegularization: 0.0009\n",
      "Iter: 9930  \tTraining Loss: -533.8197    \n",
      "    Negative Log Likelihood: 62.0547\tSigma2 Prior: -595.8752\tRegularization: 0.0009\n",
      "Iter: 9940  \tTraining Loss: -510.9728    \n",
      "    Negative Log Likelihood: 66.1840\tSigma2 Prior: -577.1577\tRegularization: 0.0009\n",
      "Iter: 9950  \tTraining Loss: -544.0006    \n",
      "    Negative Log Likelihood: 60.6266\tSigma2 Prior: -604.6281\tRegularization: 0.0009\n",
      "Iter: 9960  \tTraining Loss: -536.6982    \n",
      "    Negative Log Likelihood: 62.6416\tSigma2 Prior: -599.3406\tRegularization: 0.0009\n",
      "Iter: 9970  \tTraining Loss: -533.7455    \n",
      "    Negative Log Likelihood: 62.0682\tSigma2 Prior: -595.8146\tRegularization: 0.0009\n",
      "Iter: 9980  \tTraining Loss: -543.1268    \n",
      "    Negative Log Likelihood: 61.0392\tSigma2 Prior: -604.1669\tRegularization: 0.0009\n",
      "Iter: 9990  \tTraining Loss: -537.7808    \n",
      "    Negative Log Likelihood: 63.1912\tSigma2 Prior: -600.9728\tRegularization: 0.0009\n",
      "Iter: 10000  \tTraining Loss: -525.6465    \n",
      "    Negative Log Likelihood: 64.0158\tSigma2 Prior: -589.6633\tRegularization: 0.0009\n",
      "Iter: 10010  \tTraining Loss: -530.2962    \n",
      "    Negative Log Likelihood: 63.3338\tSigma2 Prior: -593.6309\tRegularization: 0.0009\n",
      "Iter: 10020  \tTraining Loss: -531.0679    \n",
      "    Negative Log Likelihood: 63.3611\tSigma2 Prior: -594.4299\tRegularization: 0.0009\n",
      "Iter: 10030  \tTraining Loss: -533.1884    \n",
      "    Negative Log Likelihood: 61.5718\tSigma2 Prior: -594.7612\tRegularization: 0.0009\n",
      "Iter: 10040  \tTraining Loss: -530.6813    \n",
      "    Negative Log Likelihood: 61.7603\tSigma2 Prior: -592.4426\tRegularization: 0.0009\n",
      "Iter: 10050  \tTraining Loss: -552.4173    \n",
      "    Negative Log Likelihood: 60.8233\tSigma2 Prior: -613.2415\tRegularization: 0.0009\n",
      "Iter: 10060  \tTraining Loss: -523.2919    \n",
      "    Negative Log Likelihood: 62.7889\tSigma2 Prior: -586.0818\tRegularization: 0.0009\n",
      "Iter: 10070  \tTraining Loss: -539.7901    \n",
      "    Negative Log Likelihood: 62.3478\tSigma2 Prior: -602.1388\tRegularization: 0.0009\n",
      "Iter: 10080  \tTraining Loss: -508.4829    \n",
      "    Negative Log Likelihood: 65.4687\tSigma2 Prior: -573.9525\tRegularization: 0.0009\n",
      "Iter: 10090  \tTraining Loss: -541.4627    \n",
      "    Negative Log Likelihood: 61.0643\tSigma2 Prior: -602.5279\tRegularization: 0.0009\n",
      "Iter: 10100  \tTraining Loss: -537.9417    \n",
      "    Negative Log Likelihood: 61.4111\tSigma2 Prior: -599.3538\tRegularization: 0.0009\n",
      "Iter: 10110  \tTraining Loss: -524.1047    \n",
      "    Negative Log Likelihood: 63.4132\tSigma2 Prior: -587.5187\tRegularization: 0.0009\n",
      "Iter: 10120  \tTraining Loss: -507.4605    \n",
      "    Negative Log Likelihood: 64.2194\tSigma2 Prior: -571.6808\tRegularization: 0.0009\n",
      "Iter: 10130  \tTraining Loss: -539.0371    \n",
      "    Negative Log Likelihood: 61.8304\tSigma2 Prior: -600.8684\tRegularization: 0.0009\n",
      "Iter: 10140  \tTraining Loss: -531.9705    \n",
      "    Negative Log Likelihood: 61.5310\tSigma2 Prior: -593.5024\tRegularization: 0.0009\n",
      "Iter: 10150  \tTraining Loss: -559.2345    \n",
      "    Negative Log Likelihood: 59.0560\tSigma2 Prior: -618.2914\tRegularization: 0.0009\n",
      "Iter: 10160  \tTraining Loss: -533.3292    \n",
      "    Negative Log Likelihood: 62.8093\tSigma2 Prior: -596.1395\tRegularization: 0.0009\n",
      "Iter: 10170  \tTraining Loss: -545.5743    \n",
      "    Negative Log Likelihood: 61.7784\tSigma2 Prior: -607.3536\tRegularization: 0.0009\n",
      "Iter: 10180  \tTraining Loss: -530.8247    \n",
      "    Negative Log Likelihood: 63.0486\tSigma2 Prior: -593.8742\tRegularization: 0.0009\n",
      "Iter: 10190  \tTraining Loss: -533.5254    \n",
      "    Negative Log Likelihood: 62.9397\tSigma2 Prior: -596.4660\tRegularization: 0.0009\n",
      "Iter: 10200  \tTraining Loss: -508.2495    \n",
      "    Negative Log Likelihood: 64.9103\tSigma2 Prior: -573.1607\tRegularization: 0.0009\n",
      "Iter: 10210  \tTraining Loss: -543.6501    \n",
      "    Negative Log Likelihood: 61.5576\tSigma2 Prior: -605.2086\tRegularization: 0.0009\n",
      "Iter: 10220  \tTraining Loss: -528.6043    \n",
      "    Negative Log Likelihood: 62.5267\tSigma2 Prior: -591.1319\tRegularization: 0.0009\n",
      "Iter: 10230  \tTraining Loss: -547.9987    \n",
      "    Negative Log Likelihood: 60.3181\tSigma2 Prior: -608.3177\tRegularization: 0.0009\n",
      "Iter: 10240  \tTraining Loss: -529.1519    \n",
      "    Negative Log Likelihood: 63.9573\tSigma2 Prior: -593.1101\tRegularization: 0.0009\n",
      "Iter: 10250  \tTraining Loss: -508.3212    \n",
      "    Negative Log Likelihood: 64.7469\tSigma2 Prior: -573.0690\tRegularization: 0.0009\n",
      "Iter: 10260  \tTraining Loss: -534.2673    \n",
      "    Negative Log Likelihood: 63.1368\tSigma2 Prior: -597.4050\tRegularization: 0.0009\n",
      "Iter: 10270  \tTraining Loss: -555.3322    \n",
      "    Negative Log Likelihood: 59.2337\tSigma2 Prior: -614.5668\tRegularization: 0.0009\n",
      "Iter: 10280  \tTraining Loss: -513.7097    \n",
      "    Negative Log Likelihood: 64.0832\tSigma2 Prior: -577.7938\tRegularization: 0.0009\n",
      "Iter: 10290  \tTraining Loss: -542.9192    \n",
      "    Negative Log Likelihood: 60.5092\tSigma2 Prior: -603.4293\tRegularization: 0.0009\n",
      "Iter: 10300  \tTraining Loss: -533.7081    \n",
      "    Negative Log Likelihood: 61.4179\tSigma2 Prior: -595.1269\tRegularization: 0.0009\n",
      "Iter: 10310  \tTraining Loss: -538.7932    \n",
      "    Negative Log Likelihood: 60.9614\tSigma2 Prior: -599.7556\tRegularization: 0.0009\n",
      "Iter: 10320  \tTraining Loss: -522.4337    \n",
      "    Negative Log Likelihood: 62.6040\tSigma2 Prior: -585.0386\tRegularization: 0.0009\n",
      "Iter: 10330  \tTraining Loss: -535.5096    \n",
      "    Negative Log Likelihood: 62.2573\tSigma2 Prior: -597.7678\tRegularization: 0.0009\n",
      "Iter: 10340  \tTraining Loss: -537.2524    \n",
      "    Negative Log Likelihood: 61.7270\tSigma2 Prior: -598.9803\tRegularization: 0.0009\n",
      "Iter: 10350  \tTraining Loss: -524.6885    \n",
      "    Negative Log Likelihood: 63.3343\tSigma2 Prior: -588.0238\tRegularization: 0.0009\n",
      "Iter: 10360  \tTraining Loss: -540.2715    \n",
      "    Negative Log Likelihood: 63.1305\tSigma2 Prior: -603.4030\tRegularization: 0.0009\n",
      "Iter: 10370  \tTraining Loss: -502.9799    \n",
      "    Negative Log Likelihood: 64.8804\tSigma2 Prior: -567.8613\tRegularization: 0.0009\n",
      "Iter: 10380  \tTraining Loss: -530.5048    \n",
      "    Negative Log Likelihood: 63.8642\tSigma2 Prior: -594.3700\tRegularization: 0.0009\n",
      "Iter: 10390  \tTraining Loss: -538.2218    \n",
      "    Negative Log Likelihood: 60.8090\tSigma2 Prior: -599.0317\tRegularization: 0.0009\n",
      "Iter: 10400  \tTraining Loss: -528.2492    \n",
      "    Negative Log Likelihood: 62.1574\tSigma2 Prior: -590.4075\tRegularization: 0.0009\n",
      "Iter: 10410  \tTraining Loss: -544.2199    \n",
      "    Negative Log Likelihood: 60.3996\tSigma2 Prior: -604.6204\tRegularization: 0.0009\n",
      "Iter: 10420  \tTraining Loss: -528.7896    \n",
      "    Negative Log Likelihood: 62.5716\tSigma2 Prior: -591.3621\tRegularization: 0.0009\n",
      "Iter: 10430  \tTraining Loss: -534.6661    \n",
      "    Negative Log Likelihood: 61.2343\tSigma2 Prior: -595.9013\tRegularization: 0.0009\n",
      "Iter: 10440  \tTraining Loss: -546.5435    \n",
      "    Negative Log Likelihood: 59.5600\tSigma2 Prior: -606.1044\tRegularization: 0.0009\n",
      "Iter: 10450  \tTraining Loss: -534.4515    \n",
      "    Negative Log Likelihood: 61.9913\tSigma2 Prior: -596.4437\tRegularization: 0.0009\n",
      "Iter: 10460  \tTraining Loss: -518.1435    \n",
      "    Negative Log Likelihood: 63.7349\tSigma2 Prior: -581.8793\tRegularization: 0.0009\n",
      "Iter: 10470  \tTraining Loss: -530.6481    \n",
      "    Negative Log Likelihood: 60.9184\tSigma2 Prior: -591.5674\tRegularization: 0.0009\n",
      "Iter: 10480  \tTraining Loss: -534.5916    \n",
      "    Negative Log Likelihood: 60.5394\tSigma2 Prior: -595.1318\tRegularization: 0.0009\n",
      "Iter: 10490  \tTraining Loss: -526.3907    \n",
      "    Negative Log Likelihood: 61.7460\tSigma2 Prior: -588.1376\tRegularization: 0.0009\n",
      "Iter: 10500  \tTraining Loss: -512.3061    \n",
      "    Negative Log Likelihood: 62.8681\tSigma2 Prior: -575.1751\tRegularization: 0.0009\n",
      "Iter: 10510  \tTraining Loss: -534.6458    \n",
      "    Negative Log Likelihood: 61.8088\tSigma2 Prior: -596.4555\tRegularization: 0.0009\n",
      "Iter: 10520  \tTraining Loss: -528.3380    \n",
      "    Negative Log Likelihood: 62.8471\tSigma2 Prior: -591.1859\tRegularization: 0.0009\n",
      "Iter: 10530  \tTraining Loss: -533.9453    \n",
      "    Negative Log Likelihood: 61.6244\tSigma2 Prior: -595.5706\tRegularization: 0.0009\n",
      "Iter: 10540  \tTraining Loss: -524.8232    \n",
      "    Negative Log Likelihood: 61.3250\tSigma2 Prior: -586.1490\tRegularization: 0.0009\n",
      "Iter: 10550  \tTraining Loss: -532.1664    \n",
      "    Negative Log Likelihood: 62.8681\tSigma2 Prior: -595.0354\tRegularization: 0.0009\n",
      "Iter: 10560  \tTraining Loss: -536.1194    \n",
      "    Negative Log Likelihood: 62.1447\tSigma2 Prior: -598.2651\tRegularization: 0.0009\n",
      "Iter: 10570  \tTraining Loss: -544.9935    \n",
      "    Negative Log Likelihood: 60.3374\tSigma2 Prior: -605.3318\tRegularization: 0.0009\n",
      "Iter: 10580  \tTraining Loss: -550.1339    \n",
      "    Negative Log Likelihood: 60.8285\tSigma2 Prior: -610.9633\tRegularization: 0.0009\n",
      "Iter: 10590  \tTraining Loss: -507.9635    \n",
      "    Negative Log Likelihood: 65.1989\tSigma2 Prior: -573.1633\tRegularization: 0.0009\n",
      "Iter: 10600  \tTraining Loss: -534.4036    \n",
      "    Negative Log Likelihood: 61.8398\tSigma2 Prior: -596.2443\tRegularization: 0.0009\n",
      "Iter: 10610  \tTraining Loss: -526.8002    \n",
      "    Negative Log Likelihood: 62.6115\tSigma2 Prior: -589.4127\tRegularization: 0.0009\n",
      "Iter: 10620  \tTraining Loss: -549.8311    \n",
      "    Negative Log Likelihood: 60.1787\tSigma2 Prior: -610.0107\tRegularization: 0.0009\n",
      "Iter: 10630  \tTraining Loss: -541.0467    \n",
      "    Negative Log Likelihood: 61.8616\tSigma2 Prior: -602.9092\tRegularization: 0.0009\n",
      "Iter: 10640  \tTraining Loss: -549.5111    \n",
      "    Negative Log Likelihood: 59.4585\tSigma2 Prior: -608.9706\tRegularization: 0.0009\n",
      "Iter: 10650  \tTraining Loss: -531.1690    \n",
      "    Negative Log Likelihood: 63.0338\tSigma2 Prior: -594.2037\tRegularization: 0.0009\n",
      "Iter: 10660  \tTraining Loss: -534.5217    \n",
      "    Negative Log Likelihood: 62.7367\tSigma2 Prior: -597.2593\tRegularization: 0.0009\n",
      "Iter: 10670  \tTraining Loss: -550.5679    \n",
      "    Negative Log Likelihood: 61.5259\tSigma2 Prior: -612.0947\tRegularization: 0.0009\n",
      "Iter: 10680  \tTraining Loss: -535.8163    \n",
      "    Negative Log Likelihood: 62.4232\tSigma2 Prior: -598.2404\tRegularization: 0.0009\n",
      "Iter: 10690  \tTraining Loss: -538.4770    \n",
      "    Negative Log Likelihood: 63.2832\tSigma2 Prior: -601.7612\tRegularization: 0.0009\n",
      "Iter: 10700  \tTraining Loss: -500.6774    \n",
      "    Negative Log Likelihood: 65.8057\tSigma2 Prior: -566.4840\tRegularization: 0.0009\n",
      "Iter: 10710  \tTraining Loss: -534.5687    \n",
      "    Negative Log Likelihood: 62.3502\tSigma2 Prior: -596.9198\tRegularization: 0.0009\n",
      "Iter: 10720  \tTraining Loss: -531.6057    \n",
      "    Negative Log Likelihood: 62.8250\tSigma2 Prior: -594.4317\tRegularization: 0.0009\n",
      "Iter: 10730  \tTraining Loss: -531.9391    \n",
      "    Negative Log Likelihood: 62.0610\tSigma2 Prior: -594.0010\tRegularization: 0.0009\n",
      "Iter: 10740  \tTraining Loss: -528.8339    \n",
      "    Negative Log Likelihood: 62.4931\tSigma2 Prior: -591.3279\tRegularization: 0.0009\n",
      "Iter: 10750  \tTraining Loss: -540.3586    \n",
      "    Negative Log Likelihood: 60.1870\tSigma2 Prior: -600.5466\tRegularization: 0.0009\n",
      "Iter: 10760  \tTraining Loss: -541.1745    \n",
      "    Negative Log Likelihood: 60.7627\tSigma2 Prior: -601.9382\tRegularization: 0.0009\n",
      "Iter: 10770  \tTraining Loss: -539.4576    \n",
      "    Negative Log Likelihood: 60.8099\tSigma2 Prior: -600.2684\tRegularization: 0.0009\n",
      "Iter: 10780  \tTraining Loss: -529.0266    \n",
      "    Negative Log Likelihood: 61.9666\tSigma2 Prior: -590.9941\tRegularization: 0.0009\n",
      "Iter: 10790  \tTraining Loss: -546.3509    \n",
      "    Negative Log Likelihood: 60.6582\tSigma2 Prior: -607.0100\tRegularization: 0.0009\n",
      "Iter: 10800  \tTraining Loss: -549.5790    \n",
      "    Negative Log Likelihood: 59.8991\tSigma2 Prior: -609.4790\tRegularization: 0.0009\n",
      "Iter: 10810  \tTraining Loss: -545.1229    \n",
      "    Negative Log Likelihood: 60.8365\tSigma2 Prior: -605.9603\tRegularization: 0.0009\n",
      "Iter: 10820  \tTraining Loss: -544.1129    \n",
      "    Negative Log Likelihood: 61.7730\tSigma2 Prior: -605.8868\tRegularization: 0.0009\n",
      "Iter: 10830  \tTraining Loss: -532.4611    \n",
      "    Negative Log Likelihood: 62.9436\tSigma2 Prior: -595.4056\tRegularization: 0.0009\n",
      "Iter: 10840  \tTraining Loss: -544.3824    \n",
      "    Negative Log Likelihood: 60.3940\tSigma2 Prior: -604.7773\tRegularization: 0.0009\n",
      "Iter: 10850  \tTraining Loss: -536.6093    \n",
      "    Negative Log Likelihood: 61.8553\tSigma2 Prior: -598.4655\tRegularization: 0.0009\n",
      "Iter: 10860  \tTraining Loss: -528.0371    \n",
      "    Negative Log Likelihood: 63.6234\tSigma2 Prior: -591.6614\tRegularization: 0.0009\n",
      "Iter: 10870  \tTraining Loss: -538.2929    \n",
      "    Negative Log Likelihood: 62.6129\tSigma2 Prior: -600.9067\tRegularization: 0.0009\n",
      "Iter: 10880  \tTraining Loss: -533.7197    \n",
      "    Negative Log Likelihood: 61.5702\tSigma2 Prior: -595.2908\tRegularization: 0.0009\n",
      "Iter: 10890  \tTraining Loss: -524.1938    \n",
      "    Negative Log Likelihood: 64.8526\tSigma2 Prior: -589.0473\tRegularization: 0.0009\n",
      "Iter: 10900  \tTraining Loss: -522.0359    \n",
      "    Negative Log Likelihood: 62.9959\tSigma2 Prior: -585.0327\tRegularization: 0.0009\n",
      "Iter: 10910  \tTraining Loss: -532.3591    \n",
      "    Negative Log Likelihood: 63.3038\tSigma2 Prior: -595.6639\tRegularization: 0.0009\n",
      "Iter: 10920  \tTraining Loss: -541.6099    \n",
      "    Negative Log Likelihood: 62.3503\tSigma2 Prior: -603.9611\tRegularization: 0.0009\n",
      "Iter: 10930  \tTraining Loss: -537.9213    \n",
      "    Negative Log Likelihood: 62.5910\tSigma2 Prior: -600.5132\tRegularization: 0.0009\n",
      "Iter: 10940  \tTraining Loss: -550.9903    \n",
      "    Negative Log Likelihood: 60.3161\tSigma2 Prior: -611.3073\tRegularization: 0.0009\n",
      "Iter: 10950  \tTraining Loss: -537.4728    \n",
      "    Negative Log Likelihood: 61.3672\tSigma2 Prior: -598.8409\tRegularization: 0.0009\n",
      "Iter: 10960  \tTraining Loss: -549.5448    \n",
      "    Negative Log Likelihood: 59.7241\tSigma2 Prior: -609.2698\tRegularization: 0.0009\n",
      "Iter: 10970  \tTraining Loss: -549.9716    \n",
      "    Negative Log Likelihood: 61.9468\tSigma2 Prior: -611.9193\tRegularization: 0.0009\n",
      "Iter: 10980  \tTraining Loss: -526.5271    \n",
      "    Negative Log Likelihood: 61.9934\tSigma2 Prior: -588.5214\tRegularization: 0.0009\n",
      "Iter: 10990  \tTraining Loss: -543.4020    \n",
      "    Negative Log Likelihood: 61.9838\tSigma2 Prior: -605.3867\tRegularization: 0.0009\n",
      "Iter: 11000  \tTraining Loss: -542.5841    \n",
      "    Negative Log Likelihood: 61.6222\tSigma2 Prior: -604.2073\tRegularization: 0.0009\n",
      "Iter: 11010  \tTraining Loss: -550.8760    \n",
      "    Negative Log Likelihood: 58.2247\tSigma2 Prior: -609.1016\tRegularization: 0.0009\n",
      "Iter: 11020  \tTraining Loss: -529.3107    \n",
      "    Negative Log Likelihood: 62.9086\tSigma2 Prior: -592.2202\tRegularization: 0.0009\n",
      "Iter: 11030  \tTraining Loss: -543.8209    \n",
      "    Negative Log Likelihood: 60.6073\tSigma2 Prior: -604.4291\tRegularization: 0.0009\n",
      "Iter: 11040  \tTraining Loss: -525.4482    \n",
      "    Negative Log Likelihood: 64.2785\tSigma2 Prior: -589.7277\tRegularization: 0.0009\n",
      "Iter: 11050  \tTraining Loss: -514.9603    \n",
      "    Negative Log Likelihood: 64.1946\tSigma2 Prior: -579.1558\tRegularization: 0.0009\n",
      "Iter: 11060  \tTraining Loss: -526.8372    \n",
      "    Negative Log Likelihood: 61.6100\tSigma2 Prior: -588.4481\tRegularization: 0.0009\n",
      "Iter: 11070  \tTraining Loss: -538.7358    \n",
      "    Negative Log Likelihood: 62.1936\tSigma2 Prior: -600.9303\tRegularization: 0.0009\n",
      "Iter: 11080  \tTraining Loss: -526.4756    \n",
      "    Negative Log Likelihood: 62.5956\tSigma2 Prior: -589.0721\tRegularization: 0.0009\n",
      "Iter: 11090  \tTraining Loss: -519.3184    \n",
      "    Negative Log Likelihood: 64.2130\tSigma2 Prior: -583.5323\tRegularization: 0.0009\n",
      "Iter: 11100  \tTraining Loss: -535.8397    \n",
      "    Negative Log Likelihood: 61.6402\tSigma2 Prior: -597.4808\tRegularization: 0.0009\n",
      "Iter: 11110  \tTraining Loss: -534.2382    \n",
      "    Negative Log Likelihood: 62.1103\tSigma2 Prior: -596.3494\tRegularization: 0.0009\n",
      "Iter: 11120  \tTraining Loss: -515.3859    \n",
      "    Negative Log Likelihood: 63.5595\tSigma2 Prior: -578.9464\tRegularization: 0.0009\n",
      "Iter: 11130  \tTraining Loss: -548.6910    \n",
      "    Negative Log Likelihood: 60.6857\tSigma2 Prior: -609.3777\tRegularization: 0.0009\n",
      "Iter: 11140  \tTraining Loss: -527.1027    \n",
      "    Negative Log Likelihood: 63.2060\tSigma2 Prior: -590.3096\tRegularization: 0.0009\n",
      "Iter: 11150  \tTraining Loss: -529.4404    \n",
      "    Negative Log Likelihood: 62.4764\tSigma2 Prior: -591.9177\tRegularization: 0.0009\n",
      "Iter: 11160  \tTraining Loss: -539.3553    \n",
      "    Negative Log Likelihood: 62.7655\tSigma2 Prior: -602.1218\tRegularization: 0.0009\n",
      "Iter: 11170  \tTraining Loss: -524.6505    \n",
      "    Negative Log Likelihood: 63.6695\tSigma2 Prior: -588.3209\tRegularization: 0.0009\n",
      "Iter: 11180  \tTraining Loss: -533.2355    \n",
      "    Negative Log Likelihood: 61.8900\tSigma2 Prior: -595.1265\tRegularization: 0.0009\n",
      "Iter: 11190  \tTraining Loss: -531.0960    \n",
      "    Negative Log Likelihood: 63.2243\tSigma2 Prior: -594.3212\tRegularization: 0.0009\n",
      "Iter: 11200  \tTraining Loss: -545.5554    \n",
      "    Negative Log Likelihood: 61.3555\tSigma2 Prior: -606.9118\tRegularization: 0.0009\n",
      "Iter: 11210  \tTraining Loss: -519.3220    \n",
      "    Negative Log Likelihood: 64.8460\tSigma2 Prior: -584.1690\tRegularization: 0.0009\n",
      "Iter: 11220  \tTraining Loss: -537.7588    \n",
      "    Negative Log Likelihood: 62.4411\tSigma2 Prior: -600.2008\tRegularization: 0.0009\n",
      "Iter: 11230  \tTraining Loss: -534.9705    \n",
      "    Negative Log Likelihood: 62.6982\tSigma2 Prior: -597.6697\tRegularization: 0.0009\n",
      "Iter: 11240  \tTraining Loss: -527.3645    \n",
      "    Negative Log Likelihood: 63.3817\tSigma2 Prior: -590.7471\tRegularization: 0.0009\n",
      "Iter: 11250  \tTraining Loss: -522.4946    \n",
      "    Negative Log Likelihood: 64.1919\tSigma2 Prior: -586.6875\tRegularization: 0.0009\n",
      "Iter: 11260  \tTraining Loss: -537.2963    \n",
      "    Negative Log Likelihood: 61.5562\tSigma2 Prior: -598.8535\tRegularization: 0.0009\n",
      "Iter: 11270  \tTraining Loss: -542.0034    \n",
      "    Negative Log Likelihood: 59.3666\tSigma2 Prior: -601.3708\tRegularization: 0.0009\n",
      "Iter: 11280  \tTraining Loss: -528.8219    \n",
      "    Negative Log Likelihood: 62.3236\tSigma2 Prior: -591.1464\tRegularization: 0.0009\n",
      "Iter: 11290  \tTraining Loss: -525.4844    \n",
      "    Negative Log Likelihood: 64.7742\tSigma2 Prior: -590.2595\tRegularization: 0.0009\n",
      "Iter: 11300  \tTraining Loss: -542.0387    \n",
      "    Negative Log Likelihood: 61.3726\tSigma2 Prior: -603.4122\tRegularization: 0.0009\n",
      "Iter: 11310  \tTraining Loss: -554.0622    \n",
      "    Negative Log Likelihood: 60.0191\tSigma2 Prior: -614.0822\tRegularization: 0.0009\n",
      "Iter: 11320  \tTraining Loss: -537.3782    \n",
      "    Negative Log Likelihood: 60.8280\tSigma2 Prior: -598.2072\tRegularization: 0.0009\n",
      "Iter: 11330  \tTraining Loss: -547.8113    \n",
      "    Negative Log Likelihood: 60.8460\tSigma2 Prior: -608.6583\tRegularization: 0.0009\n",
      "Iter: 11340  \tTraining Loss: -528.4732    \n",
      "    Negative Log Likelihood: 64.4531\tSigma2 Prior: -592.9272\tRegularization: 0.0009\n",
      "Iter: 11350  \tTraining Loss: -545.7249    \n",
      "    Negative Log Likelihood: 61.5551\tSigma2 Prior: -607.2808\tRegularization: 0.0009\n",
      "Iter: 11360  \tTraining Loss: -526.0344    \n",
      "    Negative Log Likelihood: 64.3077\tSigma2 Prior: -590.3430\tRegularization: 0.0009\n",
      "Iter: 11370  \tTraining Loss: -539.7286    \n",
      "    Negative Log Likelihood: 61.2610\tSigma2 Prior: -600.9905\tRegularization: 0.0009\n",
      "Iter: 11380  \tTraining Loss: -544.1710    \n",
      "    Negative Log Likelihood: 61.4627\tSigma2 Prior: -605.6346\tRegularization: 0.0009\n",
      "Iter: 11390  \tTraining Loss: -548.4067    \n",
      "    Negative Log Likelihood: 60.6204\tSigma2 Prior: -609.0281\tRegularization: 0.0009\n",
      "Iter: 11400  \tTraining Loss: -522.2156    \n",
      "    Negative Log Likelihood: 63.8979\tSigma2 Prior: -586.1144\tRegularization: 0.0009\n",
      "Iter: 11410  \tTraining Loss: -519.3447    \n",
      "    Negative Log Likelihood: 63.3883\tSigma2 Prior: -582.7339\tRegularization: 0.0009\n",
      "Iter: 11420  \tTraining Loss: -535.8277    \n",
      "    Negative Log Likelihood: 62.0663\tSigma2 Prior: -597.8949\tRegularization: 0.0009\n",
      "Iter: 11430  \tTraining Loss: -531.8123    \n",
      "    Negative Log Likelihood: 61.5576\tSigma2 Prior: -593.3708\tRegularization: 0.0009\n",
      "Iter: 11440  \tTraining Loss: -554.9799    \n",
      "    Negative Log Likelihood: 60.0404\tSigma2 Prior: -615.0212\tRegularization: 0.0009\n",
      "Iter: 11450  \tTraining Loss: -532.9998    \n",
      "    Negative Log Likelihood: 62.2654\tSigma2 Prior: -595.2661\tRegularization: 0.0009\n",
      "Iter: 11460  \tTraining Loss: -534.8348    \n",
      "    Negative Log Likelihood: 61.6050\tSigma2 Prior: -596.4407\tRegularization: 0.0009\n",
      "Iter: 11470  \tTraining Loss: -534.3008    \n",
      "    Negative Log Likelihood: 61.0815\tSigma2 Prior: -595.3833\tRegularization: 0.0009\n",
      "Iter: 11480  \tTraining Loss: -518.3713    \n",
      "    Negative Log Likelihood: 63.7540\tSigma2 Prior: -582.1262\tRegularization: 0.0009\n",
      "Iter: 11490  \tTraining Loss: -516.5824    \n",
      "    Negative Log Likelihood: 64.5427\tSigma2 Prior: -581.1260\tRegularization: 0.0009\n",
      "Iter: 11500  \tTraining Loss: -523.8185    \n",
      "    Negative Log Likelihood: 64.2767\tSigma2 Prior: -588.0961\tRegularization: 0.0009\n",
      "Iter: 11510  \tTraining Loss: -527.1841    \n",
      "    Negative Log Likelihood: 63.5172\tSigma2 Prior: -590.7022\tRegularization: 0.0009\n",
      "Iter: 11520  \tTraining Loss: -532.5123    \n",
      "    Negative Log Likelihood: 60.7112\tSigma2 Prior: -593.2244\tRegularization: 0.0009\n",
      "Iter: 11530  \tTraining Loss: -521.9081    \n",
      "    Negative Log Likelihood: 63.2241\tSigma2 Prior: -585.1331\tRegularization: 0.0009\n",
      "Iter: 11540  \tTraining Loss: -541.4910    \n",
      "    Negative Log Likelihood: 61.0799\tSigma2 Prior: -602.5718\tRegularization: 0.0009\n",
      "Iter: 11550  \tTraining Loss: -537.1614    \n",
      "    Negative Log Likelihood: 62.1568\tSigma2 Prior: -599.3192\tRegularization: 0.0009\n",
      "Iter: 11560  \tTraining Loss: -531.2708    \n",
      "    Negative Log Likelihood: 61.3636\tSigma2 Prior: -592.6353\tRegularization: 0.0009\n",
      "Iter: 11570  \tTraining Loss: -535.3603    \n",
      "    Negative Log Likelihood: 61.7358\tSigma2 Prior: -597.0970\tRegularization: 0.0009\n",
      "Iter: 11580  \tTraining Loss: -531.5683    \n",
      "    Negative Log Likelihood: 63.0352\tSigma2 Prior: -594.6044\tRegularization: 0.0009\n",
      "Iter: 11590  \tTraining Loss: -543.0831    \n",
      "    Negative Log Likelihood: 60.2730\tSigma2 Prior: -603.3571\tRegularization: 0.0009\n",
      "Iter: 11600  \tTraining Loss: -536.5888    \n",
      "    Negative Log Likelihood: 62.3626\tSigma2 Prior: -598.9523\tRegularization: 0.0009\n",
      "Iter: 11610  \tTraining Loss: -542.8295    \n",
      "    Negative Log Likelihood: 60.9118\tSigma2 Prior: -603.7422\tRegularization: 0.0009\n",
      "Iter: 11620  \tTraining Loss: -546.8624    \n",
      "    Negative Log Likelihood: 60.4245\tSigma2 Prior: -607.2878\tRegularization: 0.0009\n",
      "Iter: 11630  \tTraining Loss: -539.7106    \n",
      "    Negative Log Likelihood: 61.2943\tSigma2 Prior: -601.0057\tRegularization: 0.0009\n",
      "Iter: 11640  \tTraining Loss: -528.3460    \n",
      "    Negative Log Likelihood: 64.1788\tSigma2 Prior: -592.5258\tRegularization: 0.0009\n",
      "Iter: 11650  \tTraining Loss: -532.6642    \n",
      "    Negative Log Likelihood: 62.1449\tSigma2 Prior: -594.8100\tRegularization: 0.0009\n",
      "Iter: 11660  \tTraining Loss: -539.8723    \n",
      "    Negative Log Likelihood: 61.2033\tSigma2 Prior: -601.0764\tRegularization: 0.0009\n",
      "Iter: 11670  \tTraining Loss: -541.2347    \n",
      "    Negative Log Likelihood: 62.7141\tSigma2 Prior: -603.9498\tRegularization: 0.0009\n",
      "Iter: 11680  \tTraining Loss: -544.0286    \n",
      "    Negative Log Likelihood: 60.4777\tSigma2 Prior: -604.5072\tRegularization: 0.0009\n",
      "Iter: 11690  \tTraining Loss: -533.6475    \n",
      "    Negative Log Likelihood: 62.4261\tSigma2 Prior: -596.0745\tRegularization: 0.0009\n",
      "Iter: 11700  \tTraining Loss: -531.4338    \n",
      "    Negative Log Likelihood: 62.7087\tSigma2 Prior: -594.1434\tRegularization: 0.0009\n",
      "Iter: 11710  \tTraining Loss: -538.4919    \n",
      "    Negative Log Likelihood: 60.7633\tSigma2 Prior: -599.2562\tRegularization: 0.0009\n",
      "Iter: 11720  \tTraining Loss: -544.2418    \n",
      "    Negative Log Likelihood: 59.8789\tSigma2 Prior: -604.1216\tRegularization: 0.0009\n",
      "Iter: 11730  \tTraining Loss: -538.6996    \n",
      "    Negative Log Likelihood: 62.3098\tSigma2 Prior: -601.0103\tRegularization: 0.0009\n",
      "Iter: 11740  \tTraining Loss: -547.4166    \n",
      "    Negative Log Likelihood: 60.0256\tSigma2 Prior: -607.4431\tRegularization: 0.0009\n",
      "Iter: 11750  \tTraining Loss: -498.9055    \n",
      "    Negative Log Likelihood: 65.5538\tSigma2 Prior: -564.4603\tRegularization: 0.0009\n",
      "Iter: 11760  \tTraining Loss: -522.7952    \n",
      "    Negative Log Likelihood: 63.1302\tSigma2 Prior: -585.9264\tRegularization: 0.0009\n",
      "Iter: 11770  \tTraining Loss: -545.4298    \n",
      "    Negative Log Likelihood: 60.4523\tSigma2 Prior: -605.8831\tRegularization: 0.0009\n",
      "Iter: 11780  \tTraining Loss: -542.4985    \n",
      "    Negative Log Likelihood: 60.7601\tSigma2 Prior: -603.2596\tRegularization: 0.0009\n",
      "Iter: 11790  \tTraining Loss: -523.0236    \n",
      "    Negative Log Likelihood: 63.6043\tSigma2 Prior: -586.6287\tRegularization: 0.0009\n",
      "Iter: 11800  \tTraining Loss: -519.8187    \n",
      "    Negative Log Likelihood: 64.8384\tSigma2 Prior: -584.6580\tRegularization: 0.0009\n",
      "Iter: 11810  \tTraining Loss: -536.6290    \n",
      "    Negative Log Likelihood: 60.6215\tSigma2 Prior: -597.2513\tRegularization: 0.0009\n",
      "Iter: 11820  \tTraining Loss: -536.5330    \n",
      "    Negative Log Likelihood: 61.6502\tSigma2 Prior: -598.1841\tRegularization: 0.0009\n",
      "Iter: 11830  \tTraining Loss: -541.1792    \n",
      "    Negative Log Likelihood: 61.8906\tSigma2 Prior: -603.0707\tRegularization: 0.0009\n",
      "Iter: 11840  \tTraining Loss: -524.1077    \n",
      "    Negative Log Likelihood: 63.7984\tSigma2 Prior: -587.9069\tRegularization: 0.0009\n",
      "Iter: 11850  \tTraining Loss: -552.2505    \n",
      "    Negative Log Likelihood: 60.1775\tSigma2 Prior: -612.4289\tRegularization: 0.0009\n",
      "Iter: 11860  \tTraining Loss: -532.2280    \n",
      "    Negative Log Likelihood: 61.9697\tSigma2 Prior: -594.1987\tRegularization: 0.0009\n",
      "Iter: 11870  \tTraining Loss: -517.5038    \n",
      "    Negative Log Likelihood: 64.8251\tSigma2 Prior: -582.3298\tRegularization: 0.0009\n",
      "Iter: 11880  \tTraining Loss: -552.4916    \n",
      "    Negative Log Likelihood: 59.5494\tSigma2 Prior: -612.0419\tRegularization: 0.0009\n",
      "Iter: 11890  \tTraining Loss: -525.2299    \n",
      "    Negative Log Likelihood: 62.4633\tSigma2 Prior: -587.6940\tRegularization: 0.0009\n",
      "Iter: 11900  \tTraining Loss: -506.7977    \n",
      "    Negative Log Likelihood: 65.6470\tSigma2 Prior: -572.4457\tRegularization: 0.0009\n",
      "Iter: 11910  \tTraining Loss: -523.0131    \n",
      "    Negative Log Likelihood: 63.2290\tSigma2 Prior: -586.2429\tRegularization: 0.0009\n",
      "Iter: 11920  \tTraining Loss: -515.2805    \n",
      "    Negative Log Likelihood: 64.4736\tSigma2 Prior: -579.7551\tRegularization: 0.0009\n",
      "Iter: 11930  \tTraining Loss: -525.2301    \n",
      "    Negative Log Likelihood: 63.2852\tSigma2 Prior: -588.5162\tRegularization: 0.0009\n",
      "Iter: 11940  \tTraining Loss: -531.7183    \n",
      "    Negative Log Likelihood: 60.5199\tSigma2 Prior: -592.2391\tRegularization: 0.0009\n",
      "Iter: 11950  \tTraining Loss: -540.1184    \n",
      "    Negative Log Likelihood: 59.6058\tSigma2 Prior: -599.7251\tRegularization: 0.0009\n",
      "Iter: 11960  \tTraining Loss: -536.4526    \n",
      "    Negative Log Likelihood: 61.3617\tSigma2 Prior: -597.8152\tRegularization: 0.0009\n",
      "Iter: 11970  \tTraining Loss: -550.2935    \n",
      "    Negative Log Likelihood: 58.4455\tSigma2 Prior: -608.7400\tRegularization: 0.0009\n",
      "Iter: 11980  \tTraining Loss: -534.5562    \n",
      "    Negative Log Likelihood: 62.0095\tSigma2 Prior: -596.5667\tRegularization: 0.0009\n",
      "Iter: 11990  \tTraining Loss: -532.2480    \n",
      "    Negative Log Likelihood: 62.0953\tSigma2 Prior: -594.3442\tRegularization: 0.0009\n",
      "Iter: 12000  \tTraining Loss: -523.4405    \n",
      "    Negative Log Likelihood: 61.9028\tSigma2 Prior: -585.3442\tRegularization: 0.0009\n",
      "Iter: 12010  \tTraining Loss: -531.3539    \n",
      "    Negative Log Likelihood: 61.7205\tSigma2 Prior: -593.0753\tRegularization: 0.0009\n",
      "Iter: 12020  \tTraining Loss: -531.5500    \n",
      "    Negative Log Likelihood: 61.7739\tSigma2 Prior: -593.3248\tRegularization: 0.0009\n",
      "Iter: 12030  \tTraining Loss: -511.4979    \n",
      "    Negative Log Likelihood: 65.2276\tSigma2 Prior: -576.7264\tRegularization: 0.0009\n",
      "Iter: 12040  \tTraining Loss: -535.1309    \n",
      "    Negative Log Likelihood: 62.1599\tSigma2 Prior: -597.2918\tRegularization: 0.0009\n",
      "Iter: 12050  \tTraining Loss: -515.6774    \n",
      "    Negative Log Likelihood: 63.5695\tSigma2 Prior: -579.2478\tRegularization: 0.0009\n",
      "Iter: 12060  \tTraining Loss: -502.5091    \n",
      "    Negative Log Likelihood: 65.1786\tSigma2 Prior: -567.6886\tRegularization: 0.0009\n",
      "Iter: 12070  \tTraining Loss: -538.3508    \n",
      "    Negative Log Likelihood: 62.1748\tSigma2 Prior: -600.5266\tRegularization: 0.0009\n",
      "Iter: 12080  \tTraining Loss: -526.0229    \n",
      "    Negative Log Likelihood: 61.5578\tSigma2 Prior: -587.5816\tRegularization: 0.0009\n",
      "Iter: 12090  \tTraining Loss: -536.0955    \n",
      "    Negative Log Likelihood: 62.4345\tSigma2 Prior: -598.5309\tRegularization: 0.0009\n",
      "Iter: 12100  \tTraining Loss: -538.2864    \n",
      "    Negative Log Likelihood: 59.5758\tSigma2 Prior: -597.8631\tRegularization: 0.0009\n",
      "Iter: 12110  \tTraining Loss: -534.4559    \n",
      "    Negative Log Likelihood: 60.9010\tSigma2 Prior: -595.3578\tRegularization: 0.0009\n",
      "Iter: 12120  \tTraining Loss: -548.1746    \n",
      "    Negative Log Likelihood: 60.2443\tSigma2 Prior: -608.4198\tRegularization: 0.0009\n",
      "Iter: 12130  \tTraining Loss: -515.0181    \n",
      "    Negative Log Likelihood: 63.0329\tSigma2 Prior: -578.0519\tRegularization: 0.0009\n",
      "Iter: 12140  \tTraining Loss: -516.9474    \n",
      "    Negative Log Likelihood: 61.1408\tSigma2 Prior: -578.0891\tRegularization: 0.0009\n",
      "Iter: 12150  \tTraining Loss: -543.1820    \n",
      "    Negative Log Likelihood: 59.6261\tSigma2 Prior: -602.8090\tRegularization: 0.0009\n",
      "Iter: 12160  \tTraining Loss: -538.1570    \n",
      "    Negative Log Likelihood: 61.2593\tSigma2 Prior: -599.4173\tRegularization: 0.0009\n",
      "Iter: 12170  \tTraining Loss: -517.0883    \n",
      "    Negative Log Likelihood: 62.6549\tSigma2 Prior: -579.7441\tRegularization: 0.0009\n",
      "Iter: 12180  \tTraining Loss: -503.5912    \n",
      "    Negative Log Likelihood: 63.5379\tSigma2 Prior: -567.1301\tRegularization: 0.0009\n",
      "Iter: 12190  \tTraining Loss: -521.7312    \n",
      "    Negative Log Likelihood: 61.9200\tSigma2 Prior: -583.6521\tRegularization: 0.0009\n",
      "Iter: 12200  \tTraining Loss: -502.0202    \n",
      "    Negative Log Likelihood: 63.1151\tSigma2 Prior: -565.1362\tRegularization: 0.0009\n",
      "Iter: 12210  \tTraining Loss: -556.6812    \n",
      "    Negative Log Likelihood: 58.1491\tSigma2 Prior: -614.8312\tRegularization: 0.0009\n",
      "Iter: 12220  \tTraining Loss: -533.3242    \n",
      "    Negative Log Likelihood: 61.6256\tSigma2 Prior: -594.9507\tRegularization: 0.0009\n",
      "Iter: 12230  \tTraining Loss: -550.2456    \n",
      "    Negative Log Likelihood: 60.3563\tSigma2 Prior: -610.6029\tRegularization: 0.0009\n",
      "Iter: 12240  \tTraining Loss: -523.4316    \n",
      "    Negative Log Likelihood: 62.9402\tSigma2 Prior: -586.3727\tRegularization: 0.0009\n",
      "Iter: 12250  \tTraining Loss: -536.6917    \n",
      "    Negative Log Likelihood: 60.9740\tSigma2 Prior: -597.6666\tRegularization: 0.0009\n",
      "Iter: 12260  \tTraining Loss: -554.6584    \n",
      "    Negative Log Likelihood: 59.2606\tSigma2 Prior: -613.9199\tRegularization: 0.0009\n",
      "Iter: 12270  \tTraining Loss: -512.3330    \n",
      "    Negative Log Likelihood: 64.0768\tSigma2 Prior: -576.4108\tRegularization: 0.0009\n",
      "Iter: 12280  \tTraining Loss: -539.7560    \n",
      "    Negative Log Likelihood: 61.0533\tSigma2 Prior: -600.8103\tRegularization: 0.0009\n",
      "Iter: 12290  \tTraining Loss: -529.4984    \n",
      "    Negative Log Likelihood: 62.2152\tSigma2 Prior: -591.7145\tRegularization: 0.0009\n",
      "Iter: 12300  \tTraining Loss: -529.2561    \n",
      "    Negative Log Likelihood: 61.7806\tSigma2 Prior: -591.0377\tRegularization: 0.0009\n",
      "Iter: 12310  \tTraining Loss: -547.9919    \n",
      "    Negative Log Likelihood: 59.9529\tSigma2 Prior: -607.9459\tRegularization: 0.0009\n",
      "Iter: 12320  \tTraining Loss: -539.8120    \n",
      "    Negative Log Likelihood: 60.5361\tSigma2 Prior: -600.3491\tRegularization: 0.0009\n",
      "Iter: 12330  \tTraining Loss: -526.9859    \n",
      "    Negative Log Likelihood: 62.6959\tSigma2 Prior: -589.6828\tRegularization: 0.0009\n",
      "Iter: 12340  \tTraining Loss: -535.8965    \n",
      "    Negative Log Likelihood: 61.2152\tSigma2 Prior: -597.1127\tRegularization: 0.0009\n",
      "Iter: 12350  \tTraining Loss: -536.7593    \n",
      "    Negative Log Likelihood: 62.0168\tSigma2 Prior: -598.7770\tRegularization: 0.0009\n",
      "Iter: 12360  \tTraining Loss: -556.7780    \n",
      "    Negative Log Likelihood: 58.7149\tSigma2 Prior: -615.4938\tRegularization: 0.0009\n",
      "Iter: 12370  \tTraining Loss: -533.4520    \n",
      "    Negative Log Likelihood: 61.8092\tSigma2 Prior: -595.2622\tRegularization: 0.0009\n",
      "Iter: 12380  \tTraining Loss: -538.9252    \n",
      "    Negative Log Likelihood: 61.6597\tSigma2 Prior: -600.5859\tRegularization: 0.0010\n",
      "Iter: 12390  \tTraining Loss: -546.7986    \n",
      "    Negative Log Likelihood: 60.9883\tSigma2 Prior: -607.7879\tRegularization: 0.0010\n",
      "Iter: 12400  \tTraining Loss: -520.0378    \n",
      "    Negative Log Likelihood: 64.2591\tSigma2 Prior: -584.2979\tRegularization: 0.0010\n",
      "Iter: 12410  \tTraining Loss: -556.7028    \n",
      "    Negative Log Likelihood: 59.1260\tSigma2 Prior: -615.8298\tRegularization: 0.0010\n",
      "Iter: 12420  \tTraining Loss: -554.3915    \n",
      "    Negative Log Likelihood: 58.4014\tSigma2 Prior: -612.7939\tRegularization: 0.0010\n",
      "Iter: 12430  \tTraining Loss: -532.0064    \n",
      "    Negative Log Likelihood: 62.6810\tSigma2 Prior: -594.6884\tRegularization: 0.0010\n",
      "Iter: 12440  \tTraining Loss: -536.2042    \n",
      "    Negative Log Likelihood: 61.4533\tSigma2 Prior: -597.6585\tRegularization: 0.0010\n",
      "Iter: 12450  \tTraining Loss: -540.6366    \n",
      "    Negative Log Likelihood: 59.4256\tSigma2 Prior: -600.0632\tRegularization: 0.0010\n",
      "Iter: 12460  \tTraining Loss: -529.4041    \n",
      "    Negative Log Likelihood: 61.7487\tSigma2 Prior: -591.1538\tRegularization: 0.0010\n",
      "Iter: 12470  \tTraining Loss: -543.5462    \n",
      "    Negative Log Likelihood: 60.4196\tSigma2 Prior: -603.9667\tRegularization: 0.0010\n",
      "Iter: 12480  \tTraining Loss: -523.8173    \n",
      "    Negative Log Likelihood: 63.0411\tSigma2 Prior: -586.8593\tRegularization: 0.0010\n",
      "Iter: 12490  \tTraining Loss: -523.3459    \n",
      "    Negative Log Likelihood: 62.1386\tSigma2 Prior: -585.4854\tRegularization: 0.0010\n",
      "Iter: 12500  \tTraining Loss: -528.6016    \n",
      "    Negative Log Likelihood: 60.8297\tSigma2 Prior: -589.4323\tRegularization: 0.0010\n",
      "Iter: 12510  \tTraining Loss: -534.3947    \n",
      "    Negative Log Likelihood: 60.0404\tSigma2 Prior: -594.4360\tRegularization: 0.0010\n",
      "Iter: 12520  \tTraining Loss: -521.1025    \n",
      "    Negative Log Likelihood: 62.0890\tSigma2 Prior: -583.1925\tRegularization: 0.0010\n",
      "Iter: 12530  \tTraining Loss: -542.9749    \n",
      "    Negative Log Likelihood: 59.9193\tSigma2 Prior: -602.8952\tRegularization: 0.0010\n",
      "Iter: 12540  \tTraining Loss: -536.6576    \n",
      "    Negative Log Likelihood: 59.7371\tSigma2 Prior: -596.3957\tRegularization: 0.0010\n",
      "Iter: 12550  \tTraining Loss: -543.0891    \n",
      "    Negative Log Likelihood: 59.3061\tSigma2 Prior: -602.3961\tRegularization: 0.0010\n",
      "Iter: 12560  \tTraining Loss: -547.9338    \n",
      "    Negative Log Likelihood: 58.7821\tSigma2 Prior: -606.7169\tRegularization: 0.0010\n",
      "Iter: 12570  \tTraining Loss: -533.7937    \n",
      "    Negative Log Likelihood: 61.5525\tSigma2 Prior: -595.3472\tRegularization: 0.0010\n",
      "Iter: 12580  \tTraining Loss: -538.2574    \n",
      "    Negative Log Likelihood: 59.6679\tSigma2 Prior: -597.9263\tRegularization: 0.0010\n",
      "Iter: 12590  \tTraining Loss: -527.1777    \n",
      "    Negative Log Likelihood: 62.7767\tSigma2 Prior: -589.9554\tRegularization: 0.0010\n",
      "Iter: 12600  \tTraining Loss: -525.0979    \n",
      "    Negative Log Likelihood: 60.8283\tSigma2 Prior: -585.9272\tRegularization: 0.0010\n",
      "Iter: 12610  \tTraining Loss: -528.9326    \n",
      "    Negative Log Likelihood: 61.0382\tSigma2 Prior: -589.9718\tRegularization: 0.0010\n",
      "Iter: 12620  \tTraining Loss: -517.2753    \n",
      "    Negative Log Likelihood: 62.0856\tSigma2 Prior: -579.3619\tRegularization: 0.0010\n",
      "Iter: 12630  \tTraining Loss: -517.0529    \n",
      "    Negative Log Likelihood: 62.0992\tSigma2 Prior: -579.1531\tRegularization: 0.0010\n",
      "Iter: 12640  \tTraining Loss: -542.9646    \n",
      "    Negative Log Likelihood: 58.1540\tSigma2 Prior: -601.1196\tRegularization: 0.0010\n",
      "Iter: 12650  \tTraining Loss: -540.2405    \n",
      "    Negative Log Likelihood: 59.7768\tSigma2 Prior: -600.0183\tRegularization: 0.0010\n",
      "Iter: 12660  \tTraining Loss: -532.2247    \n",
      "    Negative Log Likelihood: 60.7728\tSigma2 Prior: -592.9985\tRegularization: 0.0010\n",
      "Iter: 12670  \tTraining Loss: -518.5680    \n",
      "    Negative Log Likelihood: 62.2036\tSigma2 Prior: -580.7725\tRegularization: 0.0010\n",
      "Iter: 12680  \tTraining Loss: -529.4431    \n",
      "    Negative Log Likelihood: 60.8715\tSigma2 Prior: -590.3156\tRegularization: 0.0010\n",
      "Iter: 12690  \tTraining Loss: -523.3589    \n",
      "    Negative Log Likelihood: 59.9461\tSigma2 Prior: -583.3060\tRegularization: 0.0010\n",
      "Iter: 12700  \tTraining Loss: -546.1921    \n",
      "    Negative Log Likelihood: 59.4829\tSigma2 Prior: -605.6760\tRegularization: 0.0010\n",
      "Iter: 12710  \tTraining Loss: -525.3063    \n",
      "    Negative Log Likelihood: 62.3675\tSigma2 Prior: -587.6749\tRegularization: 0.0010\n",
      "Iter: 12720  \tTraining Loss: -548.2131    \n",
      "    Negative Log Likelihood: 58.5844\tSigma2 Prior: -606.7985\tRegularization: 0.0010\n",
      "Iter: 12730  \tTraining Loss: -543.4677    \n",
      "    Negative Log Likelihood: 59.8652\tSigma2 Prior: -603.3339\tRegularization: 0.0010\n",
      "Iter: 12740  \tTraining Loss: -533.0606    \n",
      "    Negative Log Likelihood: 61.2815\tSigma2 Prior: -594.3431\tRegularization: 0.0010\n",
      "Iter: 12750  \tTraining Loss: -539.5468    \n",
      "    Negative Log Likelihood: 58.9998\tSigma2 Prior: -598.5475\tRegularization: 0.0010\n",
      "Iter: 12760  \tTraining Loss: -530.3279    \n",
      "    Negative Log Likelihood: 60.4585\tSigma2 Prior: -590.7874\tRegularization: 0.0010\n",
      "Iter: 12770  \tTraining Loss: -536.7563    \n",
      "    Negative Log Likelihood: 60.2094\tSigma2 Prior: -596.9668\tRegularization: 0.0010\n",
      "Iter: 12780  \tTraining Loss: -536.9144    \n",
      "    Negative Log Likelihood: 59.7420\tSigma2 Prior: -596.6573\tRegularization: 0.0010\n",
      "Iter: 12790  \tTraining Loss: -535.5248    \n",
      "    Negative Log Likelihood: 60.1855\tSigma2 Prior: -595.7113\tRegularization: 0.0010\n",
      "Iter: 12800  \tTraining Loss: -543.9451    \n",
      "    Negative Log Likelihood: 59.5964\tSigma2 Prior: -603.5425\tRegularization: 0.0010\n",
      "Iter: 12810  \tTraining Loss: -514.6064    \n",
      "    Negative Log Likelihood: 62.1305\tSigma2 Prior: -576.7379\tRegularization: 0.0010\n",
      "Iter: 12820  \tTraining Loss: -527.8098    \n",
      "    Negative Log Likelihood: 62.2860\tSigma2 Prior: -590.0967\tRegularization: 0.0010\n",
      "Iter: 12830  \tTraining Loss: -497.9778    \n",
      "    Negative Log Likelihood: 63.0560\tSigma2 Prior: -561.0348\tRegularization: 0.0010\n",
      "Iter: 12840  \tTraining Loss: -535.8400    \n",
      "    Negative Log Likelihood: 60.8047\tSigma2 Prior: -596.6457\tRegularization: 0.0010\n",
      "Iter: 12850  \tTraining Loss: -506.8543    \n",
      "    Negative Log Likelihood: 62.4289\tSigma2 Prior: -569.2842\tRegularization: 0.0010\n",
      "Iter: 12860  \tTraining Loss: -519.9636    \n",
      "    Negative Log Likelihood: 61.2857\tSigma2 Prior: -581.2502\tRegularization: 0.0010\n",
      "Iter: 12870  \tTraining Loss: -535.9579    \n",
      "    Negative Log Likelihood: 60.5169\tSigma2 Prior: -596.4757\tRegularization: 0.0010\n",
      "Iter: 12880  \tTraining Loss: -543.6984    \n",
      "    Negative Log Likelihood: 59.3427\tSigma2 Prior: -603.0421\tRegularization: 0.0010\n",
      "Iter: 12890  \tTraining Loss: -540.9061    \n",
      "    Negative Log Likelihood: 60.3867\tSigma2 Prior: -601.2937\tRegularization: 0.0010\n",
      "Iter: 12900  \tTraining Loss: -535.3011    \n",
      "    Negative Log Likelihood: 59.5018\tSigma2 Prior: -594.8040\tRegularization: 0.0010\n",
      "Iter: 12910  \tTraining Loss: -552.5445    \n",
      "    Negative Log Likelihood: 60.0833\tSigma2 Prior: -612.6287\tRegularization: 0.0010\n",
      "Iter: 12920  \tTraining Loss: -537.6971    \n",
      "    Negative Log Likelihood: 60.6785\tSigma2 Prior: -598.3766\tRegularization: 0.0010\n",
      "Iter: 12930  \tTraining Loss: -540.3121    \n",
      "    Negative Log Likelihood: 61.5030\tSigma2 Prior: -601.8160\tRegularization: 0.0010\n",
      "Iter: 12940  \tTraining Loss: -552.9285    \n",
      "    Negative Log Likelihood: 59.0531\tSigma2 Prior: -611.9825\tRegularization: 0.0010\n",
      "Iter: 12950  \tTraining Loss: -530.7388    \n",
      "    Negative Log Likelihood: 62.5541\tSigma2 Prior: -593.2938\tRegularization: 0.0010\n",
      "Iter: 12960  \tTraining Loss: -549.4898    \n",
      "    Negative Log Likelihood: 59.2679\tSigma2 Prior: -608.7587\tRegularization: 0.0010\n",
      "Iter: 12970  \tTraining Loss: -538.1134    \n",
      "    Negative Log Likelihood: 61.9845\tSigma2 Prior: -600.0989\tRegularization: 0.0010\n",
      "Iter: 12980  \tTraining Loss: -531.6324    \n",
      "    Negative Log Likelihood: 60.7110\tSigma2 Prior: -592.3444\tRegularization: 0.0010\n",
      "Iter: 12990  \tTraining Loss: -551.0106    \n",
      "    Negative Log Likelihood: 60.2918\tSigma2 Prior: -611.3033\tRegularization: 0.0010\n",
      "Iter: 13000  \tTraining Loss: -536.4528    \n",
      "    Negative Log Likelihood: 61.0042\tSigma2 Prior: -597.4579\tRegularization: 0.0010\n",
      "Iter: 13010  \tTraining Loss: -535.7839    \n",
      "    Negative Log Likelihood: 62.3416\tSigma2 Prior: -598.1265\tRegularization: 0.0010\n",
      "Iter: 13020  \tTraining Loss: -513.1167    \n",
      "    Negative Log Likelihood: 63.7885\tSigma2 Prior: -576.9062\tRegularization: 0.0010\n",
      "Iter: 13030  \tTraining Loss: -554.3743    \n",
      "    Negative Log Likelihood: 59.6960\tSigma2 Prior: -614.0713\tRegularization: 0.0010\n",
      "Iter: 13040  \tTraining Loss: -531.9297    \n",
      "    Negative Log Likelihood: 61.7857\tSigma2 Prior: -593.7163\tRegularization: 0.0010\n",
      "Iter: 13050  \tTraining Loss: -554.3543    \n",
      "    Negative Log Likelihood: 58.4036\tSigma2 Prior: -612.7589\tRegularization: 0.0010\n",
      "Iter: 13060  \tTraining Loss: -523.8856    \n",
      "    Negative Log Likelihood: 61.6232\tSigma2 Prior: -585.5098\tRegularization: 0.0010\n",
      "Iter: 13070  \tTraining Loss: -540.4963    \n",
      "    Negative Log Likelihood: 60.1155\tSigma2 Prior: -600.6128\tRegularization: 0.0010\n",
      "Iter: 13080  \tTraining Loss: -550.2023    \n",
      "    Negative Log Likelihood: 59.4804\tSigma2 Prior: -609.6837\tRegularization: 0.0010\n",
      "Iter: 13090  \tTraining Loss: -540.9944    \n",
      "    Negative Log Likelihood: 61.8224\tSigma2 Prior: -602.8177\tRegularization: 0.0010\n",
      "Iter: 13100  \tTraining Loss: -533.6365    \n",
      "    Negative Log Likelihood: 61.3175\tSigma2 Prior: -594.9550\tRegularization: 0.0010\n",
      "Iter: 13110  \tTraining Loss: -537.1168    \n",
      "    Negative Log Likelihood: 59.8120\tSigma2 Prior: -596.9298\tRegularization: 0.0010\n",
      "Iter: 13120  \tTraining Loss: -517.7072    \n",
      "    Negative Log Likelihood: 64.1534\tSigma2 Prior: -581.8616\tRegularization: 0.0010\n",
      "Iter: 13130  \tTraining Loss: -532.4484    \n",
      "    Negative Log Likelihood: 62.4608\tSigma2 Prior: -594.9102\tRegularization: 0.0010\n",
      "Iter: 13140  \tTraining Loss: -531.3052    \n",
      "    Negative Log Likelihood: 60.8164\tSigma2 Prior: -592.1226\tRegularization: 0.0010\n",
      "Iter: 13150  \tTraining Loss: -542.0012    \n",
      "    Negative Log Likelihood: 60.4811\tSigma2 Prior: -602.4833\tRegularization: 0.0010\n",
      "Iter: 13160  \tTraining Loss: -546.0519    \n",
      "    Negative Log Likelihood: 60.5490\tSigma2 Prior: -606.6019\tRegularization: 0.0010\n",
      "Iter: 13170  \tTraining Loss: -528.0985    \n",
      "    Negative Log Likelihood: 61.9789\tSigma2 Prior: -590.0784\tRegularization: 0.0010\n",
      "Iter: 13180  \tTraining Loss: -543.7685    \n",
      "    Negative Log Likelihood: 60.1179\tSigma2 Prior: -603.8874\tRegularization: 0.0010\n",
      "Iter: 13190  \tTraining Loss: -520.9208    \n",
      "    Negative Log Likelihood: 62.9262\tSigma2 Prior: -583.8480\tRegularization: 0.0010\n",
      "Iter: 13200  \tTraining Loss: -541.9800    \n",
      "    Negative Log Likelihood: 61.3184\tSigma2 Prior: -603.2994\tRegularization: 0.0010\n",
      "Iter: 13210  \tTraining Loss: -524.3460    \n",
      "    Negative Log Likelihood: 61.6296\tSigma2 Prior: -585.9766\tRegularization: 0.0010\n",
      "Iter: 13220  \tTraining Loss: -524.9335    \n",
      "    Negative Log Likelihood: 61.0839\tSigma2 Prior: -586.0184\tRegularization: 0.0010\n",
      "Iter: 13230  \tTraining Loss: -543.1718    \n",
      "    Negative Log Likelihood: 59.9547\tSigma2 Prior: -603.1274\tRegularization: 0.0010\n",
      "Iter: 13240  \tTraining Loss: -542.4370    \n",
      "    Negative Log Likelihood: 59.6445\tSigma2 Prior: -602.0825\tRegularization: 0.0010\n",
      "Iter: 13250  \tTraining Loss: -540.1166    \n",
      "    Negative Log Likelihood: 60.3971\tSigma2 Prior: -600.5146\tRegularization: 0.0010\n",
      "Iter: 13260  \tTraining Loss: -541.3477    \n",
      "    Negative Log Likelihood: 60.6020\tSigma2 Prior: -601.9506\tRegularization: 0.0010\n",
      "Iter: 13270  \tTraining Loss: -531.9316    \n",
      "    Negative Log Likelihood: 60.8707\tSigma2 Prior: -592.8032\tRegularization: 0.0010\n",
      "Iter: 13280  \tTraining Loss: -517.8562    \n",
      "    Negative Log Likelihood: 63.5071\tSigma2 Prior: -581.3643\tRegularization: 0.0010\n",
      "Iter: 13290  \tTraining Loss: -544.4484    \n",
      "    Negative Log Likelihood: 59.6744\tSigma2 Prior: -604.1238\tRegularization: 0.0010\n",
      "Iter: 13300  \tTraining Loss: -530.4811    \n",
      "    Negative Log Likelihood: 61.7783\tSigma2 Prior: -592.2604\tRegularization: 0.0010\n",
      "Iter: 13310  \tTraining Loss: -540.7354    \n",
      "    Negative Log Likelihood: 59.6358\tSigma2 Prior: -600.3721\tRegularization: 0.0010\n",
      "Iter: 13320  \tTraining Loss: -532.9742    \n",
      "    Negative Log Likelihood: 62.1791\tSigma2 Prior: -595.1543\tRegularization: 0.0010\n",
      "Iter: 13330  \tTraining Loss: -540.1885    \n",
      "    Negative Log Likelihood: 60.1501\tSigma2 Prior: -600.3395\tRegularization: 0.0010\n",
      "Iter: 13340  \tTraining Loss: -537.8739    \n",
      "    Negative Log Likelihood: 61.1154\tSigma2 Prior: -598.9903\tRegularization: 0.0010\n",
      "Iter: 13350  \tTraining Loss: -545.8995    \n",
      "    Negative Log Likelihood: 59.5020\tSigma2 Prior: -605.4025\tRegularization: 0.0010\n",
      "Iter: 13360  \tTraining Loss: -544.4027    \n",
      "    Negative Log Likelihood: 60.7619\tSigma2 Prior: -605.1655\tRegularization: 0.0010\n",
      "Iter: 13370  \tTraining Loss: -530.4084    \n",
      "    Negative Log Likelihood: 60.7368\tSigma2 Prior: -591.1462\tRegularization: 0.0010\n",
      "Iter: 13380  \tTraining Loss: -528.2625    \n",
      "    Negative Log Likelihood: 60.8688\tSigma2 Prior: -589.1323\tRegularization: 0.0010\n",
      "Iter: 13390  \tTraining Loss: -534.7084    \n",
      "    Negative Log Likelihood: 59.7099\tSigma2 Prior: -594.4193\tRegularization: 0.0010\n",
      "Iter: 13400  \tTraining Loss: -526.7959    \n",
      "    Negative Log Likelihood: 62.1392\tSigma2 Prior: -588.9361\tRegularization: 0.0010\n",
      "Iter: 13410  \tTraining Loss: -536.7501    \n",
      "    Negative Log Likelihood: 59.2402\tSigma2 Prior: -595.9913\tRegularization: 0.0010\n",
      "Iter: 13420  \tTraining Loss: -507.7812    \n",
      "    Negative Log Likelihood: 63.4318\tSigma2 Prior: -571.2140\tRegularization: 0.0010\n",
      "Iter: 13430  \tTraining Loss: -533.4052    \n",
      "    Negative Log Likelihood: 59.8278\tSigma2 Prior: -593.2339\tRegularization: 0.0010\n",
      "Iter: 13440  \tTraining Loss: -544.3748    \n",
      "    Negative Log Likelihood: 58.5607\tSigma2 Prior: -602.9365\tRegularization: 0.0010\n",
      "Iter: 13450  \tTraining Loss: -541.6372    \n",
      "    Negative Log Likelihood: 59.8543\tSigma2 Prior: -601.4925\tRegularization: 0.0010\n",
      "Iter: 13460  \tTraining Loss: -550.8489    \n",
      "    Negative Log Likelihood: 58.8167\tSigma2 Prior: -609.6666\tRegularization: 0.0010\n",
      "Iter: 13470  \tTraining Loss: -545.6922    \n",
      "    Negative Log Likelihood: 58.5747\tSigma2 Prior: -604.2679\tRegularization: 0.0010\n",
      "Iter: 13480  \tTraining Loss: -538.2324    \n",
      "    Negative Log Likelihood: 60.2448\tSigma2 Prior: -598.4781\tRegularization: 0.0010\n",
      "Iter: 13490  \tTraining Loss: -539.1520    \n",
      "    Negative Log Likelihood: 61.1477\tSigma2 Prior: -600.3007\tRegularization: 0.0010\n",
      "Iter: 13500  \tTraining Loss: -536.1771    \n",
      "    Negative Log Likelihood: 60.4776\tSigma2 Prior: -596.6557\tRegularization: 0.0010\n",
      "Iter: 13510  \tTraining Loss: -549.5289    \n",
      "    Negative Log Likelihood: 60.0547\tSigma2 Prior: -609.5845\tRegularization: 0.0010\n",
      "Iter: 13520  \tTraining Loss: -529.6226    \n",
      "    Negative Log Likelihood: 60.7885\tSigma2 Prior: -590.4121\tRegularization: 0.0010\n",
      "Iter: 13530  \tTraining Loss: -515.7305    \n",
      "    Negative Log Likelihood: 63.5842\tSigma2 Prior: -579.3157\tRegularization: 0.0010\n",
      "Iter: 13540  \tTraining Loss: -506.4129    \n",
      "    Negative Log Likelihood: 63.8581\tSigma2 Prior: -570.2720\tRegularization: 0.0010\n",
      "Iter: 13550  \tTraining Loss: -533.9122    \n",
      "    Negative Log Likelihood: 60.0483\tSigma2 Prior: -593.9615\tRegularization: 0.0010\n",
      "Iter: 13560  \tTraining Loss: -528.7109    \n",
      "    Negative Log Likelihood: 61.6171\tSigma2 Prior: -590.3290\tRegularization: 0.0010\n",
      "Iter: 13570  \tTraining Loss: -524.1776    \n",
      "    Negative Log Likelihood: 61.8959\tSigma2 Prior: -586.0745\tRegularization: 0.0010\n",
      "Iter: 13580  \tTraining Loss: -527.6868    \n",
      "    Negative Log Likelihood: 61.8353\tSigma2 Prior: -589.5231\tRegularization: 0.0010\n",
      "Iter: 13590  \tTraining Loss: -549.7700    \n",
      "    Negative Log Likelihood: 59.9411\tSigma2 Prior: -609.7121\tRegularization: 0.0010\n",
      "Iter: 13600  \tTraining Loss: -555.0725    \n",
      "    Negative Log Likelihood: 58.3970\tSigma2 Prior: -613.4705\tRegularization: 0.0010\n",
      "Iter: 13610  \tTraining Loss: -525.2726    \n",
      "    Negative Log Likelihood: 62.6487\tSigma2 Prior: -587.9223\tRegularization: 0.0010\n",
      "Iter: 13620  \tTraining Loss: -520.3059    \n",
      "    Negative Log Likelihood: 62.0966\tSigma2 Prior: -582.4035\tRegularization: 0.0010\n",
      "Iter: 13630  \tTraining Loss: -542.9045    \n",
      "    Negative Log Likelihood: 59.4296\tSigma2 Prior: -602.3351\tRegularization: 0.0010\n",
      "Iter: 13640  \tTraining Loss: -523.9901    \n",
      "    Negative Log Likelihood: 61.9702\tSigma2 Prior: -585.9613\tRegularization: 0.0010\n",
      "Iter: 13650  \tTraining Loss: -525.1097    \n",
      "    Negative Log Likelihood: 61.3575\tSigma2 Prior: -586.4682\tRegularization: 0.0010\n",
      "Iter: 13660  \tTraining Loss: -545.5768    \n",
      "    Negative Log Likelihood: 58.8857\tSigma2 Prior: -604.4635\tRegularization: 0.0010\n",
      "Iter: 13670  \tTraining Loss: -555.5142    \n",
      "    Negative Log Likelihood: 56.9012\tSigma2 Prior: -612.4164\tRegularization: 0.0010\n",
      "Iter: 13680  \tTraining Loss: -530.8124    \n",
      "    Negative Log Likelihood: 60.1595\tSigma2 Prior: -590.9728\tRegularization: 0.0010\n",
      "Iter: 13690  \tTraining Loss: -531.3080    \n",
      "    Negative Log Likelihood: 60.6666\tSigma2 Prior: -591.9756\tRegularization: 0.0010\n",
      "Iter: 13700  \tTraining Loss: -537.3160    \n",
      "    Negative Log Likelihood: 60.9129\tSigma2 Prior: -598.2299\tRegularization: 0.0010\n",
      "Iter: 13710  \tTraining Loss: -528.7380    \n",
      "    Negative Log Likelihood: 60.3129\tSigma2 Prior: -589.0519\tRegularization: 0.0010\n",
      "Iter: 13720  \tTraining Loss: -529.0621    \n",
      "    Negative Log Likelihood: 62.1040\tSigma2 Prior: -591.1672\tRegularization: 0.0010\n",
      "Iter: 13730  \tTraining Loss: -540.2705    \n",
      "    Negative Log Likelihood: 59.4133\tSigma2 Prior: -599.6848\tRegularization: 0.0010\n",
      "Iter: 13740  \tTraining Loss: -541.8187    \n",
      "    Negative Log Likelihood: 59.7965\tSigma2 Prior: -601.6162\tRegularization: 0.0010\n",
      "Iter: 13750  \tTraining Loss: -541.1479    \n",
      "    Negative Log Likelihood: 59.2403\tSigma2 Prior: -600.3892\tRegularization: 0.0010\n",
      "Iter: 13760  \tTraining Loss: -527.9086    \n",
      "    Negative Log Likelihood: 60.9139\tSigma2 Prior: -588.8235\tRegularization: 0.0010\n",
      "Iter: 13770  \tTraining Loss: -534.2498    \n",
      "    Negative Log Likelihood: 59.8569\tSigma2 Prior: -594.1077\tRegularization: 0.0010\n",
      "Iter: 13780  \tTraining Loss: -549.5687    \n",
      "    Negative Log Likelihood: 58.7439\tSigma2 Prior: -608.3135\tRegularization: 0.0010\n",
      "Iter: 13790  \tTraining Loss: -544.8063    \n",
      "    Negative Log Likelihood: 59.5932\tSigma2 Prior: -604.4006\tRegularization: 0.0010\n",
      "Iter: 13800  \tTraining Loss: -550.7653    \n",
      "    Negative Log Likelihood: 59.4948\tSigma2 Prior: -610.2610\tRegularization: 0.0010\n",
      "Iter: 13810  \tTraining Loss: -544.0901    \n",
      "    Negative Log Likelihood: 60.5578\tSigma2 Prior: -604.6489\tRegularization: 0.0010\n",
      "Iter: 13820  \tTraining Loss: -549.8656    \n",
      "    Negative Log Likelihood: 58.0532\tSigma2 Prior: -607.9198\tRegularization: 0.0010\n",
      "Iter: 13830  \tTraining Loss: -520.8932    \n",
      "    Negative Log Likelihood: 63.7754\tSigma2 Prior: -584.6697\tRegularization: 0.0010\n",
      "Iter: 13840  \tTraining Loss: -557.9753    \n",
      "    Negative Log Likelihood: 59.0877\tSigma2 Prior: -617.0641\tRegularization: 0.0010\n",
      "Iter: 13850  \tTraining Loss: -527.6697    \n",
      "    Negative Log Likelihood: 62.5887\tSigma2 Prior: -590.2593\tRegularization: 0.0010\n",
      "Iter: 13860  \tTraining Loss: -534.8264    \n",
      "    Negative Log Likelihood: 61.8120\tSigma2 Prior: -596.6394\tRegularization: 0.0010\n",
      "Iter: 13870  \tTraining Loss: -530.0123    \n",
      "    Negative Log Likelihood: 62.6475\tSigma2 Prior: -592.6608\tRegularization: 0.0010\n",
      "Iter: 13880  \tTraining Loss: -548.5259    \n",
      "    Negative Log Likelihood: 59.6208\tSigma2 Prior: -608.1477\tRegularization: 0.0010\n",
      "Iter: 13890  \tTraining Loss: -551.4196    \n",
      "    Negative Log Likelihood: 59.1153\tSigma2 Prior: -610.5358\tRegularization: 0.0010\n",
      "Iter: 13900  \tTraining Loss: -511.3617    \n",
      "    Negative Log Likelihood: 63.9805\tSigma2 Prior: -575.3432\tRegularization: 0.0010\n",
      "Iter: 13910  \tTraining Loss: -540.2363    \n",
      "    Negative Log Likelihood: 60.6555\tSigma2 Prior: -600.8927\tRegularization: 0.0010\n",
      "Iter: 13920  \tTraining Loss: -526.6966    \n",
      "    Negative Log Likelihood: 60.0506\tSigma2 Prior: -586.7482\tRegularization: 0.0010\n",
      "Iter: 13930  \tTraining Loss: -529.7797    \n",
      "    Negative Log Likelihood: 61.9927\tSigma2 Prior: -591.7734\tRegularization: 0.0010\n",
      "Iter: 13940  \tTraining Loss: -545.1857    \n",
      "    Negative Log Likelihood: 60.5830\tSigma2 Prior: -605.7698\tRegularization: 0.0010\n",
      "Iter: 13950  \tTraining Loss: -528.4201    \n",
      "    Negative Log Likelihood: 62.4732\tSigma2 Prior: -590.8943\tRegularization: 0.0010\n",
      "Iter: 13960  \tTraining Loss: -536.0314    \n",
      "    Negative Log Likelihood: 59.8709\tSigma2 Prior: -595.9033\tRegularization: 0.0010\n",
      "Iter: 13970  \tTraining Loss: -553.9820    \n",
      "    Negative Log Likelihood: 58.6145\tSigma2 Prior: -612.5975\tRegularization: 0.0010\n",
      "Iter: 13980  \tTraining Loss: -533.6657    \n",
      "    Negative Log Likelihood: 62.7901\tSigma2 Prior: -596.4568\tRegularization: 0.0010\n",
      "Iter: 13990  \tTraining Loss: -522.6863    \n",
      "    Negative Log Likelihood: 62.0672\tSigma2 Prior: -584.7545\tRegularization: 0.0010\n",
      "Iter: 14000  \tTraining Loss: -542.3132    \n",
      "    Negative Log Likelihood: 59.3903\tSigma2 Prior: -601.7045\tRegularization: 0.0010\n",
      "Iter: 14010  \tTraining Loss: -547.3271    \n",
      "    Negative Log Likelihood: 59.9656\tSigma2 Prior: -607.2937\tRegularization: 0.0010\n",
      "Iter: 14020  \tTraining Loss: -547.3655    \n",
      "    Negative Log Likelihood: 59.5898\tSigma2 Prior: -606.9562\tRegularization: 0.0010\n",
      "Iter: 14030  \tTraining Loss: -544.3913    \n",
      "    Negative Log Likelihood: 60.2046\tSigma2 Prior: -604.5969\tRegularization: 0.0010\n",
      "Iter: 14040  \tTraining Loss: -524.2179    \n",
      "    Negative Log Likelihood: 62.2745\tSigma2 Prior: -586.4934\tRegularization: 0.0010\n",
      "Iter: 14050  \tTraining Loss: -520.6286    \n",
      "    Negative Log Likelihood: 62.1193\tSigma2 Prior: -582.7488\tRegularization: 0.0010\n",
      "Iter: 14060  \tTraining Loss: -556.8083    \n",
      "    Negative Log Likelihood: 56.0807\tSigma2 Prior: -612.8900\tRegularization: 0.0010\n",
      "Iter: 14070  \tTraining Loss: -517.3715    \n",
      "    Negative Log Likelihood: 61.7905\tSigma2 Prior: -579.1630\tRegularization: 0.0010\n",
      "Iter: 14080  \tTraining Loss: -528.8281    \n",
      "    Negative Log Likelihood: 62.4372\tSigma2 Prior: -591.2662\tRegularization: 0.0010\n",
      "Iter: 14090  \tTraining Loss: -554.8034    \n",
      "    Negative Log Likelihood: 57.8934\tSigma2 Prior: -612.6978\tRegularization: 0.0010\n",
      "Iter: 14100  \tTraining Loss: -539.3734    \n",
      "    Negative Log Likelihood: 58.9600\tSigma2 Prior: -598.3343\tRegularization: 0.0010\n",
      "Iter: 14110  \tTraining Loss: -549.5954    \n",
      "    Negative Log Likelihood: 57.7890\tSigma2 Prior: -607.3854\tRegularization: 0.0010\n",
      "Iter: 14120  \tTraining Loss: -557.9285    \n",
      "    Negative Log Likelihood: 57.4346\tSigma2 Prior: -615.3641\tRegularization: 0.0010\n",
      "Iter: 14130  \tTraining Loss: -546.6147    \n",
      "    Negative Log Likelihood: 58.6583\tSigma2 Prior: -605.2740\tRegularization: 0.0010\n",
      "Iter: 14140  \tTraining Loss: -541.1068    \n",
      "    Negative Log Likelihood: 60.5900\tSigma2 Prior: -601.6978\tRegularization: 0.0010\n",
      "Iter: 14150  \tTraining Loss: -534.2042    \n",
      "    Negative Log Likelihood: 61.6790\tSigma2 Prior: -595.8842\tRegularization: 0.0010\n",
      "Iter: 14160  \tTraining Loss: -536.9229    \n",
      "    Negative Log Likelihood: 60.8645\tSigma2 Prior: -597.7883\tRegularization: 0.0010\n",
      "Iter: 14170  \tTraining Loss: -541.1274    \n",
      "    Negative Log Likelihood: 59.3924\tSigma2 Prior: -600.5208\tRegularization: 0.0010\n",
      "Iter: 14180  \tTraining Loss: -526.3459    \n",
      "    Negative Log Likelihood: 60.0904\tSigma2 Prior: -586.4373\tRegularization: 0.0010\n",
      "Iter: 14190  \tTraining Loss: -547.2095    \n",
      "    Negative Log Likelihood: 59.7120\tSigma2 Prior: -606.9225\tRegularization: 0.0010\n",
      "Iter: 14200  \tTraining Loss: -539.7552    \n",
      "    Negative Log Likelihood: 60.2556\tSigma2 Prior: -600.0118\tRegularization: 0.0010\n",
      "Iter: 14210  \tTraining Loss: -519.3946    \n",
      "    Negative Log Likelihood: 63.9160\tSigma2 Prior: -583.3116\tRegularization: 0.0010\n",
      "Iter: 14220  \tTraining Loss: -548.1935    \n",
      "    Negative Log Likelihood: 60.5285\tSigma2 Prior: -608.7230\tRegularization: 0.0010\n",
      "Iter: 14230  \tTraining Loss: -540.9801    \n",
      "    Negative Log Likelihood: 61.1555\tSigma2 Prior: -602.1365\tRegularization: 0.0010\n",
      "Iter: 14240  \tTraining Loss: -527.7169    \n",
      "    Negative Log Likelihood: 62.3171\tSigma2 Prior: -590.0349\tRegularization: 0.0010\n",
      "Iter: 14250  \tTraining Loss: -541.1735    \n",
      "    Negative Log Likelihood: 60.1143\tSigma2 Prior: -601.2888\tRegularization: 0.0010\n",
      "Iter: 14260  \tTraining Loss: -536.1661    \n",
      "    Negative Log Likelihood: 62.5128\tSigma2 Prior: -598.6799\tRegularization: 0.0010\n",
      "Iter: 14270  \tTraining Loss: -557.2097    \n",
      "    Negative Log Likelihood: 59.8895\tSigma2 Prior: -617.1002\tRegularization: 0.0010\n",
      "Iter: 14280  \tTraining Loss: -530.2469    \n",
      "    Negative Log Likelihood: 63.6770\tSigma2 Prior: -593.9250\tRegularization: 0.0010\n",
      "Iter: 14290  \tTraining Loss: -547.2385    \n",
      "    Negative Log Likelihood: 60.3540\tSigma2 Prior: -607.5934\tRegularization: 0.0010\n",
      "Iter: 14300  \tTraining Loss: -541.3679    \n",
      "    Negative Log Likelihood: 60.9724\tSigma2 Prior: -602.3412\tRegularization: 0.0010\n",
      "Iter: 14310  \tTraining Loss: -532.0703    \n",
      "    Negative Log Likelihood: 61.4165\tSigma2 Prior: -593.4877\tRegularization: 0.0010\n",
      "Iter: 14320  \tTraining Loss: -557.3480    \n",
      "    Negative Log Likelihood: 58.5476\tSigma2 Prior: -615.8965\tRegularization: 0.0010\n",
      "Iter: 14330  \tTraining Loss: -531.8330    \n",
      "    Negative Log Likelihood: 62.8547\tSigma2 Prior: -594.6887\tRegularization: 0.0010\n",
      "Iter: 14340  \tTraining Loss: -527.0513    \n",
      "    Negative Log Likelihood: 62.1440\tSigma2 Prior: -589.1962\tRegularization: 0.0010\n",
      "Iter: 14350  \tTraining Loss: -542.6406    \n",
      "    Negative Log Likelihood: 60.1847\tSigma2 Prior: -602.8263\tRegularization: 0.0010\n",
      "Iter: 14360  \tTraining Loss: -543.4625    \n",
      "    Negative Log Likelihood: 58.4027\tSigma2 Prior: -601.8661\tRegularization: 0.0010\n",
      "Iter: 14370  \tTraining Loss: -544.3687    \n",
      "    Negative Log Likelihood: 60.0178\tSigma2 Prior: -604.3875\tRegularization: 0.0010\n",
      "Iter: 14380  \tTraining Loss: -532.8867    \n",
      "    Negative Log Likelihood: 60.6406\tSigma2 Prior: -593.5283\tRegularization: 0.0010\n",
      "Iter: 14390  \tTraining Loss: -516.4310    \n",
      "    Negative Log Likelihood: 61.3711\tSigma2 Prior: -577.8031\tRegularization: 0.0010\n",
      "Iter: 14400  \tTraining Loss: -534.0211    \n",
      "    Negative Log Likelihood: 58.8554\tSigma2 Prior: -592.8774\tRegularization: 0.0010\n",
      "Iter: 14410  \tTraining Loss: -548.1448    \n",
      "    Negative Log Likelihood: 58.6092\tSigma2 Prior: -606.7550\tRegularization: 0.0010\n",
      "Iter: 14420  \tTraining Loss: -550.6311    \n",
      "    Negative Log Likelihood: 58.3892\tSigma2 Prior: -609.0212\tRegularization: 0.0010\n",
      "Iter: 14430  \tTraining Loss: -547.0255    \n",
      "    Negative Log Likelihood: 59.6192\tSigma2 Prior: -606.6458\tRegularization: 0.0010\n",
      "Iter: 14440  \tTraining Loss: -539.1301    \n",
      "    Negative Log Likelihood: 61.1017\tSigma2 Prior: -600.2328\tRegularization: 0.0010\n",
      "Iter: 14450  \tTraining Loss: -538.4069    \n",
      "    Negative Log Likelihood: 61.1805\tSigma2 Prior: -599.5884\tRegularization: 0.0010\n",
      "Iter: 14460  \tTraining Loss: -520.2899    \n",
      "    Negative Log Likelihood: 62.2928\tSigma2 Prior: -582.5836\tRegularization: 0.0010\n",
      "Iter: 14470  \tTraining Loss: -530.3392    \n",
      "    Negative Log Likelihood: 62.6629\tSigma2 Prior: -593.0031\tRegularization: 0.0010\n",
      "Iter: 14480  \tTraining Loss: -540.3167    \n",
      "    Negative Log Likelihood: 59.7954\tSigma2 Prior: -600.1131\tRegularization: 0.0010\n",
      "Iter: 14490  \tTraining Loss: -546.5857    \n",
      "    Negative Log Likelihood: 59.2411\tSigma2 Prior: -605.8278\tRegularization: 0.0010\n",
      "Iter: 14500  \tTraining Loss: -538.3308    \n",
      "    Negative Log Likelihood: 59.2420\tSigma2 Prior: -597.5737\tRegularization: 0.0010\n",
      "Iter: 14510  \tTraining Loss: -534.0563    \n",
      "    Negative Log Likelihood: 58.9186\tSigma2 Prior: -592.9759\tRegularization: 0.0010\n",
      "Iter: 14520  \tTraining Loss: -543.2262    \n",
      "    Negative Log Likelihood: 59.6900\tSigma2 Prior: -602.9172\tRegularization: 0.0010\n",
      "Iter: 14530  \tTraining Loss: -549.2307    \n",
      "    Negative Log Likelihood: 59.3563\tSigma2 Prior: -608.5880\tRegularization: 0.0010\n",
      "Iter: 14540  \tTraining Loss: -544.7490    \n",
      "    Negative Log Likelihood: 59.0995\tSigma2 Prior: -603.8495\tRegularization: 0.0010\n",
      "Iter: 14550  \tTraining Loss: -515.3706    \n",
      "    Negative Log Likelihood: 62.7353\tSigma2 Prior: -578.1068\tRegularization: 0.0010\n",
      "Iter: 14560  \tTraining Loss: -555.8373    \n",
      "    Negative Log Likelihood: 57.4440\tSigma2 Prior: -613.2823\tRegularization: 0.0010\n",
      "Iter: 14570  \tTraining Loss: -538.5715    \n",
      "    Negative Log Likelihood: 57.8403\tSigma2 Prior: -596.4127\tRegularization: 0.0010\n",
      "Iter: 14580  \tTraining Loss: -543.0079    \n",
      "    Negative Log Likelihood: 58.7094\tSigma2 Prior: -601.7183\tRegularization: 0.0010\n",
      "Iter: 14590  \tTraining Loss: -535.7709    \n",
      "    Negative Log Likelihood: 59.6645\tSigma2 Prior: -595.4365\tRegularization: 0.0010\n",
      "Iter: 14600  \tTraining Loss: -527.6693    \n",
      "    Negative Log Likelihood: 61.2368\tSigma2 Prior: -588.9071\tRegularization: 0.0010\n",
      "Iter: 14610  \tTraining Loss: -543.5580    \n",
      "    Negative Log Likelihood: 60.1116\tSigma2 Prior: -603.6705\tRegularization: 0.0010\n",
      "Iter: 14620  \tTraining Loss: -535.5752    \n",
      "    Negative Log Likelihood: 61.4648\tSigma2 Prior: -597.0410\tRegularization: 0.0010\n",
      "Iter: 14630  \tTraining Loss: -554.5421    \n",
      "    Negative Log Likelihood: 58.0716\tSigma2 Prior: -612.6146\tRegularization: 0.0010\n",
      "Iter: 14640  \tTraining Loss: -539.4198    \n",
      "    Negative Log Likelihood: 60.9909\tSigma2 Prior: -600.4117\tRegularization: 0.0010\n",
      "Iter: 14650  \tTraining Loss: -542.4353    \n",
      "    Negative Log Likelihood: 60.1636\tSigma2 Prior: -602.5999\tRegularization: 0.0010\n",
      "Iter: 14660  \tTraining Loss: -544.8203    \n",
      "    Negative Log Likelihood: 59.8681\tSigma2 Prior: -604.6893\tRegularization: 0.0010\n",
      "Iter: 14670  \tTraining Loss: -532.3383    \n",
      "    Negative Log Likelihood: 62.3960\tSigma2 Prior: -594.7352\tRegularization: 0.0010\n",
      "Iter: 14680  \tTraining Loss: -526.9031    \n",
      "    Negative Log Likelihood: 61.5383\tSigma2 Prior: -588.4424\tRegularization: 0.0010\n",
      "Iter: 14690  \tTraining Loss: -536.8339    \n",
      "    Negative Log Likelihood: 59.3586\tSigma2 Prior: -596.1935\tRegularization: 0.0010\n",
      "Iter: 14700  \tTraining Loss: -546.8757    \n",
      "    Negative Log Likelihood: 58.5912\tSigma2 Prior: -605.4680\tRegularization: 0.0010\n",
      "Iter: 14710  \tTraining Loss: -543.1354    \n",
      "    Negative Log Likelihood: 59.4351\tSigma2 Prior: -602.5715\tRegularization: 0.0010\n",
      "Iter: 14720  \tTraining Loss: -538.4722    \n",
      "    Negative Log Likelihood: 60.8618\tSigma2 Prior: -599.3351\tRegularization: 0.0010\n",
      "Iter: 14730  \tTraining Loss: -557.2720    \n",
      "    Negative Log Likelihood: 57.7899\tSigma2 Prior: -615.0630\tRegularization: 0.0010\n",
      "Iter: 14740  \tTraining Loss: -542.7415    \n",
      "    Negative Log Likelihood: 60.6140\tSigma2 Prior: -603.3566\tRegularization: 0.0010\n",
      "Iter: 14750  \tTraining Loss: -549.6010    \n",
      "    Negative Log Likelihood: 59.4418\tSigma2 Prior: -609.0438\tRegularization: 0.0010\n",
      "Iter: 14760  \tTraining Loss: -537.2231    \n",
      "    Negative Log Likelihood: 60.3863\tSigma2 Prior: -597.6104\tRegularization: 0.0010\n",
      "Iter: 14770  \tTraining Loss: -541.7318    \n",
      "    Negative Log Likelihood: 59.3918\tSigma2 Prior: -601.1246\tRegularization: 0.0010\n",
      "Iter: 14780  \tTraining Loss: -540.8999    \n",
      "    Negative Log Likelihood: 60.7931\tSigma2 Prior: -601.6941\tRegularization: 0.0010\n",
      "Iter: 14790  \tTraining Loss: -557.4183    \n",
      "    Negative Log Likelihood: 57.3178\tSigma2 Prior: -614.7371\tRegularization: 0.0010\n",
      "Iter: 14800  \tTraining Loss: -541.0482    \n",
      "    Negative Log Likelihood: 59.8684\tSigma2 Prior: -600.9176\tRegularization: 0.0010\n",
      "Iter: 14810  \tTraining Loss: -546.6794    \n",
      "    Negative Log Likelihood: 59.6277\tSigma2 Prior: -606.3082\tRegularization: 0.0010\n",
      "Iter: 14820  \tTraining Loss: -549.0035    \n",
      "    Negative Log Likelihood: 58.5466\tSigma2 Prior: -607.5511\tRegularization: 0.0010\n",
      "Iter: 14830  \tTraining Loss: -535.8958    \n",
      "    Negative Log Likelihood: 60.9446\tSigma2 Prior: -596.8414\tRegularization: 0.0010\n",
      "Iter: 14840  \tTraining Loss: -547.5930    \n",
      "    Negative Log Likelihood: 58.9290\tSigma2 Prior: -606.5230\tRegularization: 0.0010\n",
      "Iter: 14850  \tTraining Loss: -529.7976    \n",
      "    Negative Log Likelihood: 61.2144\tSigma2 Prior: -591.0130\tRegularization: 0.0010\n",
      "Iter: 14860  \tTraining Loss: -549.7574    \n",
      "    Negative Log Likelihood: 59.9630\tSigma2 Prior: -609.7215\tRegularization: 0.0010\n",
      "Iter: 14870  \tTraining Loss: -544.5588    \n",
      "    Negative Log Likelihood: 58.6838\tSigma2 Prior: -603.2436\tRegularization: 0.0010\n",
      "Iter: 14880  \tTraining Loss: -555.0892    \n",
      "    Negative Log Likelihood: 58.4384\tSigma2 Prior: -613.5286\tRegularization: 0.0010\n",
      "Iter: 14890  \tTraining Loss: -547.7231    \n",
      "    Negative Log Likelihood: 60.1997\tSigma2 Prior: -607.9239\tRegularization: 0.0010\n",
      "Iter: 14900  \tTraining Loss: -528.3405    \n",
      "    Negative Log Likelihood: 61.6233\tSigma2 Prior: -589.9647\tRegularization: 0.0010\n",
      "Iter: 14910  \tTraining Loss: -538.9074    \n",
      "    Negative Log Likelihood: 60.4021\tSigma2 Prior: -599.3106\tRegularization: 0.0010\n",
      "Iter: 14920  \tTraining Loss: -539.8324    \n",
      "    Negative Log Likelihood: 60.4344\tSigma2 Prior: -600.2678\tRegularization: 0.0010\n",
      "Iter: 14930  \tTraining Loss: -558.8315    \n",
      "    Negative Log Likelihood: 57.4151\tSigma2 Prior: -616.2476\tRegularization: 0.0010\n",
      "Iter: 14940  \tTraining Loss: -515.5904    \n",
      "    Negative Log Likelihood: 63.4449\tSigma2 Prior: -579.0364\tRegularization: 0.0010\n",
      "Iter: 14950  \tTraining Loss: -545.3997    \n",
      "    Negative Log Likelihood: 59.0491\tSigma2 Prior: -604.4498\tRegularization: 0.0010\n",
      "Iter: 14960  \tTraining Loss: -544.7501    \n",
      "    Negative Log Likelihood: 59.1043\tSigma2 Prior: -603.8553\tRegularization: 0.0010\n",
      "Iter: 14970  \tTraining Loss: -548.6066    \n",
      "    Negative Log Likelihood: 58.8471\tSigma2 Prior: -607.4548\tRegularization: 0.0010\n",
      "Iter: 14980  \tTraining Loss: -550.4885    \n",
      "    Negative Log Likelihood: 58.2665\tSigma2 Prior: -608.7560\tRegularization: 0.0010\n",
      "Iter: 14990  \tTraining Loss: -544.3208    \n",
      "    Negative Log Likelihood: 58.6436\tSigma2 Prior: -602.9655\tRegularization: 0.0010\n",
      "Iter: 15000  \tTraining Loss: -537.6251    \n",
      "    Negative Log Likelihood: 61.4395\tSigma2 Prior: -599.0657\tRegularization: 0.0010\n",
      "Iter: 15010  \tTraining Loss: -536.4769    \n",
      "    Negative Log Likelihood: 61.2471\tSigma2 Prior: -597.7251\tRegularization: 0.0010\n",
      "Iter: 15020  \tTraining Loss: -532.2128    \n",
      "    Negative Log Likelihood: 61.1235\tSigma2 Prior: -593.3373\tRegularization: 0.0010\n",
      "Iter: 15030  \tTraining Loss: -549.0155    \n",
      "    Negative Log Likelihood: 58.9148\tSigma2 Prior: -607.9313\tRegularization: 0.0010\n",
      "Iter: 15040  \tTraining Loss: -545.1454    \n",
      "    Negative Log Likelihood: 58.9826\tSigma2 Prior: -604.1290\tRegularization: 0.0010\n",
      "Iter: 15050  \tTraining Loss: -535.5773    \n",
      "    Negative Log Likelihood: 59.9604\tSigma2 Prior: -595.5388\tRegularization: 0.0010\n",
      "Iter: 15060  \tTraining Loss: -523.2206    \n",
      "    Negative Log Likelihood: 61.9352\tSigma2 Prior: -585.1569\tRegularization: 0.0010\n",
      "Iter: 15070  \tTraining Loss: -554.7113    \n",
      "    Negative Log Likelihood: 57.6474\tSigma2 Prior: -612.3598\tRegularization: 0.0010\n",
      "Iter: 15080  \tTraining Loss: -519.6735    \n",
      "    Negative Log Likelihood: 61.6073\tSigma2 Prior: -581.2819\tRegularization: 0.0010\n",
      "Iter: 15090  \tTraining Loss: -531.8108    \n",
      "    Negative Log Likelihood: 60.9685\tSigma2 Prior: -592.7804\tRegularization: 0.0010\n",
      "Iter: 15100  \tTraining Loss: -553.2757    \n",
      "    Negative Log Likelihood: 57.7071\tSigma2 Prior: -610.9838\tRegularization: 0.0010\n",
      "Iter: 15110  \tTraining Loss: -547.0020    \n",
      "    Negative Log Likelihood: 58.8499\tSigma2 Prior: -605.8530\tRegularization: 0.0010\n",
      "Iter: 15120  \tTraining Loss: -533.2339    \n",
      "    Negative Log Likelihood: 60.4692\tSigma2 Prior: -593.7041\tRegularization: 0.0010\n",
      "Iter: 15130  \tTraining Loss: -541.4543    \n",
      "    Negative Log Likelihood: 59.8957\tSigma2 Prior: -601.3510\tRegularization: 0.0010\n",
      "Iter: 15140  \tTraining Loss: -518.5043    \n",
      "    Negative Log Likelihood: 61.1853\tSigma2 Prior: -579.6907\tRegularization: 0.0010\n",
      "Iter: 15150  \tTraining Loss: -519.6940    \n",
      "    Negative Log Likelihood: 61.7122\tSigma2 Prior: -581.4072\tRegularization: 0.0010\n",
      "Iter: 15160  \tTraining Loss: -547.4529    \n",
      "    Negative Log Likelihood: 58.0670\tSigma2 Prior: -605.5209\tRegularization: 0.0010\n",
      "Iter: 15170  \tTraining Loss: -549.8222    \n",
      "    Negative Log Likelihood: 58.0702\tSigma2 Prior: -607.8935\tRegularization: 0.0010\n",
      "Iter: 15180  \tTraining Loss: -536.1423    \n",
      "    Negative Log Likelihood: 58.9951\tSigma2 Prior: -595.1384\tRegularization: 0.0010\n",
      "Iter: 15190  \tTraining Loss: -556.9930    \n",
      "    Negative Log Likelihood: 56.8088\tSigma2 Prior: -613.8029\tRegularization: 0.0010\n",
      "Iter: 15200  \tTraining Loss: -538.2440    \n",
      "    Negative Log Likelihood: 59.9650\tSigma2 Prior: -598.2100\tRegularization: 0.0010\n",
      "Iter: 15210  \tTraining Loss: -520.3881    \n",
      "    Negative Log Likelihood: 60.8581\tSigma2 Prior: -581.2472\tRegularization: 0.0010\n",
      "Iter: 15220  \tTraining Loss: -537.1983    \n",
      "    Negative Log Likelihood: 58.9113\tSigma2 Prior: -596.1107\tRegularization: 0.0010\n",
      "Iter: 15230  \tTraining Loss: -531.9944    \n",
      "    Negative Log Likelihood: 60.1786\tSigma2 Prior: -592.1740\tRegularization: 0.0010\n",
      "Iter: 15240  \tTraining Loss: -540.1516    \n",
      "    Negative Log Likelihood: 59.7147\tSigma2 Prior: -599.8674\tRegularization: 0.0010\n",
      "Iter: 15250  \tTraining Loss: -506.9624    \n",
      "    Negative Log Likelihood: 61.7723\tSigma2 Prior: -568.7357\tRegularization: 0.0010\n",
      "Iter: 15260  \tTraining Loss: -524.5564    \n",
      "    Negative Log Likelihood: 61.3535\tSigma2 Prior: -585.9109\tRegularization: 0.0010\n",
      "Iter: 15270  \tTraining Loss: -526.5622    \n",
      "    Negative Log Likelihood: 60.6834\tSigma2 Prior: -587.2466\tRegularization: 0.0010\n",
      "Iter: 15280  \tTraining Loss: -544.7834    \n",
      "    Negative Log Likelihood: 57.8461\tSigma2 Prior: -602.6305\tRegularization: 0.0010\n",
      "Iter: 15290  \tTraining Loss: -561.4122    \n",
      "    Negative Log Likelihood: 56.3468\tSigma2 Prior: -617.7600\tRegularization: 0.0010\n",
      "Iter: 15300  \tTraining Loss: -522.3791    \n",
      "    Negative Log Likelihood: 61.1764\tSigma2 Prior: -583.5565\tRegularization: 0.0010\n",
      "Iter: 15310  \tTraining Loss: -536.7037    \n",
      "    Negative Log Likelihood: 60.1417\tSigma2 Prior: -596.8464\tRegularization: 0.0010\n",
      "Iter: 15320  \tTraining Loss: -544.5756    \n",
      "    Negative Log Likelihood: 58.8207\tSigma2 Prior: -603.3973\tRegularization: 0.0010\n",
      "Iter: 15330  \tTraining Loss: -540.7323    \n",
      "    Negative Log Likelihood: 59.6736\tSigma2 Prior: -600.4069\tRegularization: 0.0010\n",
      "Iter: 15340  \tTraining Loss: -548.7970    \n",
      "    Negative Log Likelihood: 59.1293\tSigma2 Prior: -607.9273\tRegularization: 0.0010\n",
      "Iter: 15350  \tTraining Loss: -550.8442    \n",
      "    Negative Log Likelihood: 58.2730\tSigma2 Prior: -609.1182\tRegularization: 0.0010\n",
      "Iter: 15360  \tTraining Loss: -546.5599    \n",
      "    Negative Log Likelihood: 59.1456\tSigma2 Prior: -605.7066\tRegularization: 0.0010\n",
      "Iter: 15370  \tTraining Loss: -550.6194    \n",
      "    Negative Log Likelihood: 59.1804\tSigma2 Prior: -609.8008\tRegularization: 0.0010\n",
      "Iter: 15380  \tTraining Loss: -539.8221    \n",
      "    Negative Log Likelihood: 59.1257\tSigma2 Prior: -598.9489\tRegularization: 0.0010\n",
      "Iter: 15390  \tTraining Loss: -548.9637    \n",
      "    Negative Log Likelihood: 58.5836\tSigma2 Prior: -607.5483\tRegularization: 0.0010\n",
      "Iter: 15400  \tTraining Loss: -540.0089    \n",
      "    Negative Log Likelihood: 59.5333\tSigma2 Prior: -599.5433\tRegularization: 0.0010\n",
      "Iter: 15410  \tTraining Loss: -545.2388    \n",
      "    Negative Log Likelihood: 60.7086\tSigma2 Prior: -605.9485\tRegularization: 0.0010\n",
      "Iter: 15420  \tTraining Loss: -543.9808    \n",
      "    Negative Log Likelihood: 61.1357\tSigma2 Prior: -605.1176\tRegularization: 0.0010\n",
      "Iter: 15430  \tTraining Loss: -554.6014    \n",
      "    Negative Log Likelihood: 60.2098\tSigma2 Prior: -614.8122\tRegularization: 0.0010\n",
      "Iter: 15440  \tTraining Loss: -538.7151    \n",
      "    Negative Log Likelihood: 61.2051\tSigma2 Prior: -599.9213\tRegularization: 0.0010\n",
      "Iter: 15450  \tTraining Loss: -531.7114    \n",
      "    Negative Log Likelihood: 62.8015\tSigma2 Prior: -594.5139\tRegularization: 0.0010\n",
      "Iter: 15460  \tTraining Loss: -564.8412    \n",
      "    Negative Log Likelihood: 57.1997\tSigma2 Prior: -622.0419\tRegularization: 0.0010\n",
      "Iter: 15470  \tTraining Loss: -552.7278    \n",
      "    Negative Log Likelihood: 58.8328\tSigma2 Prior: -611.5616\tRegularization: 0.0010\n",
      "Iter: 15480  \tTraining Loss: -535.0663    \n",
      "    Negative Log Likelihood: 61.9442\tSigma2 Prior: -597.0116\tRegularization: 0.0010\n",
      "Iter: 15490  \tTraining Loss: -535.4120    \n",
      "    Negative Log Likelihood: 61.2588\tSigma2 Prior: -596.6718\tRegularization: 0.0010\n",
      "Iter: 15500  \tTraining Loss: -538.7901    \n",
      "    Negative Log Likelihood: 60.6924\tSigma2 Prior: -599.4835\tRegularization: 0.0010\n",
      "Iter: 15510  \tTraining Loss: -509.5868    \n",
      "    Negative Log Likelihood: 64.0734\tSigma2 Prior: -573.6613\tRegularization: 0.0010\n",
      "Iter: 15520  \tTraining Loss: -521.7132    \n",
      "    Negative Log Likelihood: 61.8601\tSigma2 Prior: -583.5743\tRegularization: 0.0010\n",
      "Iter: 15530  \tTraining Loss: -536.7385    \n",
      "    Negative Log Likelihood: 59.8324\tSigma2 Prior: -596.5719\tRegularization: 0.0010\n",
      "Iter: 15540  \tTraining Loss: -554.2223    \n",
      "    Negative Log Likelihood: 56.8299\tSigma2 Prior: -611.0532\tRegularization: 0.0010\n",
      "Iter: 15550  \tTraining Loss: -530.3655    \n",
      "    Negative Log Likelihood: 60.8320\tSigma2 Prior: -591.1986\tRegularization: 0.0010\n",
      "Iter: 15560  \tTraining Loss: -529.5013    \n",
      "    Negative Log Likelihood: 59.6824\tSigma2 Prior: -589.1848\tRegularization: 0.0010\n",
      "Iter: 15570  \tTraining Loss: -527.0466    \n",
      "    Negative Log Likelihood: 61.3482\tSigma2 Prior: -588.3958\tRegularization: 0.0010\n",
      "Iter: 15580  \tTraining Loss: -548.3423    \n",
      "    Negative Log Likelihood: 58.3205\tSigma2 Prior: -606.6639\tRegularization: 0.0010\n",
      "Iter: 15590  \tTraining Loss: -533.8931    \n",
      "    Negative Log Likelihood: 59.6945\tSigma2 Prior: -593.5886\tRegularization: 0.0010\n",
      "Iter: 15600  \tTraining Loss: -530.3238    \n",
      "    Negative Log Likelihood: 61.3246\tSigma2 Prior: -591.6494\tRegularization: 0.0010\n",
      "Iter: 15610  \tTraining Loss: -548.5579    \n",
      "    Negative Log Likelihood: 59.1578\tSigma2 Prior: -607.7167\tRegularization: 0.0010\n",
      "Iter: 15620  \tTraining Loss: -549.1608    \n",
      "    Negative Log Likelihood: 59.4301\tSigma2 Prior: -608.5920\tRegularization: 0.0010\n",
      "Iter: 15630  \tTraining Loss: -541.9418    \n",
      "    Negative Log Likelihood: 60.0020\tSigma2 Prior: -601.9448\tRegularization: 0.0010\n",
      "Iter: 15640  \tTraining Loss: -547.3037    \n",
      "    Negative Log Likelihood: 59.3207\tSigma2 Prior: -606.6254\tRegularization: 0.0010\n",
      "Iter: 15650  \tTraining Loss: -527.2669    \n",
      "    Negative Log Likelihood: 61.3615\tSigma2 Prior: -588.6295\tRegularization: 0.0010\n",
      "Iter: 15660  \tTraining Loss: -534.6400    \n",
      "    Negative Log Likelihood: 60.0734\tSigma2 Prior: -594.7144\tRegularization: 0.0010\n",
      "Iter: 15670  \tTraining Loss: -562.1078    \n",
      "    Negative Log Likelihood: 57.5511\tSigma2 Prior: -619.6599\tRegularization: 0.0010\n",
      "Iter: 15680  \tTraining Loss: -550.5046    \n",
      "    Negative Log Likelihood: 57.6550\tSigma2 Prior: -608.1606\tRegularization: 0.0010\n",
      "Iter: 15690  \tTraining Loss: -558.2259    \n",
      "    Negative Log Likelihood: 58.0605\tSigma2 Prior: -616.2874\tRegularization: 0.0010\n",
      "Iter: 15700  \tTraining Loss: -548.9210    \n",
      "    Negative Log Likelihood: 58.8916\tSigma2 Prior: -607.8137\tRegularization: 0.0010\n",
      "Iter: 15710  \tTraining Loss: -509.6874    \n",
      "    Negative Log Likelihood: 62.8495\tSigma2 Prior: -572.5380\tRegularization: 0.0010\n",
      "Iter: 15720  \tTraining Loss: -550.5090    \n",
      "    Negative Log Likelihood: 59.0686\tSigma2 Prior: -609.5786\tRegularization: 0.0010\n",
      "Iter: 15730  \tTraining Loss: -539.9488    \n",
      "    Negative Log Likelihood: 58.8952\tSigma2 Prior: -598.8450\tRegularization: 0.0010\n",
      "Iter: 15740  \tTraining Loss: -532.6225    \n",
      "    Negative Log Likelihood: 59.9397\tSigma2 Prior: -592.5632\tRegularization: 0.0010\n",
      "Iter: 15750  \tTraining Loss: -546.4374    \n",
      "    Negative Log Likelihood: 58.2520\tSigma2 Prior: -604.6906\tRegularization: 0.0010\n",
      "Iter: 15760  \tTraining Loss: -549.5236    \n",
      "    Negative Log Likelihood: 58.1981\tSigma2 Prior: -607.7227\tRegularization: 0.0010\n",
      "Iter: 15770  \tTraining Loss: -527.7896    \n",
      "    Negative Log Likelihood: 59.9658\tSigma2 Prior: -587.7563\tRegularization: 0.0010\n",
      "Iter: 15780  \tTraining Loss: -547.5087    \n",
      "    Negative Log Likelihood: 58.0675\tSigma2 Prior: -605.5772\tRegularization: 0.0010\n",
      "Iter: 15790  \tTraining Loss: -536.3192    \n",
      "    Negative Log Likelihood: 58.1426\tSigma2 Prior: -594.4628\tRegularization: 0.0010\n",
      "Iter: 15800  \tTraining Loss: -536.3828    \n",
      "    Negative Log Likelihood: 60.8925\tSigma2 Prior: -597.2763\tRegularization: 0.0010\n",
      "Iter: 15810  \tTraining Loss: -554.5517    \n",
      "    Negative Log Likelihood: 57.4616\tSigma2 Prior: -612.0143\tRegularization: 0.0010\n",
      "Iter: 15820  \tTraining Loss: -529.6368    \n",
      "    Negative Log Likelihood: 60.8784\tSigma2 Prior: -590.5163\tRegularization: 0.0010\n",
      "Iter: 15830  \tTraining Loss: -533.2610    \n",
      "    Negative Log Likelihood: 58.7272\tSigma2 Prior: -591.9893\tRegularization: 0.0010\n",
      "Iter: 15840  \tTraining Loss: -532.4040    \n",
      "    Negative Log Likelihood: 58.8484\tSigma2 Prior: -591.2535\tRegularization: 0.0010\n",
      "Iter: 15850  \tTraining Loss: -530.1404    \n",
      "    Negative Log Likelihood: 60.3580\tSigma2 Prior: -590.4995\tRegularization: 0.0010\n",
      "Iter: 15860  \tTraining Loss: -528.8234    \n",
      "    Negative Log Likelihood: 59.6108\tSigma2 Prior: -588.4353\tRegularization: 0.0010\n",
      "Iter: 15870  \tTraining Loss: -522.8001    \n",
      "    Negative Log Likelihood: 61.3922\tSigma2 Prior: -584.1934\tRegularization: 0.0010\n",
      "Iter: 15880  \tTraining Loss: -565.2153    \n",
      "    Negative Log Likelihood: 56.3248\tSigma2 Prior: -621.5412\tRegularization: 0.0010\n",
      "Iter: 15890  \tTraining Loss: -553.4161    \n",
      "    Negative Log Likelihood: 57.2903\tSigma2 Prior: -610.7075\tRegularization: 0.0010\n",
      "Iter: 15900  \tTraining Loss: -525.0695    \n",
      "    Negative Log Likelihood: 61.4010\tSigma2 Prior: -586.4716\tRegularization: 0.0010\n",
      "Iter: 15910  \tTraining Loss: -552.4122    \n",
      "    Negative Log Likelihood: 58.8319\tSigma2 Prior: -611.2451\tRegularization: 0.0010\n",
      "Iter: 15920  \tTraining Loss: -549.1711    \n",
      "    Negative Log Likelihood: 58.1189\tSigma2 Prior: -607.2910\tRegularization: 0.0010\n",
      "Iter: 15930  \tTraining Loss: -513.7525    \n",
      "    Negative Log Likelihood: 62.6610\tSigma2 Prior: -576.4146\tRegularization: 0.0010\n",
      "Iter: 15940  \tTraining Loss: -518.5004    \n",
      "    Negative Log Likelihood: 62.0246\tSigma2 Prior: -580.5260\tRegularization: 0.0010\n",
      "Iter: 15950  \tTraining Loss: -532.0457    \n",
      "    Negative Log Likelihood: 61.8793\tSigma2 Prior: -593.9261\tRegularization: 0.0010\n",
      "Iter: 15960  \tTraining Loss: -549.7545    \n",
      "    Negative Log Likelihood: 59.2201\tSigma2 Prior: -608.9756\tRegularization: 0.0010\n",
      "Iter: 15970  \tTraining Loss: -542.0365    \n",
      "    Negative Log Likelihood: 58.9944\tSigma2 Prior: -601.0320\tRegularization: 0.0010\n",
      "Iter: 15980  \tTraining Loss: -542.8540    \n",
      "    Negative Log Likelihood: 60.6669\tSigma2 Prior: -603.5220\tRegularization: 0.0010\n",
      "Iter: 15990  \tTraining Loss: -545.1261    \n",
      "    Negative Log Likelihood: 59.2122\tSigma2 Prior: -604.3393\tRegularization: 0.0010\n",
      "Iter: 16000  \tTraining Loss: -530.2512    \n",
      "    Negative Log Likelihood: 60.1480\tSigma2 Prior: -590.4003\tRegularization: 0.0010\n",
      "Iter: 16010  \tTraining Loss: -526.6841    \n",
      "    Negative Log Likelihood: 62.1578\tSigma2 Prior: -588.8430\tRegularization: 0.0010\n",
      "Iter: 16020  \tTraining Loss: -540.6971    \n",
      "    Negative Log Likelihood: 60.0910\tSigma2 Prior: -600.7892\tRegularization: 0.0010\n",
      "Iter: 16030  \tTraining Loss: -556.8158    \n",
      "    Negative Log Likelihood: 57.3700\tSigma2 Prior: -614.1869\tRegularization: 0.0010\n",
      "Iter: 16040  \tTraining Loss: -530.6721    \n",
      "    Negative Log Likelihood: 61.5916\tSigma2 Prior: -592.2647\tRegularization: 0.0010\n",
      "Iter: 16050  \tTraining Loss: -544.9650    \n",
      "    Negative Log Likelihood: 59.9261\tSigma2 Prior: -604.8922\tRegularization: 0.0010\n",
      "Iter: 16060  \tTraining Loss: -535.4312    \n",
      "    Negative Log Likelihood: 59.0830\tSigma2 Prior: -594.5153\tRegularization: 0.0010\n",
      "Iter: 16070  \tTraining Loss: -544.5472    \n",
      "    Negative Log Likelihood: 60.4919\tSigma2 Prior: -605.0401\tRegularization: 0.0010\n",
      "Iter: 16080  \tTraining Loss: -543.5671    \n",
      "    Negative Log Likelihood: 58.1346\tSigma2 Prior: -601.7028\tRegularization: 0.0010\n",
      "Iter: 16090  \tTraining Loss: -543.1542    \n",
      "    Negative Log Likelihood: 60.7355\tSigma2 Prior: -603.8907\tRegularization: 0.0010\n",
      "Iter: 16100  \tTraining Loss: -555.7537    \n",
      "    Negative Log Likelihood: 57.0691\tSigma2 Prior: -612.8238\tRegularization: 0.0010\n",
      "Iter: 16110  \tTraining Loss: -540.7454    \n",
      "    Negative Log Likelihood: 61.3788\tSigma2 Prior: -602.1252\tRegularization: 0.0010\n",
      "Iter: 16120  \tTraining Loss: -539.4096    \n",
      "    Negative Log Likelihood: 60.4523\tSigma2 Prior: -599.8630\tRegularization: 0.0010\n",
      "Iter: 16130  \tTraining Loss: -551.6505    \n",
      "    Negative Log Likelihood: 56.5029\tSigma2 Prior: -608.1544\tRegularization: 0.0010\n",
      "Iter: 16140  \tTraining Loss: -547.3185    \n",
      "    Negative Log Likelihood: 59.5284\tSigma2 Prior: -606.8480\tRegularization: 0.0010\n",
      "Iter: 16150  \tTraining Loss: -541.3845    \n",
      "    Negative Log Likelihood: 58.7159\tSigma2 Prior: -600.1015\tRegularization: 0.0010\n",
      "Iter: 16160  \tTraining Loss: -543.4692    \n",
      "    Negative Log Likelihood: 59.4799\tSigma2 Prior: -602.9502\tRegularization: 0.0010\n",
      "Iter: 16170  \tTraining Loss: -539.9039    \n",
      "    Negative Log Likelihood: 58.5658\tSigma2 Prior: -598.4708\tRegularization: 0.0010\n",
      "Iter: 16180  \tTraining Loss: -530.5311    \n",
      "    Negative Log Likelihood: 61.5282\tSigma2 Prior: -592.0604\tRegularization: 0.0010\n",
      "Iter: 16190  \tTraining Loss: -547.8057    \n",
      "    Negative Log Likelihood: 58.2656\tSigma2 Prior: -606.0723\tRegularization: 0.0010\n",
      "Iter: 16200  \tTraining Loss: -528.1883    \n",
      "    Negative Log Likelihood: 61.1722\tSigma2 Prior: -589.3616\tRegularization: 0.0010\n",
      "Iter: 16210  \tTraining Loss: -527.6855    \n",
      "    Negative Log Likelihood: 60.1907\tSigma2 Prior: -587.8773\tRegularization: 0.0010\n",
      "Iter: 16220  \tTraining Loss: -516.7941    \n",
      "    Negative Log Likelihood: 62.0594\tSigma2 Prior: -578.8545\tRegularization: 0.0010\n",
      "Iter: 16230  \tTraining Loss: -544.7912    \n",
      "    Negative Log Likelihood: 57.7066\tSigma2 Prior: -602.4988\tRegularization: 0.0010\n",
      "Iter: 16240  \tTraining Loss: -540.4517    \n",
      "    Negative Log Likelihood: 58.8699\tSigma2 Prior: -599.3227\tRegularization: 0.0010\n",
      "Iter: 16250  \tTraining Loss: -554.7113    \n",
      "    Negative Log Likelihood: 58.0654\tSigma2 Prior: -612.7777\tRegularization: 0.0010\n",
      "Iter: 16260  \tTraining Loss: -541.0277    \n",
      "    Negative Log Likelihood: 57.5608\tSigma2 Prior: -598.5895\tRegularization: 0.0010\n",
      "Iter: 16270  \tTraining Loss: -553.0802    \n",
      "    Negative Log Likelihood: 57.2027\tSigma2 Prior: -610.2840\tRegularization: 0.0010\n",
      "Iter: 16280  \tTraining Loss: -547.3651    \n",
      "    Negative Log Likelihood: 58.4694\tSigma2 Prior: -605.8356\tRegularization: 0.0010\n",
      "Iter: 16290  \tTraining Loss: -539.9291    \n",
      "    Negative Log Likelihood: 57.5696\tSigma2 Prior: -597.4997\tRegularization: 0.0010\n",
      "Iter: 16300  \tTraining Loss: -541.7640    \n",
      "    Negative Log Likelihood: 58.6808\tSigma2 Prior: -600.4458\tRegularization: 0.0010\n",
      "Iter: 16310  \tTraining Loss: -537.1553    \n",
      "    Negative Log Likelihood: 58.8896\tSigma2 Prior: -596.0460\tRegularization: 0.0010\n",
      "Iter: 16320  \tTraining Loss: -541.9214    \n",
      "    Negative Log Likelihood: 60.0382\tSigma2 Prior: -601.9606\tRegularization: 0.0010\n",
      "Iter: 16330  \tTraining Loss: -530.0884    \n",
      "    Negative Log Likelihood: 59.6893\tSigma2 Prior: -589.7787\tRegularization: 0.0010\n",
      "Iter: 16340  \tTraining Loss: -548.2248    \n",
      "    Negative Log Likelihood: 58.1186\tSigma2 Prior: -606.3444\tRegularization: 0.0010\n",
      "Iter: 16350  \tTraining Loss: -547.0391    \n",
      "    Negative Log Likelihood: 58.1623\tSigma2 Prior: -605.2025\tRegularization: 0.0010\n",
      "Iter: 16360  \tTraining Loss: -526.1116    \n",
      "    Negative Log Likelihood: 60.9635\tSigma2 Prior: -587.0762\tRegularization: 0.0010\n",
      "Iter: 16370  \tTraining Loss: -543.4443    \n",
      "    Negative Log Likelihood: 58.1297\tSigma2 Prior: -601.5751\tRegularization: 0.0010\n",
      "Iter: 16380  \tTraining Loss: -536.9568    \n",
      "    Negative Log Likelihood: 60.3394\tSigma2 Prior: -597.2972\tRegularization: 0.0010\n",
      "Iter: 16390  \tTraining Loss: -552.0774    \n",
      "    Negative Log Likelihood: 59.2228\tSigma2 Prior: -611.3013\tRegularization: 0.0010\n",
      "Iter: 16400  \tTraining Loss: -549.8720    \n",
      "    Negative Log Likelihood: 57.8320\tSigma2 Prior: -607.7051\tRegularization: 0.0010\n",
      "Iter: 16410  \tTraining Loss: -538.4332    \n",
      "    Negative Log Likelihood: 59.9846\tSigma2 Prior: -598.4188\tRegularization: 0.0010\n",
      "Iter: 16420  \tTraining Loss: -530.3979    \n",
      "    Negative Log Likelihood: 59.2466\tSigma2 Prior: -589.6456\tRegularization: 0.0010\n",
      "Iter: 16430  \tTraining Loss: -555.2909    \n",
      "    Negative Log Likelihood: 57.7575\tSigma2 Prior: -613.0494\tRegularization: 0.0010\n",
      "Iter: 16440  \tTraining Loss: -540.8093    \n",
      "    Negative Log Likelihood: 59.8591\tSigma2 Prior: -600.6694\tRegularization: 0.0010\n",
      "Iter: 16450  \tTraining Loss: -539.0142    \n",
      "    Negative Log Likelihood: 60.5750\tSigma2 Prior: -599.5903\tRegularization: 0.0010\n",
      "Iter: 16460  \tTraining Loss: -524.4495    \n",
      "    Negative Log Likelihood: 61.6835\tSigma2 Prior: -586.1340\tRegularization: 0.0010\n",
      "Iter: 16470  \tTraining Loss: -556.0219    \n",
      "    Negative Log Likelihood: 58.2921\tSigma2 Prior: -614.3151\tRegularization: 0.0010\n",
      "Iter: 16480  \tTraining Loss: -541.3027    \n",
      "    Negative Log Likelihood: 59.8577\tSigma2 Prior: -601.1614\tRegularization: 0.0010\n",
      "Iter: 16490  \tTraining Loss: -542.0193    \n",
      "    Negative Log Likelihood: 60.0593\tSigma2 Prior: -602.0797\tRegularization: 0.0010\n",
      "Iter: 16500  \tTraining Loss: -540.2914    \n",
      "    Negative Log Likelihood: 59.9553\tSigma2 Prior: -600.2478\tRegularization: 0.0010\n",
      "Iter: 16510  \tTraining Loss: -522.6555    \n",
      "    Negative Log Likelihood: 62.2162\tSigma2 Prior: -584.8727\tRegularization: 0.0010\n",
      "Iter: 16520  \tTraining Loss: -525.5817    \n",
      "    Negative Log Likelihood: 62.7815\tSigma2 Prior: -588.3643\tRegularization: 0.0010\n",
      "Iter: 16530  \tTraining Loss: -522.3264    \n",
      "    Negative Log Likelihood: 62.1597\tSigma2 Prior: -584.4872\tRegularization: 0.0010\n",
      "Iter: 16540  \tTraining Loss: -545.3290    \n",
      "    Negative Log Likelihood: 59.2921\tSigma2 Prior: -604.6221\tRegularization: 0.0010\n",
      "Iter: 16550  \tTraining Loss: -549.5735    \n",
      "    Negative Log Likelihood: 58.5080\tSigma2 Prior: -608.0825\tRegularization: 0.0011\n",
      "Iter: 16560  \tTraining Loss: -540.8545    \n",
      "    Negative Log Likelihood: 60.2157\tSigma2 Prior: -601.0713\tRegularization: 0.0011\n",
      "Iter: 16570  \tTraining Loss: -538.6272    \n",
      "    Negative Log Likelihood: 60.1188\tSigma2 Prior: -598.7470\tRegularization: 0.0011\n",
      "Iter: 16580  \tTraining Loss: -532.3247    \n",
      "    Negative Log Likelihood: 60.8885\tSigma2 Prior: -593.2142\tRegularization: 0.0011\n",
      "Iter: 16590  \tTraining Loss: -554.0927    \n",
      "    Negative Log Likelihood: 57.6153\tSigma2 Prior: -611.7090\tRegularization: 0.0011\n",
      "Iter: 16600  \tTraining Loss: -546.0666    \n",
      "    Negative Log Likelihood: 59.5734\tSigma2 Prior: -605.6411\tRegularization: 0.0011\n",
      "Iter: 16610  \tTraining Loss: -542.9396    \n",
      "    Negative Log Likelihood: 59.9752\tSigma2 Prior: -602.9159\tRegularization: 0.0011\n",
      "Iter: 16620  \tTraining Loss: -529.1191    \n",
      "    Negative Log Likelihood: 61.9917\tSigma2 Prior: -591.1118\tRegularization: 0.0011\n",
      "Iter: 16630  \tTraining Loss: -563.4650    \n",
      "    Negative Log Likelihood: 57.1935\tSigma2 Prior: -620.6595\tRegularization: 0.0011\n",
      "Iter: 16640  \tTraining Loss: -546.1397    \n",
      "    Negative Log Likelihood: 59.3600\tSigma2 Prior: -605.5007\tRegularization: 0.0011\n",
      "Iter: 16650  \tTraining Loss: -544.6206    \n",
      "    Negative Log Likelihood: 59.1992\tSigma2 Prior: -603.8208\tRegularization: 0.0011\n",
      "Iter: 16660  \tTraining Loss: -520.8706    \n",
      "    Negative Log Likelihood: 62.3171\tSigma2 Prior: -583.1888\tRegularization: 0.0011\n",
      "Iter: 16670  \tTraining Loss: -543.5920    \n",
      "    Negative Log Likelihood: 59.7925\tSigma2 Prior: -603.3855\tRegularization: 0.0011\n",
      "Iter: 16680  \tTraining Loss: -547.0189    \n",
      "    Negative Log Likelihood: 58.5397\tSigma2 Prior: -605.5596\tRegularization: 0.0011\n",
      "Iter: 16690  \tTraining Loss: -540.0411    \n",
      "    Negative Log Likelihood: 59.5638\tSigma2 Prior: -599.6059\tRegularization: 0.0011\n",
      "Iter: 16700  \tTraining Loss: -539.9864    \n",
      "    Negative Log Likelihood: 60.5794\tSigma2 Prior: -600.5668\tRegularization: 0.0011\n",
      "Iter: 16710  \tTraining Loss: -526.0611    \n",
      "    Negative Log Likelihood: 61.4458\tSigma2 Prior: -587.5079\tRegularization: 0.0011\n",
      "Iter: 16720  \tTraining Loss: -543.2416    \n",
      "    Negative Log Likelihood: 59.3656\tSigma2 Prior: -602.6082\tRegularization: 0.0011\n",
      "Iter: 16730  \tTraining Loss: -536.2980    \n",
      "    Negative Log Likelihood: 58.2965\tSigma2 Prior: -594.5956\tRegularization: 0.0011\n",
      "Iter: 16740  \tTraining Loss: -514.3130    \n",
      "    Negative Log Likelihood: 61.6990\tSigma2 Prior: -576.0131\tRegularization: 0.0011\n",
      "Iter: 16750  \tTraining Loss: -542.8455    \n",
      "    Negative Log Likelihood: 60.3854\tSigma2 Prior: -603.2319\tRegularization: 0.0011\n",
      "Iter: 16760  \tTraining Loss: -536.4212    \n",
      "    Negative Log Likelihood: 60.4816\tSigma2 Prior: -596.9038\tRegularization: 0.0011\n",
      "Iter: 16770  \tTraining Loss: -537.6892    \n",
      "    Negative Log Likelihood: 60.6838\tSigma2 Prior: -598.3741\tRegularization: 0.0011\n",
      "Iter: 16780  \tTraining Loss: -507.1429    \n",
      "    Negative Log Likelihood: 62.2840\tSigma2 Prior: -569.4280\tRegularization: 0.0011\n",
      "Iter: 16790  \tTraining Loss: -542.9581    \n",
      "    Negative Log Likelihood: 60.6610\tSigma2 Prior: -603.6202\tRegularization: 0.0011\n",
      "Iter: 16800  \tTraining Loss: -541.4451    \n",
      "    Negative Log Likelihood: 58.7048\tSigma2 Prior: -600.1509\tRegularization: 0.0011\n",
      "Iter: 16810  \tTraining Loss: -557.1960    \n",
      "    Negative Log Likelihood: 59.1232\tSigma2 Prior: -616.3202\tRegularization: 0.0011\n",
      "Iter: 16820  \tTraining Loss: -550.9183    \n",
      "    Negative Log Likelihood: 58.2178\tSigma2 Prior: -609.1371\tRegularization: 0.0011\n",
      "Iter: 16830  \tTraining Loss: -541.3830    \n",
      "    Negative Log Likelihood: 57.9578\tSigma2 Prior: -599.3418\tRegularization: 0.0011\n",
      "Iter: 16840  \tTraining Loss: -553.1827    \n",
      "    Negative Log Likelihood: 57.8635\tSigma2 Prior: -611.0472\tRegularization: 0.0011\n",
      "Iter: 16850  \tTraining Loss: -537.9676    \n",
      "    Negative Log Likelihood: 60.7216\tSigma2 Prior: -598.6902\tRegularization: 0.0011\n",
      "Iter: 16860  \tTraining Loss: -535.6245    \n",
      "    Negative Log Likelihood: 60.0752\tSigma2 Prior: -595.7007\tRegularization: 0.0011\n",
      "Iter: 16870  \tTraining Loss: -554.0362    \n",
      "    Negative Log Likelihood: 57.5770\tSigma2 Prior: -611.6143\tRegularization: 0.0011\n",
      "Iter: 16880  \tTraining Loss: -558.8179    \n",
      "    Negative Log Likelihood: 56.5521\tSigma2 Prior: -615.3711\tRegularization: 0.0011\n",
      "Iter: 16890  \tTraining Loss: -527.7729    \n",
      "    Negative Log Likelihood: 61.2711\tSigma2 Prior: -589.0450\tRegularization: 0.0011\n",
      "Iter: 16900  \tTraining Loss: -528.2409    \n",
      "    Negative Log Likelihood: 59.3055\tSigma2 Prior: -587.5475\tRegularization: 0.0011\n",
      "Iter: 16910  \tTraining Loss: -548.7369    \n",
      "    Negative Log Likelihood: 58.6879\tSigma2 Prior: -607.4258\tRegularization: 0.0011\n",
      "Iter: 16920  \tTraining Loss: -556.0823    \n",
      "    Negative Log Likelihood: 56.1782\tSigma2 Prior: -612.2615\tRegularization: 0.0011\n",
      "Iter: 16930  \tTraining Loss: -533.2951    \n",
      "    Negative Log Likelihood: 60.7462\tSigma2 Prior: -594.0424\tRegularization: 0.0011\n",
      "Iter: 16940  \tTraining Loss: -557.3200    \n",
      "    Negative Log Likelihood: 58.6023\tSigma2 Prior: -615.9234\tRegularization: 0.0011\n",
      "Iter: 16950  \tTraining Loss: -545.1569    \n",
      "    Negative Log Likelihood: 58.6302\tSigma2 Prior: -603.7881\tRegularization: 0.0011\n",
      "Iter: 16960  \tTraining Loss: -545.1489    \n",
      "    Negative Log Likelihood: 58.1678\tSigma2 Prior: -603.3177\tRegularization: 0.0011\n",
      "Iter: 16970  \tTraining Loss: -557.5299    \n",
      "    Negative Log Likelihood: 56.1098\tSigma2 Prior: -613.6407\tRegularization: 0.0011\n",
      "Iter: 16980  \tTraining Loss: -516.4958    \n",
      "    Negative Log Likelihood: 61.6459\tSigma2 Prior: -578.1428\tRegularization: 0.0011\n",
      "Iter: 16990  \tTraining Loss: -564.8116    \n",
      "    Negative Log Likelihood: 56.2646\tSigma2 Prior: -621.0773\tRegularization: 0.0011\n",
      "Iter: 17000  \tTraining Loss: -543.4920    \n",
      "    Negative Log Likelihood: 59.0693\tSigma2 Prior: -602.5623\tRegularization: 0.0011\n",
      "Iter: 17010  \tTraining Loss: -542.8311    \n",
      "    Negative Log Likelihood: 57.6536\tSigma2 Prior: -600.4857\tRegularization: 0.0011\n",
      "Iter: 17020  \tTraining Loss: -538.2448    \n",
      "    Negative Log Likelihood: 60.5542\tSigma2 Prior: -598.8001\tRegularization: 0.0011\n",
      "Iter: 17030  \tTraining Loss: -523.9308    \n",
      "    Negative Log Likelihood: 62.4261\tSigma2 Prior: -586.3580\tRegularization: 0.0011\n",
      "Iter: 17040  \tTraining Loss: -531.8539    \n",
      "    Negative Log Likelihood: 61.1964\tSigma2 Prior: -593.0514\tRegularization: 0.0011\n",
      "Iter: 17050  \tTraining Loss: -557.4998    \n",
      "    Negative Log Likelihood: 56.7745\tSigma2 Prior: -614.2754\tRegularization: 0.0011\n",
      "Iter: 17060  \tTraining Loss: -554.7547    \n",
      "    Negative Log Likelihood: 57.8861\tSigma2 Prior: -612.6418\tRegularization: 0.0011\n",
      "Iter: 17070  \tTraining Loss: -553.4848    \n",
      "    Negative Log Likelihood: 58.2265\tSigma2 Prior: -611.7124\tRegularization: 0.0011\n",
      "Iter: 17080  \tTraining Loss: -555.7309    \n",
      "    Negative Log Likelihood: 58.1876\tSigma2 Prior: -613.9195\tRegularization: 0.0011\n",
      "Iter: 17090  \tTraining Loss: -534.5432    \n",
      "    Negative Log Likelihood: 61.3750\tSigma2 Prior: -595.9192\tRegularization: 0.0011\n",
      "Iter: 17100  \tTraining Loss: -538.4971    \n",
      "    Negative Log Likelihood: 60.4850\tSigma2 Prior: -598.9832\tRegularization: 0.0011\n",
      "Iter: 17110  \tTraining Loss: -545.2178    \n",
      "    Negative Log Likelihood: 59.1035\tSigma2 Prior: -604.3223\tRegularization: 0.0011\n",
      "Iter: 17120  \tTraining Loss: -562.4728    \n",
      "    Negative Log Likelihood: 57.7506\tSigma2 Prior: -620.2245\tRegularization: 0.0011\n",
      "Iter: 17130  \tTraining Loss: -526.1408    \n",
      "    Negative Log Likelihood: 61.3102\tSigma2 Prior: -587.4521\tRegularization: 0.0011\n",
      "Iter: 17140  \tTraining Loss: -531.9558    \n",
      "    Negative Log Likelihood: 60.8692\tSigma2 Prior: -592.8260\tRegularization: 0.0011\n",
      "Iter: 17150  \tTraining Loss: -536.8818    \n",
      "    Negative Log Likelihood: 59.6250\tSigma2 Prior: -596.5078\tRegularization: 0.0011\n",
      "Iter: 17160  \tTraining Loss: -521.5566    \n",
      "    Negative Log Likelihood: 61.8208\tSigma2 Prior: -583.3784\tRegularization: 0.0011\n",
      "Iter: 17170  \tTraining Loss: -520.8190    \n",
      "    Negative Log Likelihood: 60.6311\tSigma2 Prior: -581.4512\tRegularization: 0.0011\n",
      "Iter: 17180  \tTraining Loss: -536.2377    \n",
      "    Negative Log Likelihood: 59.7904\tSigma2 Prior: -596.0291\tRegularization: 0.0011\n",
      "Iter: 17190  \tTraining Loss: -514.0218    \n",
      "    Negative Log Likelihood: 61.8051\tSigma2 Prior: -575.8279\tRegularization: 0.0011\n",
      "Iter: 17200  \tTraining Loss: -542.7484    \n",
      "    Negative Log Likelihood: 59.3020\tSigma2 Prior: -602.0514\tRegularization: 0.0011\n",
      "Iter: 17210  \tTraining Loss: -549.1439    \n",
      "    Negative Log Likelihood: 58.2732\tSigma2 Prior: -607.4182\tRegularization: 0.0011\n",
      "Iter: 17220  \tTraining Loss: -534.6946    \n",
      "    Negative Log Likelihood: 59.2375\tSigma2 Prior: -593.9332\tRegularization: 0.0011\n",
      "Iter: 17230  \tTraining Loss: -558.4140    \n",
      "    Negative Log Likelihood: 56.2793\tSigma2 Prior: -614.6944\tRegularization: 0.0011\n",
      "Iter: 17240  \tTraining Loss: -548.2349    \n",
      "    Negative Log Likelihood: 56.7621\tSigma2 Prior: -604.9980\tRegularization: 0.0011\n",
      "Iter: 17250  \tTraining Loss: -517.2297    \n",
      "    Negative Log Likelihood: 61.3746\tSigma2 Prior: -578.6053\tRegularization: 0.0011\n",
      "Iter: 17260  \tTraining Loss: -523.8726    \n",
      "    Negative Log Likelihood: 59.8753\tSigma2 Prior: -583.7490\tRegularization: 0.0011\n",
      "Iter: 17270  \tTraining Loss: -557.2193    \n",
      "    Negative Log Likelihood: 55.3941\tSigma2 Prior: -612.6145\tRegularization: 0.0011\n",
      "Iter: 17280  \tTraining Loss: -552.3511    \n",
      "    Negative Log Likelihood: 57.7132\tSigma2 Prior: -610.0653\tRegularization: 0.0011\n",
      "Iter: 17290  \tTraining Loss: -548.1152    \n",
      "    Negative Log Likelihood: 56.8993\tSigma2 Prior: -605.0155\tRegularization: 0.0011\n",
      "Iter: 17300  \tTraining Loss: -562.9796    \n",
      "    Negative Log Likelihood: 54.4012\tSigma2 Prior: -617.3818\tRegularization: 0.0011\n",
      "Iter: 17310  \tTraining Loss: -556.0486    \n",
      "    Negative Log Likelihood: 57.4827\tSigma2 Prior: -613.5323\tRegularization: 0.0011\n",
      "Iter: 17320  \tTraining Loss: -552.4997    \n",
      "    Negative Log Likelihood: 57.5750\tSigma2 Prior: -610.0757\tRegularization: 0.0011\n",
      "Iter: 17330  \tTraining Loss: -552.4703    \n",
      "    Negative Log Likelihood: 57.3826\tSigma2 Prior: -609.8539\tRegularization: 0.0011\n",
      "Iter: 17340  \tTraining Loss: -544.1539    \n",
      "    Negative Log Likelihood: 58.4694\tSigma2 Prior: -602.6244\tRegularization: 0.0011\n",
      "Iter: 17350  \tTraining Loss: -536.4012    \n",
      "    Negative Log Likelihood: 58.8390\tSigma2 Prior: -595.2413\tRegularization: 0.0011\n",
      "Iter: 17360  \tTraining Loss: -536.5495    \n",
      "    Negative Log Likelihood: 59.2926\tSigma2 Prior: -595.8431\tRegularization: 0.0011\n",
      "Iter: 17370  \tTraining Loss: -537.8873    \n",
      "    Negative Log Likelihood: 58.8757\tSigma2 Prior: -596.7640\tRegularization: 0.0011\n",
      "Iter: 17380  \tTraining Loss: -542.4413    \n",
      "    Negative Log Likelihood: 59.0817\tSigma2 Prior: -601.5240\tRegularization: 0.0011\n",
      "Iter: 17390  \tTraining Loss: -523.9481    \n",
      "    Negative Log Likelihood: 59.8056\tSigma2 Prior: -583.7548\tRegularization: 0.0011\n",
      "Iter: 17400  \tTraining Loss: -557.3181    \n",
      "    Negative Log Likelihood: 56.4660\tSigma2 Prior: -613.7852\tRegularization: 0.0011\n",
      "Iter: 17410  \tTraining Loss: -517.9426    \n",
      "    Negative Log Likelihood: 62.4415\tSigma2 Prior: -580.3853\tRegularization: 0.0011\n",
      "Iter: 17420  \tTraining Loss: -541.1579    \n",
      "    Negative Log Likelihood: 60.1185\tSigma2 Prior: -601.2775\tRegularization: 0.0011\n",
      "Iter: 17430  \tTraining Loss: -537.2371    \n",
      "    Negative Log Likelihood: 58.2947\tSigma2 Prior: -595.5329\tRegularization: 0.0011\n",
      "Iter: 17440  \tTraining Loss: -543.6346    \n",
      "    Negative Log Likelihood: 59.0642\tSigma2 Prior: -602.6998\tRegularization: 0.0011\n",
      "Iter: 17450  \tTraining Loss: -540.9406    \n",
      "    Negative Log Likelihood: 58.5843\tSigma2 Prior: -599.5259\tRegularization: 0.0011\n",
      "Iter: 17460  \tTraining Loss: -540.3026    \n",
      "    Negative Log Likelihood: 59.0440\tSigma2 Prior: -599.3477\tRegularization: 0.0011\n",
      "Iter: 17470  \tTraining Loss: -539.4881    \n",
      "    Negative Log Likelihood: 59.5390\tSigma2 Prior: -599.0281\tRegularization: 0.0011\n",
      "Iter: 17480  \tTraining Loss: -546.3766    \n",
      "    Negative Log Likelihood: 58.9981\tSigma2 Prior: -605.3759\tRegularization: 0.0011\n",
      "Iter: 17490  \tTraining Loss: -535.7278    \n",
      "    Negative Log Likelihood: 58.1773\tSigma2 Prior: -593.9061\tRegularization: 0.0011\n",
      "Iter: 17500  \tTraining Loss: -527.7246    \n",
      "    Negative Log Likelihood: 60.8848\tSigma2 Prior: -588.6105\tRegularization: 0.0011\n",
      "Iter: 17510  \tTraining Loss: -537.3559    \n",
      "    Negative Log Likelihood: 59.7331\tSigma2 Prior: -597.0901\tRegularization: 0.0011\n",
      "Iter: 17520  \tTraining Loss: -549.1610    \n",
      "    Negative Log Likelihood: 57.1016\tSigma2 Prior: -606.2637\tRegularization: 0.0011\n",
      "Iter: 17530  \tTraining Loss: -548.4849    \n",
      "    Negative Log Likelihood: 56.2671\tSigma2 Prior: -604.7532\tRegularization: 0.0011\n",
      "Iter: 17540  \tTraining Loss: -524.8044    \n",
      "    Negative Log Likelihood: 60.2386\tSigma2 Prior: -585.0442\tRegularization: 0.0011\n",
      "Iter: 17550  \tTraining Loss: -557.3376    \n",
      "    Negative Log Likelihood: 55.6776\tSigma2 Prior: -613.0164\tRegularization: 0.0011\n",
      "Iter: 17560  \tTraining Loss: -521.3777    \n",
      "    Negative Log Likelihood: 60.7864\tSigma2 Prior: -582.1652\tRegularization: 0.0011\n",
      "Iter: 17570  \tTraining Loss: -548.7886    \n",
      "    Negative Log Likelihood: 56.7108\tSigma2 Prior: -605.5005\tRegularization: 0.0011\n",
      "Iter: 17580  \tTraining Loss: -536.1238    \n",
      "    Negative Log Likelihood: 59.0005\tSigma2 Prior: -595.1253\tRegularization: 0.0011\n",
      "Iter: 17590  \tTraining Loss: -553.6292    \n",
      "    Negative Log Likelihood: 57.7050\tSigma2 Prior: -611.3352\tRegularization: 0.0011\n",
      "Iter: 17600  \tTraining Loss: -548.8367    \n",
      "    Negative Log Likelihood: 58.1034\tSigma2 Prior: -606.9412\tRegularization: 0.0011\n",
      "Iter: 17610  \tTraining Loss: -535.0561    \n",
      "    Negative Log Likelihood: 59.9089\tSigma2 Prior: -594.9661\tRegularization: 0.0011\n",
      "Iter: 17620  \tTraining Loss: -556.6271    \n",
      "    Negative Log Likelihood: 57.0863\tSigma2 Prior: -613.7145\tRegularization: 0.0011\n",
      "Iter: 17630  \tTraining Loss: -550.9332    \n",
      "    Negative Log Likelihood: 57.7353\tSigma2 Prior: -608.6697\tRegularization: 0.0011\n",
      "Iter: 17640  \tTraining Loss: -542.2914    \n",
      "    Negative Log Likelihood: 59.2114\tSigma2 Prior: -601.5039\tRegularization: 0.0011\n",
      "Iter: 17650  \tTraining Loss: -550.5359    \n",
      "    Negative Log Likelihood: 58.3780\tSigma2 Prior: -608.9150\tRegularization: 0.0011\n",
      "Iter: 17660  \tTraining Loss: -551.1378    \n",
      "    Negative Log Likelihood: 58.8434\tSigma2 Prior: -609.9823\tRegularization: 0.0011\n",
      "Iter: 17670  \tTraining Loss: -540.6166    \n",
      "    Negative Log Likelihood: 59.7479\tSigma2 Prior: -600.3655\tRegularization: 0.0011\n",
      "Iter: 17680  \tTraining Loss: -532.7827    \n",
      "    Negative Log Likelihood: 60.0439\tSigma2 Prior: -592.8278\tRegularization: 0.0011\n",
      "Iter: 17690  \tTraining Loss: -539.2600    \n",
      "    Negative Log Likelihood: 60.5968\tSigma2 Prior: -599.8579\tRegularization: 0.0011\n",
      "Iter: 17700  \tTraining Loss: -536.3776    \n",
      "    Negative Log Likelihood: 59.2087\tSigma2 Prior: -595.5873\tRegularization: 0.0011\n",
      "Iter: 17710  \tTraining Loss: -519.2157    \n",
      "    Negative Log Likelihood: 60.1589\tSigma2 Prior: -579.3757\tRegularization: 0.0011\n",
      "Iter: 17720  \tTraining Loss: -556.6149    \n",
      "    Negative Log Likelihood: 57.1217\tSigma2 Prior: -613.7377\tRegularization: 0.0011\n",
      "Iter: 17730  \tTraining Loss: -558.3533    \n",
      "    Negative Log Likelihood: 56.9183\tSigma2 Prior: -615.2726\tRegularization: 0.0011\n",
      "Iter: 17740  \tTraining Loss: -528.9283    \n",
      "    Negative Log Likelihood: 60.6060\tSigma2 Prior: -589.5354\tRegularization: 0.0011\n",
      "Iter: 17750  \tTraining Loss: -527.2075    \n",
      "    Negative Log Likelihood: 58.9705\tSigma2 Prior: -586.1791\tRegularization: 0.0011\n",
      "Iter: 17760  \tTraining Loss: -531.3189    \n",
      "    Negative Log Likelihood: 59.1269\tSigma2 Prior: -590.4469\tRegularization: 0.0011\n",
      "Iter: 17770  \tTraining Loss: -532.9421    \n",
      "    Negative Log Likelihood: 59.3533\tSigma2 Prior: -592.2966\tRegularization: 0.0011\n",
      "Iter: 17780  \tTraining Loss: -547.8722    \n",
      "    Negative Log Likelihood: 56.9098\tSigma2 Prior: -604.7831\tRegularization: 0.0011\n",
      "Iter: 17790  \tTraining Loss: -522.3278    \n",
      "    Negative Log Likelihood: 61.0744\tSigma2 Prior: -583.4033\tRegularization: 0.0011\n",
      "Iter: 17800  \tTraining Loss: -537.1916    \n",
      "    Negative Log Likelihood: 59.0101\tSigma2 Prior: -596.2028\tRegularization: 0.0011\n",
      "Iter: 17810  \tTraining Loss: -551.0162    \n",
      "    Negative Log Likelihood: 57.7918\tSigma2 Prior: -608.8091\tRegularization: 0.0011\n",
      "Iter: 17820  \tTraining Loss: -533.9810    \n",
      "    Negative Log Likelihood: 59.6866\tSigma2 Prior: -593.6688\tRegularization: 0.0011\n",
      "Iter: 17830  \tTraining Loss: -541.0236    \n",
      "    Negative Log Likelihood: 58.3741\tSigma2 Prior: -599.3987\tRegularization: 0.0011\n",
      "Iter: 17840  \tTraining Loss: -553.7222    \n",
      "    Negative Log Likelihood: 56.1188\tSigma2 Prior: -609.8420\tRegularization: 0.0011\n",
      "Iter: 17850  \tTraining Loss: -529.6137    \n",
      "    Negative Log Likelihood: 60.2170\tSigma2 Prior: -589.8318\tRegularization: 0.0011\n",
      "Iter: 17860  \tTraining Loss: -551.3689    \n",
      "    Negative Log Likelihood: 57.4373\tSigma2 Prior: -608.8073\tRegularization: 0.0011\n",
      "Iter: 17870  \tTraining Loss: -549.2538    \n",
      "    Negative Log Likelihood: 57.4685\tSigma2 Prior: -606.7234\tRegularization: 0.0011\n",
      "Iter: 17880  \tTraining Loss: -539.8440    \n",
      "    Negative Log Likelihood: 59.8930\tSigma2 Prior: -599.7381\tRegularization: 0.0011\n",
      "Iter: 17890  \tTraining Loss: -548.7992    \n",
      "    Negative Log Likelihood: 58.9406\tSigma2 Prior: -607.7409\tRegularization: 0.0011\n",
      "Iter: 17900  \tTraining Loss: -553.7162    \n",
      "    Negative Log Likelihood: 57.5633\tSigma2 Prior: -611.2806\tRegularization: 0.0011\n",
      "Iter: 17910  \tTraining Loss: -525.0673    \n",
      "    Negative Log Likelihood: 59.5100\tSigma2 Prior: -584.5784\tRegularization: 0.0011\n",
      "Iter: 17920  \tTraining Loss: -548.9412    \n",
      "    Negative Log Likelihood: 57.9962\tSigma2 Prior: -606.9385\tRegularization: 0.0011\n",
      "Iter: 17930  \tTraining Loss: -535.4240    \n",
      "    Negative Log Likelihood: 59.3746\tSigma2 Prior: -594.7997\tRegularization: 0.0011\n",
      "Iter: 17940  \tTraining Loss: -546.2049    \n",
      "    Negative Log Likelihood: 58.5095\tSigma2 Prior: -604.7155\tRegularization: 0.0011\n",
      "Iter: 17950  \tTraining Loss: -525.9496    \n",
      "    Negative Log Likelihood: 58.9115\tSigma2 Prior: -584.8622\tRegularization: 0.0011\n",
      "Iter: 17960  \tTraining Loss: -529.7565    \n",
      "    Negative Log Likelihood: 62.1548\tSigma2 Prior: -591.9124\tRegularization: 0.0011\n",
      "Iter: 17970  \tTraining Loss: -540.2786    \n",
      "    Negative Log Likelihood: 59.4612\tSigma2 Prior: -599.7409\tRegularization: 0.0011\n",
      "Iter: 17980  \tTraining Loss: -545.9667    \n",
      "    Negative Log Likelihood: 58.2877\tSigma2 Prior: -604.2554\tRegularization: 0.0011\n",
      "Iter: 17990  \tTraining Loss: -538.1304    \n",
      "    Negative Log Likelihood: 59.8669\tSigma2 Prior: -597.9984\tRegularization: 0.0011\n",
      "Iter: 18000  \tTraining Loss: -559.5290    \n",
      "    Negative Log Likelihood: 56.7526\tSigma2 Prior: -616.2827\tRegularization: 0.0011\n",
      "Iter: 18010  \tTraining Loss: -544.7715    \n",
      "    Negative Log Likelihood: 59.6083\tSigma2 Prior: -604.3809\tRegularization: 0.0011\n",
      "Iter: 18020  \tTraining Loss: -550.5012    \n",
      "    Negative Log Likelihood: 58.6898\tSigma2 Prior: -609.1921\tRegularization: 0.0011\n",
      "Iter: 18030  \tTraining Loss: -515.6892    \n",
      "    Negative Log Likelihood: 62.2263\tSigma2 Prior: -577.9166\tRegularization: 0.0011\n",
      "Iter: 18040  \tTraining Loss: -552.6666    \n",
      "    Negative Log Likelihood: 57.3553\tSigma2 Prior: -610.0230\tRegularization: 0.0011\n",
      "Iter: 18050  \tTraining Loss: -551.1509    \n",
      "    Negative Log Likelihood: 57.6768\tSigma2 Prior: -608.8287\tRegularization: 0.0011\n",
      "Iter: 18060  \tTraining Loss: -510.9737    \n",
      "    Negative Log Likelihood: 61.7571\tSigma2 Prior: -572.7319\tRegularization: 0.0011\n",
      "Iter: 18070  \tTraining Loss: -516.9023    \n",
      "    Negative Log Likelihood: 59.1253\tSigma2 Prior: -576.0286\tRegularization: 0.0011\n",
      "Iter: 18080  \tTraining Loss: -547.9965    \n",
      "    Negative Log Likelihood: 57.4238\tSigma2 Prior: -605.4214\tRegularization: 0.0011\n",
      "Iter: 18090  \tTraining Loss: -545.0602    \n",
      "    Negative Log Likelihood: 56.5539\tSigma2 Prior: -601.6152\tRegularization: 0.0011\n",
      "Iter: 18100  \tTraining Loss: -529.9723    \n",
      "    Negative Log Likelihood: 59.4150\tSigma2 Prior: -589.3884\tRegularization: 0.0011\n",
      "Iter: 18110  \tTraining Loss: -514.6100    \n",
      "    Negative Log Likelihood: 60.1340\tSigma2 Prior: -574.7452\tRegularization: 0.0011\n",
      "Iter: 18120  \tTraining Loss: -551.0809    \n",
      "    Negative Log Likelihood: 55.9018\tSigma2 Prior: -606.9838\tRegularization: 0.0011\n",
      "Iter: 18130  \tTraining Loss: -534.3449    \n",
      "    Negative Log Likelihood: 58.5094\tSigma2 Prior: -592.8554\tRegularization: 0.0011\n",
      "Iter: 18140  \tTraining Loss: -524.3525    \n",
      "    Negative Log Likelihood: 58.6525\tSigma2 Prior: -583.0060\tRegularization: 0.0011\n",
      "Iter: 18150  \tTraining Loss: -519.5352    \n",
      "    Negative Log Likelihood: 60.0461\tSigma2 Prior: -579.5824\tRegularization: 0.0011\n",
      "Iter: 18160  \tTraining Loss: -539.1328    \n",
      "    Negative Log Likelihood: 57.0206\tSigma2 Prior: -596.1545\tRegularization: 0.0011\n",
      "Iter: 18170  \tTraining Loss: -514.9048    \n",
      "    Negative Log Likelihood: 59.7318\tSigma2 Prior: -574.6377\tRegularization: 0.0011\n",
      "Iter: 18180  \tTraining Loss: -560.2688    \n",
      "    Negative Log Likelihood: 55.1211\tSigma2 Prior: -615.3911\tRegularization: 0.0011\n",
      "Iter: 18190  \tTraining Loss: -548.0491    \n",
      "    Negative Log Likelihood: 56.4201\tSigma2 Prior: -604.4703\tRegularization: 0.0011\n",
      "Iter: 18200  \tTraining Loss: -549.6481    \n",
      "    Negative Log Likelihood: 58.0750\tSigma2 Prior: -607.7242\tRegularization: 0.0011\n",
      "Iter: 18210  \tTraining Loss: -540.8432    \n",
      "    Negative Log Likelihood: 58.7536\tSigma2 Prior: -599.5979\tRegularization: 0.0011\n",
      "Iter: 18220  \tTraining Loss: -556.2328    \n",
      "    Negative Log Likelihood: 55.9996\tSigma2 Prior: -612.2335\tRegularization: 0.0011\n",
      "Iter: 18230  \tTraining Loss: -553.3762    \n",
      "    Negative Log Likelihood: 57.2840\tSigma2 Prior: -610.6613\tRegularization: 0.0011\n",
      "Iter: 18240  \tTraining Loss: -551.0880    \n",
      "    Negative Log Likelihood: 59.0485\tSigma2 Prior: -610.1376\tRegularization: 0.0011\n",
      "Iter: 18250  \tTraining Loss: -553.8563    \n",
      "    Negative Log Likelihood: 57.8852\tSigma2 Prior: -611.7425\tRegularization: 0.0011\n",
      "Iter: 18260  \tTraining Loss: -558.5600    \n",
      "    Negative Log Likelihood: 57.4773\tSigma2 Prior: -616.0384\tRegularization: 0.0011\n",
      "Iter: 18270  \tTraining Loss: -541.6110    \n",
      "    Negative Log Likelihood: 59.2127\tSigma2 Prior: -600.8248\tRegularization: 0.0011\n",
      "Iter: 18280  \tTraining Loss: -548.4573    \n",
      "    Negative Log Likelihood: 58.1357\tSigma2 Prior: -606.5941\tRegularization: 0.0011\n",
      "Iter: 18290  \tTraining Loss: -547.1902    \n",
      "    Negative Log Likelihood: 57.9406\tSigma2 Prior: -605.1319\tRegularization: 0.0011\n",
      "Iter: 18300  \tTraining Loss: -528.0389    \n",
      "    Negative Log Likelihood: 61.3228\tSigma2 Prior: -589.3628\tRegularization: 0.0011\n",
      "Iter: 18310  \tTraining Loss: -544.3015    \n",
      "    Negative Log Likelihood: 59.7590\tSigma2 Prior: -604.0615\tRegularization: 0.0011\n",
      "Iter: 18320  \tTraining Loss: -552.6953    \n",
      "    Negative Log Likelihood: 56.9456\tSigma2 Prior: -609.6420\tRegularization: 0.0011\n",
      "Iter: 18330  \tTraining Loss: -561.9388    \n",
      "    Negative Log Likelihood: 56.4586\tSigma2 Prior: -618.3984\tRegularization: 0.0011\n",
      "Iter: 18340  \tTraining Loss: -532.5567    \n",
      "    Negative Log Likelihood: 59.4008\tSigma2 Prior: -591.9586\tRegularization: 0.0011\n",
      "Iter: 18350  \tTraining Loss: -516.2863    \n",
      "    Negative Log Likelihood: 60.1701\tSigma2 Prior: -576.4575\tRegularization: 0.0011\n",
      "Iter: 18360  \tTraining Loss: -540.8054    \n",
      "    Negative Log Likelihood: 58.3930\tSigma2 Prior: -599.1995\tRegularization: 0.0011\n",
      "Iter: 18370  \tTraining Loss: -523.3040    \n",
      "    Negative Log Likelihood: 59.6377\tSigma2 Prior: -582.9428\tRegularization: 0.0011\n",
      "Iter: 18380  \tTraining Loss: -538.4684    \n",
      "    Negative Log Likelihood: 57.6323\tSigma2 Prior: -596.1018\tRegularization: 0.0011\n",
      "Iter: 18390  \tTraining Loss: -534.8829    \n",
      "    Negative Log Likelihood: 60.5364\tSigma2 Prior: -595.4205\tRegularization: 0.0011\n",
      "Iter: 18400  \tTraining Loss: -541.1705    \n",
      "    Negative Log Likelihood: 58.1785\tSigma2 Prior: -599.3501\tRegularization: 0.0011\n",
      "Iter: 18410  \tTraining Loss: -549.1144    \n",
      "    Negative Log Likelihood: 57.9698\tSigma2 Prior: -607.0854\tRegularization: 0.0011\n",
      "Iter: 18420  \tTraining Loss: -531.2987    \n",
      "    Negative Log Likelihood: 59.8992\tSigma2 Prior: -591.1990\tRegularization: 0.0011\n",
      "Iter: 18430  \tTraining Loss: -531.2343    \n",
      "    Negative Log Likelihood: 59.7739\tSigma2 Prior: -591.0092\tRegularization: 0.0011\n",
      "Iter: 18440  \tTraining Loss: -546.7764    \n",
      "    Negative Log Likelihood: 57.9846\tSigma2 Prior: -604.7620\tRegularization: 0.0011\n",
      "Iter: 18450  \tTraining Loss: -530.8187    \n",
      "    Negative Log Likelihood: 60.1829\tSigma2 Prior: -591.0027\tRegularization: 0.0011\n",
      "Iter: 18460  \tTraining Loss: -540.4244    \n",
      "    Negative Log Likelihood: 57.6168\tSigma2 Prior: -598.0423\tRegularization: 0.0011\n",
      "Iter: 18470  \tTraining Loss: -544.6840    \n",
      "    Negative Log Likelihood: 58.2995\tSigma2 Prior: -602.9846\tRegularization: 0.0011\n",
      "Iter: 18480  \tTraining Loss: -531.8329    \n",
      "    Negative Log Likelihood: 60.0300\tSigma2 Prior: -591.8640\tRegularization: 0.0011\n",
      "Iter: 18490  \tTraining Loss: -540.4197    \n",
      "    Negative Log Likelihood: 58.4514\tSigma2 Prior: -598.8722\tRegularization: 0.0011\n",
      "Iter: 18500  \tTraining Loss: -544.4328    \n",
      "    Negative Log Likelihood: 58.7532\tSigma2 Prior: -603.1871\tRegularization: 0.0011\n",
      "Iter: 18510  \tTraining Loss: -555.9988    \n",
      "    Negative Log Likelihood: 56.7443\tSigma2 Prior: -612.7443\tRegularization: 0.0011\n",
      "Iter: 18520  \tTraining Loss: -525.9125    \n",
      "    Negative Log Likelihood: 58.9595\tSigma2 Prior: -584.8732\tRegularization: 0.0011\n",
      "Iter: 18530  \tTraining Loss: -549.9234    \n",
      "    Negative Log Likelihood: 57.1919\tSigma2 Prior: -607.1164\tRegularization: 0.0011\n",
      "Iter: 18540  \tTraining Loss: -547.9291    \n",
      "    Negative Log Likelihood: 57.2180\tSigma2 Prior: -605.1482\tRegularization: 0.0011\n",
      "Iter: 18550  \tTraining Loss: -546.2460    \n",
      "    Negative Log Likelihood: 57.5943\tSigma2 Prior: -603.8415\tRegularization: 0.0011\n",
      "Iter: 18560  \tTraining Loss: -555.3442    \n",
      "    Negative Log Likelihood: 56.1200\tSigma2 Prior: -611.4653\tRegularization: 0.0011\n",
      "Iter: 18570  \tTraining Loss: -539.9531    \n",
      "    Negative Log Likelihood: 58.0969\tSigma2 Prior: -598.0511\tRegularization: 0.0011\n",
      "Iter: 18580  \tTraining Loss: -539.9534    \n",
      "    Negative Log Likelihood: 57.8408\tSigma2 Prior: -597.7953\tRegularization: 0.0011\n",
      "Iter: 18590  \tTraining Loss: -506.9480    \n",
      "    Negative Log Likelihood: 61.0480\tSigma2 Prior: -567.9971\tRegularization: 0.0011\n",
      "Iter: 18600  \tTraining Loss: -533.2847    \n",
      "    Negative Log Likelihood: 58.6107\tSigma2 Prior: -591.8965\tRegularization: 0.0011\n",
      "Iter: 18610  \tTraining Loss: -525.5293    \n",
      "    Negative Log Likelihood: 59.0033\tSigma2 Prior: -584.5338\tRegularization: 0.0011\n",
      "Iter: 18620  \tTraining Loss: -542.5607    \n",
      "    Negative Log Likelihood: 59.1790\tSigma2 Prior: -601.7408\tRegularization: 0.0011\n",
      "Iter: 18630  \tTraining Loss: -519.0583    \n",
      "    Negative Log Likelihood: 60.9597\tSigma2 Prior: -580.0192\tRegularization: 0.0011\n",
      "Iter: 18640  \tTraining Loss: -529.1899    \n",
      "    Negative Log Likelihood: 59.0112\tSigma2 Prior: -588.2022\tRegularization: 0.0011\n",
      "Iter: 18650  \tTraining Loss: -547.9036    \n",
      "    Negative Log Likelihood: 57.3916\tSigma2 Prior: -605.2963\tRegularization: 0.0011\n",
      "Iter: 18660  \tTraining Loss: -545.4374    \n",
      "    Negative Log Likelihood: 57.6076\tSigma2 Prior: -603.0460\tRegularization: 0.0011\n",
      "Iter: 18670  \tTraining Loss: -535.5843    \n",
      "    Negative Log Likelihood: 58.1527\tSigma2 Prior: -593.7381\tRegularization: 0.0011\n",
      "Iter: 18680  \tTraining Loss: -525.6909    \n",
      "    Negative Log Likelihood: 60.1129\tSigma2 Prior: -585.8048\tRegularization: 0.0011\n",
      "Iter: 18690  \tTraining Loss: -546.1339    \n",
      "    Negative Log Likelihood: 57.7917\tSigma2 Prior: -603.9266\tRegularization: 0.0011\n",
      "Iter: 18700  \tTraining Loss: -522.4052    \n",
      "    Negative Log Likelihood: 60.3391\tSigma2 Prior: -582.7454\tRegularization: 0.0011\n",
      "Iter: 18710  \tTraining Loss: -551.5561    \n",
      "    Negative Log Likelihood: 57.9795\tSigma2 Prior: -609.5367\tRegularization: 0.0011\n",
      "Iter: 18720  \tTraining Loss: -535.4304    \n",
      "    Negative Log Likelihood: 59.5303\tSigma2 Prior: -594.9618\tRegularization: 0.0011\n",
      "Iter: 18730  \tTraining Loss: -540.8624    \n",
      "    Negative Log Likelihood: 59.8252\tSigma2 Prior: -600.6887\tRegularization: 0.0011\n",
      "Iter: 18740  \tTraining Loss: -535.4955    \n",
      "    Negative Log Likelihood: 58.8542\tSigma2 Prior: -594.3508\tRegularization: 0.0011\n",
      "Iter: 18750  \tTraining Loss: -545.1509    \n",
      "    Negative Log Likelihood: 58.2979\tSigma2 Prior: -603.4499\tRegularization: 0.0011\n",
      "Iter: 18760  \tTraining Loss: -529.6283    \n",
      "    Negative Log Likelihood: 60.4909\tSigma2 Prior: -590.1202\tRegularization: 0.0011\n",
      "Iter: 18770  \tTraining Loss: -552.4367    \n",
      "    Negative Log Likelihood: 56.6569\tSigma2 Prior: -609.0947\tRegularization: 0.0011\n",
      "Iter: 18780  \tTraining Loss: -537.6757    \n",
      "    Negative Log Likelihood: 58.6814\tSigma2 Prior: -596.3582\tRegularization: 0.0011\n",
      "Iter: 18790  \tTraining Loss: -547.2950    \n",
      "    Negative Log Likelihood: 58.0333\tSigma2 Prior: -605.3295\tRegularization: 0.0011\n",
      "Iter: 18800  \tTraining Loss: -543.3366    \n",
      "    Negative Log Likelihood: 58.5283\tSigma2 Prior: -601.8660\tRegularization: 0.0011\n",
      "Iter: 18810  \tTraining Loss: -552.5244    \n",
      "    Negative Log Likelihood: 56.4724\tSigma2 Prior: -608.9979\tRegularization: 0.0011\n",
      "Iter: 18820  \tTraining Loss: -525.9122    \n",
      "    Negative Log Likelihood: 58.8859\tSigma2 Prior: -584.7992\tRegularization: 0.0011\n",
      "Iter: 18830  \tTraining Loss: -546.7039    \n",
      "    Negative Log Likelihood: 56.8820\tSigma2 Prior: -603.5869\tRegularization: 0.0011\n",
      "Iter: 18840  \tTraining Loss: -532.6077    \n",
      "    Negative Log Likelihood: 59.6278\tSigma2 Prior: -592.2365\tRegularization: 0.0011\n",
      "Iter: 18850  \tTraining Loss: -549.9294    \n",
      "    Negative Log Likelihood: 57.7689\tSigma2 Prior: -607.6994\tRegularization: 0.0011\n",
      "Iter: 18860  \tTraining Loss: -539.0490    \n",
      "    Negative Log Likelihood: 59.6665\tSigma2 Prior: -598.7166\tRegularization: 0.0011\n",
      "Iter: 18870  \tTraining Loss: -555.3824    \n",
      "    Negative Log Likelihood: 55.0185\tSigma2 Prior: -610.4020\tRegularization: 0.0011\n",
      "Iter: 18880  \tTraining Loss: -544.7138    \n",
      "    Negative Log Likelihood: 57.2013\tSigma2 Prior: -601.9161\tRegularization: 0.0011\n",
      "Iter: 18890  \tTraining Loss: -522.3978    \n",
      "    Negative Log Likelihood: 59.5731\tSigma2 Prior: -581.9719\tRegularization: 0.0011\n",
      "Iter: 18900  \tTraining Loss: -543.3271    \n",
      "    Negative Log Likelihood: 58.1100\tSigma2 Prior: -601.4382\tRegularization: 0.0011\n",
      "Iter: 18910  \tTraining Loss: -554.7968    \n",
      "    Negative Log Likelihood: 56.9159\tSigma2 Prior: -611.7137\tRegularization: 0.0011\n",
      "Iter: 18920  \tTraining Loss: -554.9752    \n",
      "    Negative Log Likelihood: 56.8470\tSigma2 Prior: -611.8233\tRegularization: 0.0011\n",
      "Iter: 18930  \tTraining Loss: -527.3995    \n",
      "    Negative Log Likelihood: 59.5017\tSigma2 Prior: -586.9023\tRegularization: 0.0011\n",
      "Iter: 18940  \tTraining Loss: -528.8675    \n",
      "    Negative Log Likelihood: 59.1552\tSigma2 Prior: -588.0237\tRegularization: 0.0011\n",
      "Iter: 18950  \tTraining Loss: -541.2605    \n",
      "    Negative Log Likelihood: 57.6194\tSigma2 Prior: -598.8810\tRegularization: 0.0011\n",
      "Iter: 18960  \tTraining Loss: -547.3157    \n",
      "    Negative Log Likelihood: 58.3513\tSigma2 Prior: -605.6681\tRegularization: 0.0011\n",
      "Iter: 18970  \tTraining Loss: -538.2170    \n",
      "    Negative Log Likelihood: 58.7969\tSigma2 Prior: -597.0150\tRegularization: 0.0011\n",
      "Iter: 18980  \tTraining Loss: -532.2534    \n",
      "    Negative Log Likelihood: 59.1808\tSigma2 Prior: -591.4352\tRegularization: 0.0011\n",
      "Iter: 18990  \tTraining Loss: -537.1535    \n",
      "    Negative Log Likelihood: 57.5606\tSigma2 Prior: -594.7152\tRegularization: 0.0011\n",
      "Iter: 19000  \tTraining Loss: -533.9279    \n",
      "    Negative Log Likelihood: 59.2576\tSigma2 Prior: -593.1865\tRegularization: 0.0011\n",
      "Iter: 19010  \tTraining Loss: -528.6729    \n",
      "    Negative Log Likelihood: 58.8674\tSigma2 Prior: -587.5414\tRegularization: 0.0011\n",
      "Iter: 19020  \tTraining Loss: -535.7838    \n",
      "    Negative Log Likelihood: 58.2417\tSigma2 Prior: -594.0266\tRegularization: 0.0011\n",
      "Iter: 19030  \tTraining Loss: -509.2564    \n",
      "    Negative Log Likelihood: 60.1154\tSigma2 Prior: -569.3730\tRegularization: 0.0011\n",
      "Iter: 19040  \tTraining Loss: -499.8460    \n",
      "    Negative Log Likelihood: 61.2945\tSigma2 Prior: -561.1415\tRegularization: 0.0011\n",
      "Iter: 19050  \tTraining Loss: -518.3998    \n",
      "    Negative Log Likelihood: 59.2757\tSigma2 Prior: -577.6766\tRegularization: 0.0011\n",
      "Iter: 19060  \tTraining Loss: -556.1089    \n",
      "    Negative Log Likelihood: 55.7286\tSigma2 Prior: -611.8386\tRegularization: 0.0011\n",
      "Iter: 19070  \tTraining Loss: -549.5645    \n",
      "    Negative Log Likelihood: 56.9610\tSigma2 Prior: -606.5265\tRegularization: 0.0011\n",
      "Iter: 19080  \tTraining Loss: -549.9799    \n",
      "    Negative Log Likelihood: 55.9764\tSigma2 Prior: -605.9574\tRegularization: 0.0011\n",
      "Iter: 19090  \tTraining Loss: -548.6885    \n",
      "    Negative Log Likelihood: 56.1815\tSigma2 Prior: -604.8711\tRegularization: 0.0011\n",
      "Iter: 19100  \tTraining Loss: -549.6329    \n",
      "    Negative Log Likelihood: 57.6557\tSigma2 Prior: -607.2897\tRegularization: 0.0011\n",
      "Iter: 19110  \tTraining Loss: -514.1920    \n",
      "    Negative Log Likelihood: 60.2811\tSigma2 Prior: -574.4742\tRegularization: 0.0011\n",
      "Iter: 19120  \tTraining Loss: -551.3388    \n",
      "    Negative Log Likelihood: 55.1447\tSigma2 Prior: -606.4846\tRegularization: 0.0011\n",
      "Iter: 19130  \tTraining Loss: -547.4807    \n",
      "    Negative Log Likelihood: 56.1777\tSigma2 Prior: -603.6595\tRegularization: 0.0011\n",
      "Iter: 19140  \tTraining Loss: -526.6910    \n",
      "    Negative Log Likelihood: 60.0090\tSigma2 Prior: -586.7011\tRegularization: 0.0011\n",
      "Iter: 19150  \tTraining Loss: -553.0707    \n",
      "    Negative Log Likelihood: 56.7610\tSigma2 Prior: -609.8329\tRegularization: 0.0011\n",
      "Iter: 19160  \tTraining Loss: -521.4462    \n",
      "    Negative Log Likelihood: 59.2488\tSigma2 Prior: -580.6962\tRegularization: 0.0011\n",
      "Iter: 19170  \tTraining Loss: -550.1647    \n",
      "    Negative Log Likelihood: 56.8114\tSigma2 Prior: -606.9772\tRegularization: 0.0011\n",
      "Iter: 19180  \tTraining Loss: -544.9802    \n",
      "    Negative Log Likelihood: 57.7398\tSigma2 Prior: -602.7211\tRegularization: 0.0011\n",
      "Iter: 19190  \tTraining Loss: -511.2564    \n",
      "    Negative Log Likelihood: 61.4734\tSigma2 Prior: -572.7310\tRegularization: 0.0011\n",
      "Iter: 19200  \tTraining Loss: -543.6914    \n",
      "    Negative Log Likelihood: 56.3208\tSigma2 Prior: -600.0133\tRegularization: 0.0011\n",
      "Iter: 19210  \tTraining Loss: -547.9188    \n",
      "    Negative Log Likelihood: 57.2650\tSigma2 Prior: -605.1849\tRegularization: 0.0011\n",
      "Iter: 19220  \tTraining Loss: -538.9860    \n",
      "    Negative Log Likelihood: 59.6233\tSigma2 Prior: -598.6104\tRegularization: 0.0011\n",
      "Iter: 19230  \tTraining Loss: -554.4403    \n",
      "    Negative Log Likelihood: 56.2134\tSigma2 Prior: -610.6548\tRegularization: 0.0011\n",
      "Iter: 19240  \tTraining Loss: -523.8414    \n",
      "    Negative Log Likelihood: 60.8177\tSigma2 Prior: -584.6602\tRegularization: 0.0011\n",
      "Iter: 19250  \tTraining Loss: -551.7657    \n",
      "    Negative Log Likelihood: 55.5906\tSigma2 Prior: -607.3574\tRegularization: 0.0011\n",
      "Iter: 19260  \tTraining Loss: -553.9222    \n",
      "    Negative Log Likelihood: 57.2783\tSigma2 Prior: -611.2016\tRegularization: 0.0011\n",
      "Iter: 19270  \tTraining Loss: -530.6300    \n",
      "    Negative Log Likelihood: 57.5521\tSigma2 Prior: -588.1832\tRegularization: 0.0011\n",
      "Iter: 19280  \tTraining Loss: -556.8337    \n",
      "    Negative Log Likelihood: 55.8787\tSigma2 Prior: -612.7135\tRegularization: 0.0011\n",
      "Iter: 19290  \tTraining Loss: -545.7311    \n",
      "    Negative Log Likelihood: 58.1307\tSigma2 Prior: -603.8630\tRegularization: 0.0011\n",
      "Iter: 19300  \tTraining Loss: -560.6035    \n",
      "    Negative Log Likelihood: 55.2545\tSigma2 Prior: -615.8591\tRegularization: 0.0011\n",
      "Iter: 19310  \tTraining Loss: -554.0562    \n",
      "    Negative Log Likelihood: 56.3378\tSigma2 Prior: -610.3951\tRegularization: 0.0011\n",
      "Iter: 19320  \tTraining Loss: -538.5416    \n",
      "    Negative Log Likelihood: 59.6662\tSigma2 Prior: -598.2089\tRegularization: 0.0011\n",
      "Iter: 19330  \tTraining Loss: -536.7750    \n",
      "    Negative Log Likelihood: 59.2554\tSigma2 Prior: -596.0315\tRegularization: 0.0011\n",
      "Iter: 19340  \tTraining Loss: -550.9923    \n",
      "    Negative Log Likelihood: 56.4585\tSigma2 Prior: -607.4520\tRegularization: 0.0011\n",
      "Iter: 19350  \tTraining Loss: -525.5316    \n",
      "    Negative Log Likelihood: 60.7072\tSigma2 Prior: -586.2399\tRegularization: 0.0011\n",
      "Iter: 19360  \tTraining Loss: -553.3373    \n",
      "    Negative Log Likelihood: 57.3950\tSigma2 Prior: -610.7334\tRegularization: 0.0011\n",
      "Iter: 19370  \tTraining Loss: -555.3416    \n",
      "    Negative Log Likelihood: 57.7397\tSigma2 Prior: -613.0824\tRegularization: 0.0011\n",
      "Iter: 19380  \tTraining Loss: -539.2993    \n",
      "    Negative Log Likelihood: 59.3336\tSigma2 Prior: -598.6340\tRegularization: 0.0011\n",
      "Iter: 19390  \tTraining Loss: -512.8691    \n",
      "    Negative Log Likelihood: 61.1772\tSigma2 Prior: -574.0474\tRegularization: 0.0011\n",
      "Iter: 19400  \tTraining Loss: -545.5534    \n",
      "    Negative Log Likelihood: 58.8826\tSigma2 Prior: -604.4371\tRegularization: 0.0011\n",
      "Iter: 19410  \tTraining Loss: -534.9061    \n",
      "    Negative Log Likelihood: 58.4166\tSigma2 Prior: -593.3239\tRegularization: 0.0011\n",
      "Iter: 19420  \tTraining Loss: -533.6473    \n",
      "    Negative Log Likelihood: 59.8175\tSigma2 Prior: -593.4659\tRegularization: 0.0011\n",
      "Iter: 19430  \tTraining Loss: -536.2073    \n",
      "    Negative Log Likelihood: 59.9811\tSigma2 Prior: -596.1896\tRegularization: 0.0011\n",
      "Iter: 19440  \tTraining Loss: -558.2870    \n",
      "    Negative Log Likelihood: 56.3966\tSigma2 Prior: -614.6847\tRegularization: 0.0011\n",
      "Iter: 19450  \tTraining Loss: -539.6761    \n",
      "    Negative Log Likelihood: 59.5700\tSigma2 Prior: -599.2472\tRegularization: 0.0011\n",
      "Iter: 19460  \tTraining Loss: -551.1387    \n",
      "    Negative Log Likelihood: 57.4932\tSigma2 Prior: -608.6330\tRegularization: 0.0011\n",
      "Iter: 19470  \tTraining Loss: -554.3445    \n",
      "    Negative Log Likelihood: 56.5731\tSigma2 Prior: -610.9187\tRegularization: 0.0011\n",
      "Iter: 19480  \tTraining Loss: -534.8041    \n",
      "    Negative Log Likelihood: 58.4474\tSigma2 Prior: -593.2526\tRegularization: 0.0011\n",
      "Iter: 19490  \tTraining Loss: -544.2979    \n",
      "    Negative Log Likelihood: 57.8483\tSigma2 Prior: -602.1472\tRegularization: 0.0011\n",
      "Iter: 19500  \tTraining Loss: -533.2318    \n",
      "    Negative Log Likelihood: 60.5090\tSigma2 Prior: -593.7419\tRegularization: 0.0011\n",
      "Iter: 19510  \tTraining Loss: -526.5997    \n",
      "    Negative Log Likelihood: 60.2254\tSigma2 Prior: -586.8262\tRegularization: 0.0011\n",
      "Iter: 19520  \tTraining Loss: -545.4029    \n",
      "    Negative Log Likelihood: 58.6234\tSigma2 Prior: -604.0274\tRegularization: 0.0011\n",
      "Iter: 19530  \tTraining Loss: -554.7604    \n",
      "    Negative Log Likelihood: 58.1507\tSigma2 Prior: -612.9122\tRegularization: 0.0011\n",
      "Iter: 19540  \tTraining Loss: -541.7546    \n",
      "    Negative Log Likelihood: 60.1563\tSigma2 Prior: -601.9119\tRegularization: 0.0011\n",
      "Iter: 19550  \tTraining Loss: -558.4041    \n",
      "    Negative Log Likelihood: 56.9209\tSigma2 Prior: -615.3260\tRegularization: 0.0011\n",
      "Iter: 19560  \tTraining Loss: -553.2059    \n",
      "    Negative Log Likelihood: 56.3651\tSigma2 Prior: -609.5721\tRegularization: 0.0011\n",
      "Iter: 19570  \tTraining Loss: -536.6475    \n",
      "    Negative Log Likelihood: 59.1831\tSigma2 Prior: -595.8317\tRegularization: 0.0011\n",
      "Iter: 19580  \tTraining Loss: -516.7259    \n",
      "    Negative Log Likelihood: 61.2097\tSigma2 Prior: -577.9366\tRegularization: 0.0011\n",
      "Iter: 19590  \tTraining Loss: -550.2027    \n",
      "    Negative Log Likelihood: 57.0186\tSigma2 Prior: -607.2224\tRegularization: 0.0011\n",
      "Iter: 19600  \tTraining Loss: -524.2551    \n",
      "    Negative Log Likelihood: 58.8493\tSigma2 Prior: -583.1055\tRegularization: 0.0011\n",
      "Iter: 19610  \tTraining Loss: -556.2020    \n",
      "    Negative Log Likelihood: 55.3409\tSigma2 Prior: -611.5440\tRegularization: 0.0011\n",
      "Iter: 19620  \tTraining Loss: -533.9100    \n",
      "    Negative Log Likelihood: 58.6496\tSigma2 Prior: -592.5607\tRegularization: 0.0011\n",
      "Iter: 19630  \tTraining Loss: -534.0041    \n",
      "    Negative Log Likelihood: 58.8630\tSigma2 Prior: -592.8682\tRegularization: 0.0011\n",
      "Iter: 19640  \tTraining Loss: -532.4902    \n",
      "    Negative Log Likelihood: 59.0077\tSigma2 Prior: -591.4990\tRegularization: 0.0011\n",
      "Iter: 19650  \tTraining Loss: -520.0085    \n",
      "    Negative Log Likelihood: 60.4730\tSigma2 Prior: -580.4825\tRegularization: 0.0011\n",
      "Iter: 19660  \tTraining Loss: -532.0612    \n",
      "    Negative Log Likelihood: 58.8313\tSigma2 Prior: -590.8936\tRegularization: 0.0011\n",
      "Iter: 19670  \tTraining Loss: -536.6946    \n",
      "    Negative Log Likelihood: 59.5626\tSigma2 Prior: -596.2583\tRegularization: 0.0011\n",
      "Iter: 19680  \tTraining Loss: -538.4940    \n",
      "    Negative Log Likelihood: 57.1246\tSigma2 Prior: -595.6197\tRegularization: 0.0011\n",
      "Iter: 19690  \tTraining Loss: -555.2800    \n",
      "    Negative Log Likelihood: 55.3775\tSigma2 Prior: -610.6587\tRegularization: 0.0011\n",
      "Iter: 19700  \tTraining Loss: -537.8065    \n",
      "    Negative Log Likelihood: 59.0267\tSigma2 Prior: -596.8343\tRegularization: 0.0011\n",
      "Iter: 19710  \tTraining Loss: -541.0234    \n",
      "    Negative Log Likelihood: 57.8732\tSigma2 Prior: -598.8977\tRegularization: 0.0011\n",
      "Iter: 19720  \tTraining Loss: -555.1960    \n",
      "    Negative Log Likelihood: 55.4031\tSigma2 Prior: -610.6002\tRegularization: 0.0011\n",
      "Iter: 19730  \tTraining Loss: -543.2743    \n",
      "    Negative Log Likelihood: 57.4834\tSigma2 Prior: -600.7588\tRegularization: 0.0011\n",
      "Iter: 19740  \tTraining Loss: -544.3262    \n",
      "    Negative Log Likelihood: 57.7816\tSigma2 Prior: -602.1089\tRegularization: 0.0011\n",
      "Iter: 19750  \tTraining Loss: -550.5558    \n",
      "    Negative Log Likelihood: 56.7604\tSigma2 Prior: -607.3173\tRegularization: 0.0011\n",
      "Iter: 19760  \tTraining Loss: -552.7953    \n",
      "    Negative Log Likelihood: 56.2311\tSigma2 Prior: -609.0275\tRegularization: 0.0011\n",
      "Iter: 19770  \tTraining Loss: -546.0081    \n",
      "    Negative Log Likelihood: 57.0487\tSigma2 Prior: -603.0578\tRegularization: 0.0011\n",
      "Iter: 19780  \tTraining Loss: -549.0035    \n",
      "    Negative Log Likelihood: 57.5205\tSigma2 Prior: -606.5251\tRegularization: 0.0011\n",
      "Iter: 19790  \tTraining Loss: -524.4640    \n",
      "    Negative Log Likelihood: 59.2802\tSigma2 Prior: -583.7452\tRegularization: 0.0011\n",
      "Iter: 19800  \tTraining Loss: -554.2164    \n",
      "    Negative Log Likelihood: 56.1001\tSigma2 Prior: -610.3176\tRegularization: 0.0011\n",
      "Iter: 19810  \tTraining Loss: -535.3909    \n",
      "    Negative Log Likelihood: 58.5399\tSigma2 Prior: -593.9318\tRegularization: 0.0011\n",
      "Iter: 19820  \tTraining Loss: -522.9048    \n",
      "    Negative Log Likelihood: 59.4444\tSigma2 Prior: -582.3503\tRegularization: 0.0011\n",
      "Iter: 19830  \tTraining Loss: -530.0651    \n",
      "    Negative Log Likelihood: 58.1391\tSigma2 Prior: -588.2053\tRegularization: 0.0011\n",
      "Iter: 19840  \tTraining Loss: -518.0052    \n",
      "    Negative Log Likelihood: 59.1415\tSigma2 Prior: -577.1478\tRegularization: 0.0011\n",
      "Iter: 19850  \tTraining Loss: -539.9031    \n",
      "    Negative Log Likelihood: 56.8241\tSigma2 Prior: -596.7283\tRegularization: 0.0011\n",
      "Iter: 19860  \tTraining Loss: -548.7193    \n",
      "    Negative Log Likelihood: 56.2540\tSigma2 Prior: -604.9744\tRegularization: 0.0011\n",
      "Iter: 19870  \tTraining Loss: -518.2302    \n",
      "    Negative Log Likelihood: 60.1760\tSigma2 Prior: -578.4073\tRegularization: 0.0011\n",
      "Iter: 19880  \tTraining Loss: -546.8106    \n",
      "    Negative Log Likelihood: 55.4068\tSigma2 Prior: -602.2185\tRegularization: 0.0011\n",
      "Iter: 19890  \tTraining Loss: -545.5919    \n",
      "    Negative Log Likelihood: 57.7765\tSigma2 Prior: -603.3696\tRegularization: 0.0011\n",
      "Iter: 19900  \tTraining Loss: -535.3230    \n",
      "    Negative Log Likelihood: 57.9339\tSigma2 Prior: -593.2579\tRegularization: 0.0011\n",
      "Iter: 19910  \tTraining Loss: -541.5198    \n",
      "    Negative Log Likelihood: 57.8827\tSigma2 Prior: -599.4036\tRegularization: 0.0011\n",
      "Iter: 19920  \tTraining Loss: -539.5042    \n",
      "    Negative Log Likelihood: 58.3447\tSigma2 Prior: -597.8500\tRegularization: 0.0011\n",
      "Iter: 19930  \tTraining Loss: -561.7770    \n",
      "    Negative Log Likelihood: 55.4836\tSigma2 Prior: -617.2617\tRegularization: 0.0011\n",
      "Iter: 19940  \tTraining Loss: -531.6567    \n",
      "    Negative Log Likelihood: 58.8565\tSigma2 Prior: -590.5142\tRegularization: 0.0011\n",
      "Iter: 19950  \tTraining Loss: -525.9113    \n",
      "    Negative Log Likelihood: 59.7681\tSigma2 Prior: -585.6805\tRegularization: 0.0011\n",
      "Iter: 19960  \tTraining Loss: -551.2968    \n",
      "    Negative Log Likelihood: 57.2749\tSigma2 Prior: -608.5728\tRegularization: 0.0011\n",
      "Iter: 19970  \tTraining Loss: -545.9413    \n",
      "    Negative Log Likelihood: 57.6670\tSigma2 Prior: -603.6094\tRegularization: 0.0011\n",
      "Iter: 19980  \tTraining Loss: -525.4857    \n",
      "    Negative Log Likelihood: 59.7289\tSigma2 Prior: -585.2157\tRegularization: 0.0011\n",
      "Iter: 19990  \tTraining Loss: -526.8978    \n",
      "    Negative Log Likelihood: 59.6308\tSigma2 Prior: -586.5298\tRegularization: 0.0011\n",
      "Iter: 19999  \tTraining Loss: -556.0757    \n",
      "    Negative Log Likelihood: 55.1877\tSigma2 Prior: -611.2645\tRegularization: 0.0011\n",
      "Done training with 20000 iterations\n",
      "<class 'list'>\n",
      "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'15_0': 0, '15_1': 1, '15_2': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_2', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_0', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1', '15_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "{'30_0': 0, '30_1': 1}\n",
      "{0: 0, 1: 1}\n",
      "Ground truth labels:\n",
      "['30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_1', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_0', '30_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "{'52_0': 0, '52_1': 1, '52_2': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_0', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_2', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1', '52_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]\n",
      "{'71_0': 0, '71_1': 1, '71_2': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_0', '71_0', '71_0', '71_0', '71_0', '71_0', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1', '71_2', '71_2', '71_2', '71_2', '71_2', '71_2', '71_1', '71_1', '71_1', '71_1', '71_1', '71_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1]\n",
      "{'75_0': 0, '75_1': 1, '75_2': 2, '75_3': 3, '75_4': 4}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Ground truth labels:\n",
      "['75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_2', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_0', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_3', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_2', '75_2', '75_2', '75_2', '75_2', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_4', '75_3', '75_3', '75_3', '75_3', '75_3', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_1', '75_2', '75_2', '75_2']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "{'80_0': 0, '80_3': 1, '80_4': 2, '80_5': 3}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3}\n",
      "Ground truth labels:\n",
      "['80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_5', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_0', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_3', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4', '80_4']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_2', '140_2', '140_2', '140_2', '140_2', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "{'140_0': 0, '140_1': 1, '140_2': 2, '140_4': 3, '140_5': 4}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Ground truth labels:\n",
      "['140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_1', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_2', '140_2', '140_2', '140_2', '140_2', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_5', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_0', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4', '140_4']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'146_0': 0, '146_1': 1}\n",
      "{0: 0, 1: 1}\n",
      "Ground truth labels:\n",
      "['146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_0', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1', '146_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
      "{'152_0': 0, '152_1': 1, '152_2': 2, '152_4': 3}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3}\n",
      "Ground truth labels:\n",
      "['152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_4', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_1', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_2', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_0', '152_2', '152_2', '152_2', '152_2']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_1', '175_1', '175_1', '175_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2]\n",
      "{'175_0': 0, '175_1': 1, '175_2': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_1', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_2', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_0', '175_1', '175_1', '175_1', '175_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'204_0': 0, '204_1': 1, '204_2': 2, '204_3': 3}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Ground truth labels:\n",
      "['204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_1', '204_0', '204_0', '204_0', '204_0', '204_0', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_3', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_2', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0', '204_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_5', '315_5', '315_5', '315_5']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n",
      "{'315_0': 0, '315_1': 1, '315_2': 2, '315_4': 3, '315_5': 4}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Ground truth labels:\n",
      "['315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_0', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_4', '315_2', '315_2', '315_2', '315_2', '315_2', '315_2', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_1', '315_5', '315_5', '315_5', '315_5']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'333_0': 0, '333_1': 1}\n",
      "{0: 0, 1: 1}\n",
      "Ground truth labels:\n",
      "['333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_0', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1', '333_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'347_0': 0}\n",
      "{0: 0}\n",
      "Ground truth labels:\n",
      "['347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0', '347_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'361_0': 0, '361_1': 1, '361_2': 2, '361_3': 3}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3}\n",
      "Ground truth labels:\n",
      "['361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_0', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_2', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_3', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1', '361_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "{'374_0': 0, '374_1': 1}\n",
      "{0: 0, 1: 1}\n",
      "Ground truth labels:\n",
      "['374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_0', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_1', '374_0', '374_0', '374_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "{'405_0': 0, '405_1': 1, '405_2': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_1', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_2', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_0', '405_1', '405_1', '405_1', '405_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2]\n",
      "{'419_0': 0, '419_1': 1, '419_2': 2, '419_3': 3, '419_4': 4, '419_6': 5}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5}\n",
      "Ground truth labels:\n",
      "['419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_3', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_2', '419_0', '419_0', '419_0', '419_0', '419_0', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_1', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_6', '419_4', '419_4', '419_4', '419_4', '419_4', '419_4']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'430_0': 0}\n",
      "{0: 0}\n",
      "Ground truth labels:\n",
      "['430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0', '430_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_2', '439_2', '439_2', '439_2', '439_2', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_2', '439_2', '439_2', '439_2', '439_2', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "{'439_0': 0, '439_1': 1, '439_2': 2, '439_3': 3, '439_4': 4}\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4}\n",
      "Ground truth labels:\n",
      "['439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_0', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_2', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_2', '439_2', '439_2', '439_2', '439_2', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_3', '439_2', '439_2', '439_2', '439_2', '439_2', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_1', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4', '439_4']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'441_0': 0, '441_1': 1}\n",
      "{0: 0, 1: 1}\n",
      "Ground truth labels:\n",
      "['441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_1', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0', '441_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "{'442_0': 0}\n",
      "{0: 0}\n",
      "Ground truth labels:\n",
      "['442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0', '442_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'459_0': 0, '459_1': 1}\n",
      "{0: 0, 1: 1}\n",
      "Ground truth labels:\n",
      "['459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_0', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1', '459_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "{'491_0': 0, '491_1': 1, '491_3': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_0', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_3', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1', '491_1']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'list'>\n",
      "['494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_2', '494_2', '494_2', '494_2', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "{'494_0': 0, '494_1': 1, '494_2': 2}\n",
      "{0: 0, 1: 1, 2: 2}\n",
      "Ground truth labels:\n",
      "['494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_2', '494_2', '494_2', '494_2', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_2', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_0', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_1', '494_0', '494_0']\n",
      "Predicted labels:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]\n",
      "--------------------------------------------------------------------------------\n",
      "Finished diarization experiment\n",
      "Config:\n",
      "  sigma_alpha: 1\n",
      "  sigma_beta: 1\n",
      "  crp_alpha: 1.0\n",
      "  learning rate: 1e-05\n",
      "  regularization: 1e-05\n",
      "  batch size: 20\n",
      "Performance:\n",
      "  averaged accuracy: 0.999574\n",
      "  accuracy numbers for all testing sequences:\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    0.989362\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "    1.000000\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test toy set\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "# training model on toy dataset\n",
    "SAVED_MODEL_NAME = 'saved_model_toy.uisrnn'\n",
    "test=np.load('drive/My Drive/toy_testing_data.npz',allow_pickle=True)\n",
    "train=np.load('drive/My Drive/toy_training_data.npz',allow_pickle=True)\n",
    "def diarization_experiment(args,train,test):\n",
    "  \"\"\"Experiment pipeline.\n",
    "  Load data --> train model --> test model --> output result\n",
    "  Args:\n",
    "    model_args: model configurations\n",
    "    training_args: training configurations\n",
    "    inference_args: inference configurations\n",
    "  \"\"\"\n",
    "\n",
    "  predicted_cluster_ids = []\n",
    "  test_record = []\n",
    "\n",
    "  train_sequence = train[train.files[0]]\n",
    "  test_sequence= test[test.files[0]]\n",
    "  train_cluster_id= train[train.files[1]]\n",
    "  test_cluster_id= test[test.files[1]]\n",
    "  #test_cluster_id=test_cluster_id.reshape(-1,1)\n",
    "  model = UISRNN(args)\n",
    "  # training\n",
    "  model.fit(train_sequence, train_cluster_id, args)\n",
    "  model.save(SAVED_MODEL_NAME)\n",
    "  # we can also skip training by calling：\n",
    "  #model.load(SAVED_MODEL_NAME)\n",
    "  # testing\n",
    "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
    "    predicted_cluster_id = model.predict(test_sequence, args)\n",
    "    predicted_cluster_ids.append(predicted_cluster_id)\n",
    "    print(type(predicted_cluster_id))\n",
    "    accuracy = compute_sequence_match_accuracy(\n",
    "        test_cluster_id, predicted_cluster_id)\n",
    "    test_record.append((accuracy, len(test_cluster_id)))\n",
    "    print('Ground truth labels:')\n",
    "    print(test_cluster_id)\n",
    "    print('Predicted labels:')\n",
    "    print(predicted_cluster_id)\n",
    "    print('-' * 80)\n",
    "  output_string = output_result(args,args,test_record)\n",
    "\n",
    "  print('Finished diarization experiment')\n",
    "  print(output_string)\n",
    "args=arguments()\n",
    "diarization_experiment(args,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRcQCDOEcaey"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "#save model to drive\n",
    "# 1. Authenticate and create the PyDrive client.\n",
    "def save_files_on_drive(filename,ismodel):\n",
    "    auth.authenticate_user()\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.credentials = GoogleCredentials.get_application_default()\n",
    "    drive = GoogleDrive(gauth)\n",
    "    if ismodel==True:\n",
    "        model = UISRNN(args)\n",
    "        model.load(filename)\n",
    "        # create on Colab directory\n",
    "        model.save(filename)    \n",
    "        files = drive.CreateFile({'title' : filename})\n",
    "        files.SetContentFile(filename)\n",
    "        files.Upload()\n",
    "        # download to google drive\n",
    "        drive.CreateFile({'id': files.get('id')})\n",
    "    if ismodel==False:\n",
    "        d=np.load(filename)\n",
    "        files = drive.CreateFile({'title' : filename})\n",
    "        files.SetContentFile(d)\n",
    "        files.Upload()\n",
    "        # download to google drive\n",
    "        drive.CreateFile({'id': files.get('id')})\n",
    "    return \n",
    "save_files_on_drive(\"saved_model_toy.uisrnn\",True)\n",
    "save_files_on_drive(\"saved_model_timit.uisrnn\",True)\n",
    "save_files_on_drive(\"train_sequence.npy\",False)\n",
    "save_files_on_drive(\"test_sequence.npy\",False)\n",
    "save_files_on_drive(\"train_cluster_id.npy\",False)\n",
    "save_files_on_drive(\"test_cluster_id.npy\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pz3JJ0lycauP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "SAVED_MODEL_NAME = '/content/drive/My Drive/saved_model_toy.uisrnn'\n",
    "#model trained on toy data set produced same results as the model trained on timit dataset\n",
    "\n",
    "def diarization_experiment(args):\n",
    "  \"\"\"Experiment pipeline.\n",
    "  Load data --> train model --> test model --> output result\n",
    "  Args:\n",
    "    model_args: model configurations\n",
    "    training_args: training configurations\n",
    "    inference_args: inference configurations\n",
    "  \"\"\"\n",
    "\n",
    "  predicted_cluster_ids = []\n",
    "  test_record = []\n",
    "\n",
    "  train_sequence = np.load('/content/drive/My Drive/train_sequence.npy')\n",
    "  test_sequence= np.load('/content/drive/My Drive/test_sequence.npy')\n",
    "  train_cluster_id= np.load('/content/drive/My Drive/train_cluster_id.npy')\n",
    "  test_cluster_id= np.load('/content/drive/My Drive/test_cluster_id.npy')\n",
    "  test_cluster_id=test_cluster_id.reshape(-1,1)\n",
    "  model = UISRNN(args)\n",
    "  # training\n",
    "  #model.fit(train_sequence, train_cluster_id, args)\n",
    "  #model.save(SAVED_MODEL_NAME)\n",
    "  # we can also skip training by calling：\n",
    "  model.load(SAVED_MODEL_NAME)\n",
    "\n",
    "  # testing\n",
    "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
    "    predicted_cluster_id = model.predict(test_sequence.reshape(1,-1), args)\n",
    "    predicted_cluster_ids.append(predicted_cluster_id)\n",
    "    print(type(predicted_cluster_id))\n",
    "    accuracy = compute_sequence_match_accuracy(\n",
    "        test_cluster_id, predicted_cluster_id)\n",
    "    test_record.append((accuracy, len(test_cluster_id)))\n",
    "    print('Ground truth labels:')\n",
    "    print(test_cluster_id)\n",
    "    print('Predicted labels:')\n",
    "    print(predicted_cluster_id)\n",
    "    print('-' * 80)\n",
    "  output_string = output_result(args,args,test_record)\n",
    "\n",
    "  print('Finished diarization experiment')\n",
    "  print(output_string)\n",
    "args=arguments()\n",
    "diarization_experiment(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQwFVLCPcarh"
   },
   "outputs": [],
   "source": [
    "#test toy set\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "# use trained model on timit to see performence on toy dataset\n",
    "SAVED_MODEL_NAME = '/content/drive/My Drive/saved_model_timit.uisrnn'\n",
    "test=np.load('drive/My Drive/toy_testing_data.npz',allow_pickle=True)\n",
    "train=np.load('drive/My Drive/toy_training_data.npz',allow_pickle=True)\n",
    "def diarization_experiment(args,train,test):\n",
    "  \"\"\"Experiment pipeline.\n",
    "  Load data --> train model --> test model --> output result\n",
    "  Args:\n",
    "    model_args: model configurations\n",
    "    training_args: training configurations\n",
    "    inference_args: inference configurations\n",
    "  \"\"\"\n",
    "\n",
    "  predicted_cluster_ids = []\n",
    "  test_record = []\n",
    "\n",
    "  train_sequence = train[train.files[0]]\n",
    "  test_sequence= test[test.files[0]]\n",
    "  train_cluster_id= train[train.files[1]]\n",
    "  test_cluster_id= test[test.files[1]]\n",
    "  #test_cluster_id=test_cluster_id.reshape(-1,1)\n",
    "  model = UISRNN(args)\n",
    "  # training\n",
    "  #model.fit(train_sequence, train_cluster_id, args)\n",
    "  #model.save(SAVED_MODEL_NAME)\n",
    "  # we can also skip training by calling：\n",
    "  model.load(SAVED_MODEL_NAME)\n",
    "  # testing\n",
    "  for (test_sequence, test_cluster_id) in zip(test_sequence, test_cluster_id):\n",
    "    predicted_cluster_id = model.predict(test_sequence, args)\n",
    "    predicted_cluster_ids.append(predicted_cluster_id)\n",
    "    print(type(predicted_cluster_id))\n",
    "    accuracy = compute_sequence_match_accuracy(\n",
    "        test_cluster_id, predicted_cluster_id)\n",
    "    test_record.append((accuracy, len(test_cluster_id)))\n",
    "    print('Ground truth labels:')\n",
    "    print(test_cluster_id)\n",
    "    print('Predicted labels:')\n",
    "    print(predicted_cluster_id)\n",
    "    print('-' * 80)\n",
    "  output_string = output_result(args,args,test_record)\n",
    "\n",
    "  print('Finished diarization experiment')\n",
    "  print(output_string)\n",
    "args=arguments()\n",
    "diarization_experiment(args,train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D0LWP50mcapL"
   },
   "outputs": [],
   "source": [
    "#results show that if training data is not diverse than trained model cannot perform well on diversed \n",
    "#dataset results of using timit-trained model on testing on toy dataset leades to low accuracy on the \n",
    "#other side toy-trained model produced 100 % diarization results (because of diversity on train dataset \n",
    "#which is not present in timit dataset"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
